{"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"jqkVtoAj84P_"}},{"cell_type":"markdown","metadata":{"id":"OIsx7X6C_TfC"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":50429,"status":"ok","timestamp":1753717079859,"user":{"displayName":"Andrew Liao","userId":"01433793926959815213"},"user_tz":240},"id":"N78qPuMA_Q53","outputId":"7122337c-3b1c-4444-9889-7a01cde218b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ONE-api\n","  Downloading one_api-3.3.0-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: ruff in /usr/local/lib/python3.11/dist-packages (from ONE-api) (0.12.4)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.11/dist-packages (from ONE-api) (2.0.2)\n","Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from ONE-api) (2.2.2)\n","Requirement already satisfied: tqdm>=4.32.1 in /usr/local/lib/python3.11/dist-packages (from ONE-api) (4.67.1)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ONE-api) (2.32.3)\n","Collecting iblutil>=1.14.0 (from ONE-api)\n","  Downloading iblutil-1.20.0-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ONE-api) (25.0)\n","Collecting boto3 (from ONE-api)\n","  Downloading boto3-1.39.14-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ONE-api) (6.0.2)\n","Collecting colorlog>=6.0.0 (from iblutil>=1.14.0->ONE-api)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numba>0.53.1 in /usr/local/lib/python3.11/dist-packages (from iblutil>=1.14.0->ONE-api) (0.60.0)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from iblutil>=1.14.0->ONE-api) (18.1.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from iblutil>=1.14.0->ONE-api) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ONE-api) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ONE-api) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ONE-api) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ONE-api) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ONE-api) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ONE-api) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ONE-api) (2025.7.14)\n","Collecting botocore<1.40.0,>=1.39.14 (from boto3->ONE-api)\n","  Downloading botocore-1.39.14-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3->ONE-api)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->ONE-api)\n","  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>0.53.1->iblutil>=1.14.0->ONE-api) (0.43.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->ONE-api) (1.17.0)\n","Downloading one_api-3.3.0-py3-none-any.whl (996 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.3/996.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading iblutil-1.20.0-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading boto3-1.39.14-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.39.14-py3-none-any.whl (13.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jmespath, colorlog, botocore, s3transfer, iblutil, boto3, ONE-api\n","Successfully installed ONE-api-3.3.0 boto3-1.39.14 botocore-1.39.14 colorlog-6.9.0 iblutil-1.20.0 jmespath-1.0.1 s3transfer-0.13.1\n","Collecting ibllib\n","  Downloading ibllib-3.4.1-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from ibllib) (1.39.14)\n","Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (8.2.1)\n","Requirement already satisfied: colorlog>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from ibllib) (6.9.0)\n","Collecting flake8>=3.7.8 (from ibllib)\n","  Downloading flake8-7.3.0-py2.py3-none-any.whl.metadata (3.8 kB)\n","Collecting globus-sdk (from ibllib)\n","  Downloading globus_sdk-3.61.0-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from ibllib) (0.21)\n","Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from ibllib) (3.10.0)\n","Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.11/dist-packages (from ibllib) (0.60.0)\n","Requirement already satisfied: numpy<=2.2,>=1.18 in /usr/local/lib/python3.11/dist-packages (from ibllib) (2.0.2)\n","Collecting nptdms (from ibllib)\n","  Downloading nptdms-1.10.0.tar.gz (181 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from ibllib) (4.12.0.88)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ibllib) (2.2.2)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from ibllib) (18.1.0)\n","Collecting pynrrd>=0.4.0 (from ibllib)\n","  Downloading pynrrd-1.1.3-py3-none-any.whl.metadata (5.4 kB)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from ibllib) (8.4.1)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (2.32.3)\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.11/dist-packages (from ibllib) (1.6.1)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (1.16.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from ibllib) (0.25.2)\n","Collecting imagecodecs (from ibllib)\n","  Downloading imagecodecs-2025.3.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Collecting sparse (from ibllib)\n","  Downloading sparse-0.17.0-py2.py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (0.13.2)\n","Requirement already satisfied: tqdm>=4.32.1 in /usr/local/lib/python3.11/dist-packages (from ibllib) (4.67.1)\n","Collecting iblatlas>=0.5.3 (from ibllib)\n","  Downloading iblatlas-0.9.0-py3-none-any.whl.metadata (3.3 kB)\n","Collecting ibl-neuropixel>=1.6.2 (from ibllib)\n","  Downloading ibl_neuropixel-1.8.1-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: iblutil>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (1.20.0)\n","Collecting iblqt>=0.4.2 (from ibllib)\n","  Downloading iblqt-0.8.0-py3-none-any.whl.metadata (1.6 kB)\n","Collecting mtscomp>=1.0.1 (from ibllib)\n","  Downloading mtscomp-1.0.2-py2.py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: ONE-api>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from ibllib) (3.3.0)\n","Collecting phylib>=2.6.0 (from ibllib)\n","  Downloading phylib-2.6.1-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting psychofit (from ibllib)\n","  Downloading Psychofit-1.0.0.post0-py3-none-any.whl.metadata (2.0 kB)\n","Collecting slidingRP>=1.1.1 (from ibllib)\n","  Downloading slidingRP-1.1.1-py3-none-any.whl.metadata (1.8 kB)\n","Collecting pyqt5 (from ibllib)\n","  Downloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n","Collecting ibl-style (from ibllib)\n","  Downloading ibl_style-0.1.0-py3-none-any.whl.metadata (10 kB)\n","Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.7.8->ibllib)\n","  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n","Collecting pycodestyle<2.15.0,>=2.14.0 (from flake8>=3.7.8->ibllib)\n","  Downloading pycodestyle-2.14.0-py2.py3-none-any.whl.metadata (4.5 kB)\n","Collecting pyflakes<3.5.0,>=3.4.0 (from flake8>=3.7.8->ibllib)\n","  Downloading pyflakes-3.4.0-py2.py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from ibl-neuropixel>=1.6.2->ibllib) (1.5.1)\n","Collecting qtpy>=2.4.1 (from iblqt>=0.4.2->ibllib)\n","  Downloading QtPy-2.4.3-py3-none-any.whl.metadata (12 kB)\n","Collecting pyqtgraph>=0.13.7 (from iblqt>=0.4.2->ibllib)\n","  Downloading pyqtgraph-0.13.7-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from iblutil>=1.13.0->ibllib) (25.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.3->ibllib) (2.9.0.post0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.56->ibllib) (0.43.0)\n","Requirement already satisfied: ruff in /usr/local/lib/python3.11/dist-packages (from ONE-api>=3.2.0->ibllib) (0.12.4)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ONE-api>=3.2.0->ibllib) (6.0.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ibllib) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ibllib) (2025.2)\n","Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (from phylib>=2.6.0->ibllib) (2025.5.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from phylib>=2.6.0->ibllib) (0.12.1)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from pynrrd>=0.4.0->ibllib) (4.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ibllib) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ibllib) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ibllib) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ibllib) (2025.7.14)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.1->ibllib) (3.6.0)\n","Requirement already satisfied: colorcet in /usr/local/lib/python3.11/dist-packages (from slidingRP>=1.1.1->ibllib) (3.1.0)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from slidingRP>=1.1.1->ibllib) (0.14.5)\n","Requirement already satisfied: botocore<1.40.0,>=1.39.14 in /usr/local/lib/python3.11/dist-packages (from boto3->ibllib) (1.39.14)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->ibllib) (1.0.1)\n","Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3->ibllib) (0.13.1)\n","Requirement already satisfied: pyjwt<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]<3.0.0,>=2.0.0->globus-sdk->ibllib) (2.10.1)\n","Requirement already satisfied: cryptography!=3.4.0,>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from globus-sdk->ibllib) (43.0.3)\n","Collecting figrid (from ibl-style->ibllib)\n","  Downloading figrid-0.1.7-py3-none-any.whl.metadata (6.4 kB)\n","Collecting PyQt5-sip<13,>=12.15 (from pyqt5->ibllib)\n","  Downloading PyQt5_sip-12.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (472 bytes)\n","Collecting PyQt5-Qt5<5.16.0,>=5.15.2 (from pyqt5->ibllib)\n","  Downloading PyQt5_Qt5-5.15.17-py3-none-manylinux2014_x86_64.whl.metadata (536 bytes)\n","Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.11/dist-packages (from pytest->ibllib) (2.1.0)\n","Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->ibllib) (1.6.0)\n","Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from pytest->ibllib) (2.19.2)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->ibllib) (3.5)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->ibllib) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->ibllib) (2025.6.11)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->ibllib) (0.4)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography!=3.4.0,>=3.3.1->globus-sdk->ibllib) (1.17.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.3->ibllib) (1.17.0)\n","Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask->phylib>=2.6.0->ibllib) (3.1.1)\n","Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask->phylib>=2.6.0->ibllib) (2025.3.0)\n","Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask->phylib>=2.6.0->ibllib) (1.4.2)\n","Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask->phylib>=2.6.0->ibllib) (8.7.0)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->slidingRP>=1.1.1->ibllib) (1.0.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography!=3.4.0,>=3.3.1->globus-sdk->ibllib) (2.22)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask->phylib>=2.6.0->ibllib) (3.23.0)\n","Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask->phylib>=2.6.0->ibllib) (1.0.0)\n","Downloading ibllib-3.4.1-py3-none-any.whl (8.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flake8-7.3.0-py2.py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ibl_neuropixel-1.8.1-py3-none-any.whl (111 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.6/111.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading iblatlas-0.9.0-py3-none-any.whl (206 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.0/206.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading iblqt-0.8.0-py3-none-any.whl (21 kB)\n","Downloading mtscomp-1.0.2-py2.py3-none-any.whl (16 kB)\n","Downloading phylib-2.6.1-py2.py3-none-any.whl (80 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.4/80.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pynrrd-1.1.3-py3-none-any.whl (23 kB)\n","Downloading slidingRP-1.1.1-py3-none-any.whl (40 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading globus_sdk-3.61.0-py3-none-any.whl (415 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.0/415.0 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ibl_style-0.1.0-py3-none-any.whl (11 kB)\n","Downloading imagecodecs-2025.3.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Psychofit-1.0.0.post0-py3-none-any.whl (5.7 kB)\n","Downloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl (8.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sparse-0.17.0-py2.py3-none-any.whl (259 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n","Downloading pycodestyle-2.14.0-py2.py3-none-any.whl (31 kB)\n","Downloading pyflakes-3.4.0-py2.py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyQt5_Qt5-5.15.17-py3-none-manylinux2014_x86_64.whl (61.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyQt5_sip-12.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.whl (276 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyqtgraph-0.13.7-py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading QtPy-2.4.3-py3-none-any.whl (95 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.0/95.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading figrid-0.1.7-py3-none-any.whl (7.1 kB)\n","Building wheels for collected packages: nptdms\n","  Building wheel for nptdms (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nptdms: filename=nptdms-1.10.0-py3-none-any.whl size=108456 sha256=18d43ea057b90ed6e83d84ed69097010f0efcaea7cca921313a57c1c3f742d46\n","  Stored in directory: /root/.cache/pip/wheels/1b/4b/17/21e8b03b37ea51ce7ec9f5570cdf0decca93f537d61c06880f\n","Successfully built nptdms\n","Installing collected packages: PyQt5-Qt5, mtscomp, qtpy, pyqtgraph, PyQt5-sip, pynrrd, pyflakes, pycodestyle, nptdms, mccabe, imagecodecs, sparse, pyqt5, psychofit, flake8, phylib, slidingRP, globus-sdk, figrid, ibl-style, iblqt, iblatlas, ibl-neuropixel, ibllib\n","Successfully installed PyQt5-Qt5-5.15.17 PyQt5-sip-12.17.0 figrid-0.1.7 flake8-7.3.0 globus-sdk-3.61.0 ibl-neuropixel-1.8.1 ibl-style-0.1.0 iblatlas-0.9.0 ibllib-3.4.1 iblqt-0.8.0 imagecodecs-2025.3.30 mccabe-0.7.0 mtscomp-1.0.2 nptdms-1.10.0 phylib-2.6.1 psychofit-1.0.0.post0 pycodestyle-2.14.0 pyflakes-3.4.0 pynrrd-1.1.3 pyqt5-5.15.11 pyqtgraph-0.13.7 qtpy-2.4.3 slidingRP-1.1.1 sparse-0.17.0\n"]}],"source":["! pip install ONE-api\n","! pip install ibllib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20817,"status":"ok","timestamp":1753717109345,"user":{"displayName":"Andrew Liao","userId":"01433793926959815213"},"user_tz":240},"id":"1_uyQam1_WNM","outputId":"3f0f16b9-1b40-4210-de83-b7171f388786"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Standard libraries\n","from pprint import pprint\n","import random\n","from typing import List, Dict, Optional, Tuple, Union\n","import warnings\n","import os\n","import requests\n","from pathlib import Path\n","import time\n","\n","# Third-party libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from matplotlib.gridspec import GridSpec\n","import matplotlib as mpl\n","from matplotlib.lines import Line2D\n","from scipy.ndimage import uniform_filter1d\n","from IPython.display import HTML, display\n","\n","# IBL-specific libraries\n","from iblatlas.atlas import BrainAtlas\n","from iblatlas import atlas\n","from ibllib.io import video\n","from one.api import ONE, OneAlyx\n","from brainbox.io.one import SessionLoader\n","\n","from one.api import ONE\n","from brainbox.io.one import SessionLoader\n","import ibllib.io.video as video\n","from iblutil.util import Bunch\n","\n","# Matplotlib animation limit\n","mpl.rcParams['animation.embed_limit'] = 200\n","\n","# Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","BASE_DIR = \"content/drive/My Drive/S25/Langone/Breathing/Videos\" # capital Drive is for cloud, drive is for local"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1115,"status":"ok","timestamp":1753717110462,"user":{"displayName":"Andrew Liao","userId":"01433793926959815213"},"user_tz":240},"id":"fnx1D6ca_XoF","outputId":"f0e1a4af-026a-4023-f308-82127da9facc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Connected to https://openalyx.internationalbrainlab.org as user \"intbrainlab\"\n"]}],"source":["ONE.setup(base_url='https://openalyx.internationalbrainlab.org', silent=True)\n","one = ONE(password='international')"]},{"cell_type":"markdown","metadata":{"id":"m81FScF3_PBb"},"source":["# IBLDataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b545U7JR_Mbx"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from one.api import ONE\n","from brainbox.io.one import SessionLoader\n","import ibllib.io.video as video\n","from typing import Dict, Optional\n","\n","class IBLDataLoader:\n","    \"\"\"\n","    Loads and organizes IBL behavioral data for animation purposes.\n","\n","    This class handles session loading, data validation, interval sampling,\n","    and synchronization between video timestamps and behavioral features.\n","    \"\"\"\n","\n","    def __init__(self, eid: str, verbose: bool = True):\n","        \"\"\"\n","        Initialize the data loader for a specific experiment.\n","\n","        Args:\n","            eid (str): Experiment ID to load\n","            verbose (bool): Enable detailed logging and debugging output\n","        \"\"\"\n","        self.eid = eid\n","        self.verbose = verbose\n","        self.one = ONE()\n","        self.sessLoader = None\n","        self.sessionInfo = {}\n","        self.trialData = None\n","        self.poseData = None\n","        self.wheelData = None\n","        self.motionEnergyData = None\n","        self.pupilData = None\n","        self.fps = None # dictionary left, right, body\n","        self.videoTimes = None # Default aligns time to left camera, can be changed in loadVideoTimestamps() self.videoTimes = timestamps['left']\n","        self.urls = None\n","\n","        # Features we want to track from pose data\n","        self.requiredPoseFeatures = [\n","            'times', 'nose_tip_x', 'nose_tip_y', 'pupil_top_r_x', 'pupil_top_r_y',\n","            'pupil_right_r_x', 'pupil_right_r_y', 'pupil_bottom_r_x', 'pupil_bottom_r_y',\n","            'pupil_left_r_x', 'pupil_left_r_y', 'tube_top_x', 'tube_top_y',\n","            'tube_bottom_x', 'tube_bottom_y', 'tongue_end_l_x', 'tongue_end_l_y',\n","            'tongue_end_r_x', 'tongue_end_r_y'\n","        ]\n","\n","        # Trial event features we want to extract\n","        self.requiredTrialFeatures = [\n","            'intervals_0', 'intervals_1', 'stimOff_times', 'goCueTrigger_times',\n","            'stimOn_times', 'response_times', 'goCue_times', 'firstMovement_times',\n","            'feedback_times'\n","        ]\n","\n","        if self.verbose:\n","            print(f\"Initializing IBL DataLoader for experiment: {eid}\")\n","\n","    def loadSessionData(self) -> bool:\n","        \"\"\"\n","        Load all required session data using SessionLoader.\n","\n","        Returns:\n","            bool: True if loading successful, False otherwise\n","        \"\"\"\n","        try:\n","            if self.verbose:\n","                print(\"Loading session data...\")\n","\n","            # Initialize SessionLoader\n","            self.sessLoader = SessionLoader(one=self.one, eid=self.eid)\n","\n","            # Load all required data types\n","            self.sessLoader.load_session_data(\n","                trials=True,           # Task timing events\n","                wheel=True,            # Wheel movement data\n","                pose=True,             # Pose tracking (all cameras)\n","                motion_energy=True,    # Motion energy (all cameras)\n","                pupil=True,            # Pupil diameter measurements\n","                reload=False\n","            )\n","\n","            if self.verbose:\n","                print(\"✓ Session data loaded successfully\")\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"✗ Error loading session data: {str(e)}\")\n","            return False\n","\n","    def extractSessionInfo(self) -> Dict:\n","        \"\"\"\n","        Extract comprehensive session metadata with robust path handling.\n","\n","        Returns:\n","            Dict: Session information including subject, date, lab, etc.\n","        \"\"\"\n","        try:\n","            if self.verbose:\n","                print(\"Extracting session information...\")\n","\n","            sessionDetails = self.one.get_details(self.eid)\n","\n","            # Handle different path object types that ONE API might return\n","            sessionPath = sessionDetails.get('local_path', 'Unknown')\n","\n","            # Convert path object to string if needed\n","            if hasattr(sessionPath, '__str__'):\n","                sessionPathStr = str(sessionPath)\n","            elif hasattr(sessionPath, 'as_posix'):\n","                sessionPathStr = sessionPath.as_posix()\n","            else:\n","                sessionPathStr = sessionPath\n","\n","            if self.verbose:\n","                print(f\"  Raw session path: {sessionPathStr}\")\n","\n","            # Parse path components for session metadata with defensive splitting\n","            pathParts = sessionPathStr.replace('\\\\', '/').split('/')  # Handle both Unix and Windows paths\n","            pathParts = [part for part in pathParts if part]  # Remove empty parts\n","\n","            # Extract session components with fallback logic\n","            if len(pathParts) >= 3:\n","                subject = pathParts[-3]\n","                date = pathParts[-2]\n","                sessionNum = pathParts[-1]\n","            elif len(pathParts) >= 2:\n","                subject = pathParts[-2]\n","                date = pathParts[-1]\n","                sessionNum = 'Unknown'\n","            elif len(pathParts) >= 1:\n","                subject = pathParts[-1]\n","                date = 'Unknown'\n","                sessionNum = 'Unknown'\n","            else:\n","                subject = 'Unknown'\n","                date = 'Unknown'\n","                sessionNum = 'Unknown'\n","\n","            # Alternative extraction method using session details if path parsing fails\n","            if subject == 'Unknown' and 'subject' in sessionDetails:\n","                subject = sessionDetails['subject']\n","            if date == 'Unknown' and 'start_time' in sessionDetails:\n","                # Extract date from start_time if available\n","                startTime = sessionDetails['start_time']\n","                if hasattr(startTime, 'date'):\n","                    date = startTime.date().isoformat()\n","                elif isinstance(startTime, str) and len(startTime) >= 10:\n","                    date = startTime[:10]  # Extract YYYY-MM-DD portion\n","\n","            # Get accurate trial count using SessionLoader\n","            try:\n","                if self.sessLoader is None:\n","                    temp_sess_loader = SessionLoader(one=self.one, eid=self.eid)\n","                    temp_sess_loader.load_trials()\n","                    numberOfTrials = temp_sess_loader.trials.shape[0]\n","                else:\n","                    # Use existing sessLoader if available\n","                    if hasattr(self.sessLoader, 'trials') and self.sessLoader.trials is not None:\n","                        numberOfTrials = self.sessLoader.trials.shape[0]\n","                    else:\n","                        self.sessLoader.load_trials()\n","                        numberOfTrials = self.sessLoader.trials.shape[0]\n","            except Exception as e:\n","                if self.verbose:\n","                    print(f\"  Warning: Could not load trial count via SessionLoader: {e}\")\n","                numberOfTrials = sessionDetails.get('n_trials', 'Unknown')\n","\n","            self.sessionInfo = {\n","                'experimentId': self.eid,\n","                'subject': subject,\n","                'date': date,\n","                'sessionNumber': sessionNum,\n","                'lab': sessionDetails.get('lab', 'Unknown'),\n","                'taskProtocol': sessionDetails.get('task_protocol', 'Unknown'),\n","                'numberOfTrials': numberOfTrials,\n","                'sessionPath': sessionPathStr,\n","                'dataRepository': sessionDetails.get('data_repository', 'Unknown')\n","            }\n","\n","            if self.verbose:\n","                print(\"✓ Session info extracted:\")\n","                for key, value in self.sessionInfo.items():\n","                    print(f\"  {key}: {value}\")\n","\n","            return self.sessionInfo\n","\n","        except Exception as e:\n","            print(f\"✗ Error extracting session info: {str(e)}\")\n","            if self.verbose:\n","                print(f\"  Session details keys available: {list(sessionDetails.keys()) if 'sessionDetails' in locals() else 'None'}\")\n","\n","            # Return minimal info with experiment ID even if extraction fails\n","            self.sessionInfo = {\n","                'experimentId': self.eid,\n","                'subject': 'Unknown',\n","                'date': 'Unknown',\n","                'sessionNumber': 'Unknown',\n","                'lab': 'Unknown',\n","                'taskProtocol': 'Unknown',\n","                'numberOfTrials': 'Unknown',\n","                'sessionPath': 'Unknown',\n","                'dataRepository': 'Unknown'\n","            }\n","            return self.sessionInfo\n","\n","    def validateAndExtractTrialData(self) -> bool:\n","        \"\"\"\n","        Validate and extract trial data with comprehensive feature checking.\n","\n","        Returns:\n","            bool: True if validation successful, False otherwise\n","        \"\"\"\n","        try:\n","            if self.verbose:\n","                print(\"Validating and extracting trial data...\")\n","\n","            if not hasattr(self.sessLoader, 'trials') or self.sessLoader.trials is None:\n","                print(\"✗ No trial data available\")\n","                return False\n","\n","            self.trialData = self.sessLoader.trials\n","\n","            # Check for required trial features\n","            availableFeatures = list(self.trialData.columns)\n","            missingFeatures = [f for f in self.requiredTrialFeatures if f not in availableFeatures]\n","\n","            if self.verbose:\n","                print(f\"✓ Trial data loaded: {len(self.trialData)} trials\")\n","                print(f\"  Available features: {len(availableFeatures)}\")\n","                if missingFeatures:\n","                    print(f\"  Missing features: {missingFeatures}\")\n","                else:\n","                    print(\"  ✓ All required trial features present\")\n","\n","                # Show trial interval statistics\n","                if 'intervals_0' in availableFeatures and 'intervals_1' in availableFeatures:\n","                    intervalDurations = self.trialData['intervals_1'] - self.trialData['intervals_0']\n","                    print(f\"  Trial duration stats:\")\n","                    print(f\"    Mean: {intervalDurations.mean():.2f}s\")\n","                    print(f\"    Min: {intervalDurations.min():.2f}s\")\n","                    print(f\"    Max: {intervalDurations.max():.2f}s\")\n","\n","            return len(missingFeatures) == 0\n","\n","        except Exception as e:\n","            print(f\"✗ Error validating trial data: {str(e)}\")\n","            return False\n","\n","    def validateAndExtractPoseData(self) -> bool:\n","        \"\"\"\n","        Validate and extract left camera pose tracking data.\n","\n","        Returns:\n","            bool: True if validation successful, False otherwise\n","        \"\"\"\n","        try:\n","            if self.verbose:\n","                print(\"Validating and extracting pose data...\")\n","\n","            if not hasattr(self.sessLoader, 'pose') or 'leftCamera' not in self.sessLoader.pose:\n","                print(\"✗ No left camera pose data available\")\n","                return False\n","\n","            self.poseData = self.sessLoader.pose['leftCamera']\n","\n","            # Check for required pose features\n","            availableFeatures = list(self.poseData.columns)\n","            missingFeatures = [f for f in self.requiredPoseFeatures if f not in availableFeatures]\n","\n","            if self.verbose:\n","                print(f\"✓ Pose data loaded: {len(self.poseData)} timepoints\")\n","                print(f\"  Available features: {len(availableFeatures)}\")\n","                if missingFeatures:\n","                    print(f\"  Missing features: {missingFeatures}\")\n","                else:\n","                    print(\"  ✓ All required pose features present\")\n","\n","                # Show data completeness for each feature\n","                print(\"  Data completeness check:\")\n","                for feature in self.requiredPoseFeatures:\n","                    if feature in availableFeatures:\n","                        nonNullCount = self.poseData[feature].notna().sum()\n","                        totalCount = len(self.poseData)\n","                        completeness = (nonNullCount / totalCount) * 100\n","                        print(f\"    {feature}: {nonNullCount}/{totalCount} ({completeness:.1f}%)\")\n","\n","                # Show temporal coverage\n","                if 'times' in availableFeatures:\n","                    timeRange = self.poseData['times'].max() - self.poseData['times'].min()\n","                    print(f\"  Temporal coverage: {timeRange:.2f} seconds\")\n","\n","            return len(missingFeatures) == 0\n","\n","        except Exception as e:\n","            print(f\"✗ Error validating pose data: {str(e)}\")\n","            return False\n","\n","    def extractOtherBehavioralData(self) -> bool:\n","        \"\"\"\n","        Extract wheel, motion energy, and pupil data using SessionLoader methods.\n","        Reports what features are tracked across all cameras (left, right, body).\n","\n","        Returns:\n","            bool: True if extraction successful, False otherwise\n","        \"\"\"\n","        try:\n","            if self.verbose:\n","                print(\"Extracting additional behavioral data...\")\n","\n","            # Load wheel data\n","            try:\n","                self.sessLoader.load_wheel()\n","                if hasattr(self.sessLoader, 'wheel') and self.sessLoader.wheel is not None:\n","                    self.wheelData = self.sessLoader.wheel\n","                    if self.verbose:\n","                        print(f\"✓ Wheel data: {len(self.wheelData)} timepoints\")\n","                else:\n","                    if self.verbose:\n","                        print(\"! No wheel data available\")\n","            except Exception as e:\n","                if self.verbose:\n","                    print(f\"! Failed to load wheel data: {e}\")\n","\n","            # Load pose data for all cameras (left, right, body)\n","            try:\n","                self.sessLoader.load_pose(views=['left', 'right', 'body'])\n","                if hasattr(self.sessLoader, 'pose'):\n","                    cameras = ['leftCamera', 'rightCamera', 'bodyCamera']\n","                    available_cameras = [cam for cam in cameras if cam in self.sessLoader.pose]\n","\n","                    if self.verbose:\n","                        print(f\"✓ Pose data loaded for cameras: {available_cameras}\")\n","\n","                    # Check what features are tracked across cameras\n","                    if available_cameras:\n","                        # Get features from each camera\n","                        camera_features = {}\n","                        for cam in available_cameras:\n","                            if self.sessLoader.pose[cam] is not None:\n","                                camera_features[cam] = set(self.sessLoader.pose[cam].columns)\n","\n","                        if camera_features and self.verbose:\n","                            # Find common features across all cameras\n","                            common_features = set.intersection(*camera_features.values()) if len(camera_features) > 1 else next(iter(camera_features.values()))\n","\n","                            print(f\"  Feature analysis across {len(camera_features)} cameras:\")\n","                            print(f\"    Common features ({len(common_features)}): {sorted(common_features)}\")\n","\n","                            for cam, features in camera_features.items():\n","                                unique_features = features - common_features\n","                                if unique_features:\n","                                    print(f\"    {cam} unique features ({len(unique_features)}): {sorted(unique_features)}\")\n","\n","                            # Report on pupil features specifically\n","                            pupil_features = {cam: [f for f in features if f.startswith('pupil_')]\n","                                            for cam, features in camera_features.items()}\n","                            pupil_available = {cam: features for cam, features in pupil_features.items() if features}\n","\n","                            if pupil_available:\n","                                print(f\"    Pupil tracking available in: {list(pupil_available.keys())}\")\n","                                for cam, features in pupil_available.items():\n","                                    print(f\"      {cam}: {features}\")\n","\n","                        # Set primary data sources\n","                        if 'leftCamera' in self.sessLoader.pose:\n","                            self.poseData = self.sessLoader.pose['leftCamera']\n","                            self.pupilData = self.sessLoader.pose['leftCamera']  # Pupil data is part of pose\n","\n","                else:\n","                    if self.verbose:\n","                        print(\"! No pose data available\")\n","            except Exception as e:\n","                if self.verbose:\n","                    print(f\"! Failed to load pose data: {e}\")\n","\n","            # Load motion energy data for all cameras\n","            try:\n","                self.sessLoader.load_motion_energy(views=['left', 'right', 'body'])\n","                if hasattr(self.sessLoader, 'motion_energy'):\n","                    cameras = ['leftCamera', 'rightCamera', 'bodyCamera']\n","                    available_cameras = [cam for cam in cameras if cam in self.sessLoader.motion_energy]\n","\n","                    if self.verbose:\n","                        print(f\"✓ Motion energy data loaded for cameras: {available_cameras}\")\n","\n","                    # Check what motion features are available\n","                    if available_cameras:\n","                        for cam in available_cameras:\n","                            motion_data = self.sessLoader.motion_energy[cam]\n","                            if isinstance(motion_data, dict):\n","                                features = list(motion_data.keys())\n","                            else:\n","                                features = ['whiskerMotionEnergy']\n","\n","                            if self.verbose:\n","                                print(f\"    {cam}: {features}\")\n","\n","                        # Set primary motion energy source\n","                        if 'leftCamera' in self.sessLoader.motion_energy:\n","                            self.motionEnergyData = self.sessLoader.motion_energy['leftCamera']\n","                else:\n","                    if self.verbose:\n","                        print(\"! No motion energy data available\")\n","            except Exception as e:\n","                if self.verbose:\n","                    print(f\"! Failed to load motion energy data: {e}\")\n","\n","            # Load pupil diameter data (separate from pose)\n","            try:\n","                self.sessLoader.load_pupil()\n","                if hasattr(self.sessLoader, 'pupil') and self.sessLoader.pupil is not None:\n","                    cameras = ['leftCamera', 'rightCamera', 'bodyCamera']\n","                    available_cameras = [cam for cam in cameras if cam in self.sessLoader.pupil]\n","\n","                    if available_cameras and self.verbose:\n","                        print(f\"✓ Dedicated pupil data loaded for cameras: {available_cameras}\")\n","                else:\n","                    if self.verbose:\n","                        print(\"! No dedicated pupil data (pupil tracking is in pose data)\")\n","            except Exception as e:\n","                if self.verbose:\n","                    print(f\"! Failed to load dedicated pupil data: {e}\")\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"✗ Error extracting behavioral data: {str(e)}\")\n","            return False\n","\n","    def loadVideoTimestamps(self) -> bool:\n","        \"\"\"\n","        Load video timestamps for left, body, and right cameras.\n","\n","        Performs two key operations:\n","        1. Aligns camera times to trial intervals (intervals_0 to intervals_1)\n","        2. Standardizes all cameras to 24 fps using frame decimation/upsampling:\n","           - For cameras > 24fps (e.g., 120fps): Take every Nth frame (120fps → take every 5th frame)\n","           - For cameras < 24fps (e.g., 30fps): Use frame repetition/interpolation to upsample\n","           - For cameras = 24fps: Use frames as-is\n","\n","        Frame decimation examples:\n","        - 30fps → 24fps: Keep 4 out of every 5 frames (30/24 = 1.25, so skip every 5th frame)\n","        - 60fps → 24fps: Keep 2 out of every 5 frames (60/24 = 2.5, so take every 2.5th frame)\n","        - 120fps → 24fps: Keep 1 out of every 5 frames (120/24 = 5, so take every 5th frame)\n","\n","        Returns:\n","            bool: True if loading successful, False otherwise\n","        \"\"\"\n","        try:\n","            if self.verbose:\n","                print(\"Loading video timestamps...\")\n","\n","            eid = self.eid\n","            one = self.one\n","            labels = ['left', 'right', 'body']\n","\n","            # Get all video URLs at once for efficiency\n","            video_urls = video.url_from_eid(eid, one=one)\n","            urls = {label: video_urls[label] for label in labels}\n","\n","            # Load timestamps for each camera\n","            timestamps = {}\n","            for label in labels:\n","                timestamps[label] = one.load_dataset(eid, f'*{label}Camera.times*', collection='alf')\n","\n","            # Report on video frame counts (they may differ)\n","            if self.verbose:\n","                frame_counts = {label: len(timestamps[label]) for label in labels}\n","                print(f\"Video frame counts: {frame_counts}\")\n","                if len(set(frame_counts.values())) == 1:\n","                    print(\"✓ All cameras have the same number of frames\")\n","                else:\n","                    print(\"! Cameras have different numbers of frames (this is normal)\")\n","\n","            self.urls = urls\n","            self.videoTimes = timestamps['left'] # default assigns video times to left camera\n","\n","            # Get actual video durations and calculate original FPS for each camera\n","            original_fps = {}\n","            video_durations = {}\n","\n","            for label in labels:\n","                try:\n","                    # Get actual video metadata for duration\n","                    meta = video.get_video_meta(urls[label], one=one)\n","                    actual_duration = meta.get('duration')\n","\n","                    # Handle datetime.timedelta objects\n","                    if hasattr(actual_duration, 'total_seconds'):\n","                        actual_duration = actual_duration.total_seconds()\n","\n","                    video_durations[label] = actual_duration\n","\n","                    # Calculate FPS using actual video duration\n","                    num_frames = len(timestamps[label])\n","                    if actual_duration and actual_duration > 0:\n","                        calculated_fps = num_frames / actual_duration\n","                        original_fps[label] = round(calculated_fps)  # Round to nearest integer\n","                        if self.verbose:\n","                            print(f\"  {label}: {num_frames} frames / {actual_duration:.2f}s = {calculated_fps:.2f} fps → rounded to {original_fps[label]} fps\")\n","                    else:\n","                        # Fallback to timestamp range if metadata fails\n","                        timestamp_duration = timestamps[label][-1] - timestamps[label][0]\n","                        calculated_fps = num_frames / timestamp_duration\n","                        original_fps[label] = round(calculated_fps)\n","                        video_durations[label] = timestamp_duration\n","                        if self.verbose:\n","                            print(f\"  {label}: Using timestamp duration fallback - {calculated_fps:.2f} fps → rounded to {original_fps[label]} fps\")\n","\n","                except Exception as e:\n","                    # Fallback to timestamp-based calculation\n","                    timestamp_duration = timestamps[label][-1] - timestamps[label][0]\n","                    num_frames = len(timestamps[label])\n","                    calculated_fps = num_frames / timestamp_duration\n","                    original_fps[label] = round(calculated_fps)\n","                    video_durations[label] = timestamp_duration\n","                    if self.verbose:\n","                        print(f\"  {label}: Metadata failed ({e}), using timestamp fallback - {calculated_fps:.2f} fps → rounded to {original_fps[label]} fps\")\n","\n","            # CORRECTED APPROACH: Preserve original camera timing instead of forcing standardization\n","            if self.verbose:\n","                print(f\"\\nPreserving original camera timing (NO frame rate standardization):\")\n","                print(\"  This maintains natural timing relationships between cameras\")\n","                for label in labels:\n","                    original_times = timestamps[label]\n","                    original_camera_fps = original_fps[label]\n","                    print(f\"  {label}: {len(original_times)} frames at {original_camera_fps:.1f} fps (PRESERVED)\")\n","\n","            # Store results with original timing preserved\n","            self.fps = {\n","                'original': original_fps,\n","                'preserved': original_fps,  # Same as original - no standardization\n","                'target': None  # No target fps - respect original rates\n","            }\n","\n","            # Use left camera as master timeline but preserve all original timestamps\n","            self.videoTimes = timestamps['left']  # Master timeline for animation\n","            self.originalTimestamps = timestamps  # Store ALL original timestamps\n","\n","            # Remove the problematic standardizedTimestamps that destroy timing\n","            # Instead, provide raw access to original timestamps for each camera\n","\n","            if self.verbose:\n","                print(f\"\\n✓ Video processing completed:\")\n","                print(f\"  Original timestamps preserved for all cameras\")\n","                print(f\"  No frame rate standardization applied\")\n","                print(f\"  Primary video time base: left camera ({len(self.videoTimes)} frames)\")\n","                print(f\"✓ URLs loaded for all cameras\")\n","\n","            return True\n","        except Exception as e:\n","            print(f\"✗ Error loading video timestamps and or urls: {str(e)}\")\n","            return False\n","\n","    def sampleInterval(self, trialIdx: int) -> Optional[Dict]:\n","        \"\"\"\n","        Sample a specific trial interval from the available trials.\n","\n","        Returns:\n","            Dict: Sampled interval data with trial info, or None if sampling fails\n","        \"\"\"\n","        try:\n","            if self.trialData is None or len(self.trialData) == 0:\n","                print(\"✗ No trial data available for sampling\")\n","                return None\n","            print(f\"There are {len(self.trialData)} trials available\")\n","            print(f\"Sampling trial {trialIdx}\")\n","\n","            selectedTrial = self.trialData.iloc[trialIdx]\n","            intervalStart = selectedTrial['intervals_0']\n","            intervalEnd = selectedTrial['intervals_1']\n","            intervalDuration = intervalEnd - intervalStart\n","\n","            if self.verbose:\n","                print(f\" Successfully sampled trial {trialIdx}, Interval: {intervalStart:.3f} - {intervalEnd:.3f}s\")\n","\n","            # Extract synchronized pose data for this interval\n","            poseInInterval = self.extractPoseDataForInterval(intervalStart, intervalEnd)\n","            intervalData = {\n","                'trialIndex': trialIdx,\n","                'intervalStart': intervalStart,\n","                'intervalEnd': intervalEnd,\n","                'duration': intervalDuration,\n","                'trialEvents': selectedTrial.to_dict(),\n","                'poseData': poseInInterval,\n","                'sessionInfo': self.sessionInfo\n","            }\n","\n","            return intervalData\n","        except Exception as e:\n","            print(f\"Error sampling interval: {str(e)}\")\n","            return None\n","\n","    def extractPoseDataForInterval(self, startTime: float, endTime: float) -> Optional[pd.DataFrame]:\n","        \"\"\"\n","        Extract pose data that falls within the specified time interval.\n","        Handles missing data by preserving original timestamps for linear interpolation.\n","\n","        Args:\n","            startTime (float): Start time of interval in seconds\n","            endTime (float): End time of interval in seconds\n","\n","        Returns:\n","            pd.DataFrame: Pose data within the time interval, or None if extraction fails\n","        \"\"\"\n","        try:\n","            if self.poseData is None or 'times' not in self.poseData.columns:\n","                print(\"✗ No pose data or timestamps available\")\n","                return None\n","\n","            # Find pose data within the time interval\n","            timeFilter = (self.poseData['times'] >= startTime) & (self.poseData['times'] <= endTime)\n","            intervalPoseData = self.poseData[timeFilter].copy()\n","\n","            if len(intervalPoseData) == 0:\n","                print(\"! No pose data in specified interval\")\n","                return None\n","\n","            # Calculate data availability statistics for this interval\n","            if self.verbose:\n","                print(f\"  Pose data in interval: {len(intervalPoseData)} timepoints\")\n","\n","                # Show feature availability for this specific interval\n","                featureStats = {}\n","                for feature in self.requiredPoseFeatures:\n","                    if feature in intervalPoseData.columns and feature != 'times':\n","                        validCount = intervalPoseData[feature].notna().sum()\n","                        totalCount = len(intervalPoseData)\n","                        completeness = (validCount / totalCount) * 100\n","                        featureStats[feature] = {\n","                            'valid_points': validCount,\n","                            'total_points': totalCount,\n","                            'completeness': completeness\n","                        }\n","\n","                # Show summary of feature availability\n","                highQualityFeatures = [f for f, stats in featureStats.items() if stats['completeness'] > 80]\n","                mediumQualityFeatures = [f for f, stats in featureStats.items() if 20 <= stats['completeness'] <= 80]\n","                lowQualityFeatures = [f for f, stats in featureStats.items() if stats['completeness'] < 20]\n","\n","                print(f\"    High quality features (>80%): {len(highQualityFeatures)}\")\n","                print(f\"    Medium quality features (20-80%): {len(mediumQualityFeatures)}\")\n","                print(f\"    Low quality features (<20%): {len(lowQualityFeatures)}\")\n","\n","                if lowQualityFeatures and self.verbose:\n","                    print(f\"    Low quality features will rely on interpolation: {lowQualityFeatures[:3]}{'...' if len(lowQualityFeatures) > 3 else ''}\")\n","\n","            return intervalPoseData\n","\n","        except Exception as e:\n","            print(f\"✗ Error extracting pose data for interval: {str(e)}\")\n","            return None\n","\n","    def extractWheelDataForInterval(self, startTime: float, endTime: float) -> Optional[Dict]:\n","        \"\"\"\n","        Extract wheel data that falls within the specified time interval.\n","        Uses sess_loader.wheel which has columns: ['times', 'position', 'velocity', 'acceleration'].\n","\n","        Args:\n","            startTime (float): Start time of interval in seconds\n","            endTime (float): End time of interval in seconds\n","\n","        Returns:\n","            Dict: Wheel data within the time interval with times and position_raw, or None if extraction fails\n","        \"\"\"\n","        try:\n","            if self.wheelData is None:\n","                if self.verbose:\n","                    print(\"✗ No wheel data available\")\n","                return None\n","\n","            # Check if wheelData has required columns\n","            if not hasattr(self.wheelData, 'columns') or 'times' not in self.wheelData.columns or 'position' not in self.wheelData.columns:\n","                if self.verbose:\n","                    print(\"✗ Wheel data missing required columns (times, position)\")\n","                return None\n","\n","            # Filter wheel data within the time interval\n","            timeFilter = (self.wheelData['times'] >= startTime) & (self.wheelData['times'] <= endTime)\n","            intervalWheelData = self.wheelData[timeFilter].copy()\n","\n","            if len(intervalWheelData) == 0:\n","                if self.verbose:\n","                    print(\"! No wheel data in specified interval\")\n","                return None\n","\n","            # Return wheel data in the format expected by AnimationRenderer\n","            wheelData = {\n","                'times': intervalWheelData['times'].values,\n","                'position_raw': intervalWheelData['position'].values,\n","            }\n","\n","            if self.verbose:\n","                print(f\"✓ Wheel data extracted: {len(intervalWheelData)} timepoints\")\n","                print(f\"  Time range: {wheelData['times'][0]:.3f} - {wheelData['times'][-1]:.3f}s\")\n","                print(f\"  Position range: {np.nanmin(wheelData['position_raw']):.6f} - {np.nanmax(wheelData['position_raw']):.6f}\")\n","                print(f\"  Valid positions: {np.sum(~np.isnan(wheelData['position_raw']))}\")\n","\n","            return wheelData\n","\n","        except Exception as e:\n","            if self.verbose:\n","                print(f\"✗ Error extracting wheel data for interval: {str(e)}\")\n","            return None\n","\n","    def extractMotionEnergyForInterval(self, startTime: float, endTime: float) -> Optional[Dict]:\n","        \"\"\"\n","        Extract motion energy data that falls within the specified time interval.\n","        Uses sess_loader.motion_energy['leftCamera'] structure as user indicated.\n","\n","        Args:\n","            startTime (float): Start time of interval in seconds\n","            endTime (float): End time of interval in seconds\n","\n","        Returns:\n","            Dict: Motion energy data within the time interval, or None if extraction fails\n","        \"\"\"\n","        try:\n","            if self.motionEnergyData is None:\n","                if self.verbose:\n","                    print(\"✗ No motion energy data available\")\n","                return None\n","\n","            if self.verbose:\n","                print(f\"Motion energy data type: {type(self.motionEnergyData)}\")\n","                if hasattr(self.motionEnergyData, 'columns'):\n","                    print(f\"Motion energy DataFrame columns: {list(self.motionEnergyData.columns)}\")\n","                elif isinstance(self.motionEnergyData, dict):\n","                    print(f\"Motion energy keys: {list(self.motionEnergyData.keys())}\")\n","\n","            # Extract whisker motion energy and timestamps\n","            # Based on user info: sess_loader.motion_energy['leftCamera'].columns\n","            whiskerData = None\n","            timestamps = None\n","\n","            # Case 1: DataFrame with columns (most likely structure)\n","            if hasattr(self.motionEnergyData, 'columns'):\n","                df = self.motionEnergyData\n","\n","                # Look for whisker motion energy column\n","                whisker_col = None\n","                for col in df.columns:\n","                    col_str = str(col).lower()\n","                    if 'whisker' in col_str and ('motion' in col_str or 'energy' in col_str):\n","                        whisker_col = col\n","                        break\n","                    elif 'motionenergy' in col_str.replace('_', '').replace(' ', ''):\n","                        whisker_col = col\n","                        break\n","\n","                if whisker_col is None and len(df.columns) > 0:\n","                    # Use first column as fallback\n","                    whisker_col = df.columns[0]\n","                    if self.verbose:\n","                        print(f\"No clear whisker column found, using first column: '{whisker_col}'\")\n","\n","                if whisker_col is not None:\n","                    whiskerData = df[whisker_col]\n","                    if self.verbose:\n","                        print(f\"Using column '{whisker_col}' for whisker motion energy\")\n","\n","                # Get timestamps - check for 'times' column or use index\n","                if 'times' in df.columns:\n","                    timestamps = df['times']\n","                elif hasattr(df.index, 'values'):\n","                    timestamps = df.index.values\n","                else:\n","                    timestamps = df.index\n","\n","            # Case 2: Dictionary structure\n","            elif isinstance(self.motionEnergyData, dict):\n","                # Look for whisker motion energy in dictionary\n","                possible_keys = ['whiskerMotionEnergy', 'whisker', 'motion_energy', 'leftCamera']\n","                for key in possible_keys:\n","                    if key in self.motionEnergyData:\n","                        whiskerData = self.motionEnergyData[key]\n","                        break\n","\n","                # Look for timestamps\n","                if 'times' in self.motionEnergyData:\n","                    timestamps = self.motionEnergyData['times']\n","                elif hasattr(whiskerData, 'index'):\n","                    timestamps = whiskerData.index\n","\n","            # Case 3: Direct Series/Array\n","            else:\n","                whiskerData = self.motionEnergyData\n","                if hasattr(whiskerData, 'index'):\n","                    timestamps = whiskerData.index\n","\n","            if whiskerData is None:\n","                if self.verbose:\n","                    print(\"✗ No whisker motion energy data found\")\n","                return None\n","\n","            if timestamps is None:\n","                if self.verbose:\n","                    print(\"✗ No timestamps found for motion energy data\")\n","                return None\n","\n","            # Convert to numpy arrays for filtering\n","            if hasattr(timestamps, 'values'):\n","                timestamps_array = timestamps.values\n","            else:\n","                timestamps_array = np.asarray(timestamps)\n","\n","            if hasattr(whiskerData, 'values'):\n","                whisker_array = whiskerData.values\n","            else:\n","                whisker_array = np.asarray(whiskerData)\n","\n","            # Filter data for time interval\n","            timeFilter = (timestamps_array >= startTime) & (timestamps_array <= endTime)\n","            intervalTimestamps = timestamps_array[timeFilter]\n","            intervalWhiskerData = whisker_array[timeFilter]\n","\n","            if len(intervalTimestamps) == 0:\n","                if self.verbose:\n","                    print(\"! No motion energy data in specified interval\")\n","                return None\n","\n","            # Ensure whisker data is 1D\n","            intervalWhiskerData = np.asarray(intervalWhiskerData).flatten()\n","\n","            # Apply smoothing\n","            smoothingWindow = 0.05\n","            if len(intervalTimestamps) > 1:\n","                estimatedSamplingRate = 1.0 / np.median(np.diff(intervalTimestamps))\n","                windowSamples = max(1, int(smoothingWindow * estimatedSamplingRate))\n","            else:\n","                windowSamples = 1\n","                estimatedSamplingRate = None\n","\n","            import pandas as pd\n","            whiskerSmoothed = pd.Series(intervalWhiskerData).rolling(\n","                window=windowSamples, center=True, min_periods=1\n","            ).mean().values\n","\n","            motionData = {\n","                'times': intervalTimestamps,\n","                'whiskerMotionEnergy_raw': intervalWhiskerData,\n","                'whiskerMotionEnergy_smoothed': whiskerSmoothed,\n","                'smoothing_window': smoothingWindow,\n","                'estimated_sampling_rate': estimatedSamplingRate\n","            }\n","\n","            if self.verbose:\n","                print(f\"✓ Motion energy extracted: {len(intervalTimestamps)} timepoints\")\n","                print(f\"  Time range: {intervalTimestamps[0]:.3f} - {intervalTimestamps[-1]:.3f}s\")\n","                print(f\"  Data range: {np.nanmin(intervalWhiskerData):.3f} - {np.nanmax(intervalWhiskerData):.3f}\")\n","\n","            return motionData\n","\n","        except Exception as e:\n","            if self.verbose:\n","                print(f\"✗ Error extracting motion energy data: {str(e)}\")\n","                import traceback\n","                traceback.print_exc()\n","            return None\n","\n","    def prepareDataForAnimation(self, intervalData: Dict) -> Optional[Dict]:\n","        \"\"\"\n","        Extract all behavioral data for interval and prepare for animation with consistent time base.\n","        Combines wheel, motion energy extraction and animation data preparation in one method.\n","\n","        Args:\n","            intervalData (Dict): Raw interval data from sampleInterval()\n","\n","        Returns:\n","            Dict: Animation-ready data with synchronized time bases, or None if preparation fails\n","        \"\"\"\n","        try:\n","            if self.verbose:\n","                print(\"Preparing data for animation...\")\n","\n","            startTime = intervalData['intervalStart']\n","            endTime = intervalData['intervalEnd']\n","\n","            # Extract wheel data for this interval using dedicated method\n","            wheelData = self.extractWheelDataForInterval(startTime, endTime)\n","\n","            # Extract motion energy data for this interval using dedicated method\n","            motionData = self.extractMotionEnergyForInterval(startTime, endTime)\n","\n","            # Prepare animation data structure\n","            animationData = {\n","                'metadata': intervalData['sessionInfo'],\n","                'trial_info': {\n","                    'trial_index': intervalData['trialIndex'],\n","                    'start_time': startTime,\n","                    'end_time': endTime,\n","                    'duration': intervalData['duration'],\n","                    'events': intervalData['trialEvents']\n","                },\n","                'synchronized_data': {}\n","            }\n","\n","            # Define common time base for animation (use video frame times as reference)\n","            if self.videoTimes is not None:\n","                videoTimeFilter = (self.videoTimes >= startTime) & (self.videoTimes <= endTime)\n","                animationTimeBase = self.videoTimes[videoTimeFilter]\n","\n","                if len(animationTimeBase) > 0:\n","                    animationData['synchronized_data']['video_times'] = animationTimeBase\n","                    animationData['synchronized_data']['video_frame_indices'] = np.where(videoTimeFilter)[0]\n","\n","                    if self.verbose:\n","                        print(f\"  Video time base: {len(animationTimeBase)} frames\")\n","                        print(f\"  Frame rate: {len(animationTimeBase) / intervalData['duration']:.1f} fps\")\n","                else:\n","                    print(\"! No video frames found in interval\")\n","                    return None\n","            else:\n","                print(\"✗ No video timestamps available for synchronization\")\n","                return None\n","\n","            # Add pose data with original timestamps (for linear interpolation during animation)\n","            if intervalData['poseData'] is not None:\n","                animationData['synchronized_data']['pose'] = {\n","                    'times': intervalData['poseData']['times'].values,\n","                    'features': {}\n","                }\n","\n","                # Store each pose feature separately for easy access during animation\n","                for feature in self.requiredPoseFeatures:\n","                    if feature in intervalData['poseData'].columns and feature != 'times':\n","                        featureData = intervalData['poseData'][feature].values\n","                        validMask = ~np.isnan(featureData)\n","\n","                        animationData['synchronized_data']['pose']['features'][feature] = {\n","                            'values': featureData,\n","                            'valid_mask': validMask,\n","                            'valid_times': intervalData['poseData']['times'].values[validMask],\n","                            'valid_values': featureData[validMask]\n","                        }\n","\n","            # Add wheel data (position-focused)\n","            if wheelData is not None:\n","                animationData['synchronized_data']['wheel'] = {\n","                    'times': wheelData['times'],\n","                    'position_raw': wheelData['position_raw'],\n","                }\n","\n","            # Add motion energy data\n","            if motionData is not None:\n","                animationData['synchronized_data']['motion_energy'] = {\n","                    'times': motionData['times'],\n","                    'whisker_raw': motionData['whiskerMotionEnergy_raw'],\n","                    'whisker_smoothed': motionData['whiskerMotionEnergy_smoothed'],\n","                    'smoothing_window': motionData['smoothing_window'],\n","                    'sampling_rate': motionData.get('estimated_sampling_rate', None)\n","                }\n","\n","            if self.verbose:\n","                print(\"✓ Animation data preparation completed\")\n","                print(f\"  Data streams prepared: {list(animationData['synchronized_data'].keys())}\")\n","\n","            return animationData\n","\n","        except Exception as e:\n","            print(f\"✗ Error preparing animation data: {str(e)}\")\n","            return None\n","\n","    def loadCompleteSession(self) -> bool:\n","        \"\"\"\n","        Complete workflow to load and validate all session data.\n","\n","        Returns:\n","            bool: True if all loading steps successful, False otherwise\n","        \"\"\"\n","        if self.verbose:\n","            print(\"=\"*60)\n","            print(\"STARTING COMPLETE SESSION LOADING\")\n","            print(\"=\"*60)\n","\n","        steps = [\n","            (\"Loading session data\", self.loadSessionData),\n","            (\"Extracting session info\", lambda: self.extractSessionInfo() != {}),\n","            (\"Validating trial data\", self.validateAndExtractTrialData),\n","            (\"Validating pose data\", self.validateAndExtractPoseData),\n","            (\"Extracting behavioral data\", self.extractOtherBehavioralData),\n","            (\"Loading video timestamps\", self.loadVideoTimestamps)\n","        ]\n","\n","        allSuccessful = True\n","        for stepName, stepFunction in steps:\n","            if self.verbose:\n","                print(f\"\\n{stepName}...\")\n","\n","            success = stepFunction()\n","            if not success:\n","                print(f\"✗ Failed: {stepName}\")\n","                allSuccessful = False\n","            elif self.verbose:\n","                print(f\"✓ Completed: {stepName}\")\n","\n","        if self.verbose:\n","            print(\"\\n\" + \"=\"*60)\n","            if allSuccessful:\n","                print(\"SESSION LOADING COMPLETED SUCCESSFULLY\")\n","            else:\n","                print(\"SESSION LOADING COMPLETED WITH ERRORS\")\n","            print(\"=\"*60)\n","\n","        return allSuccessful"]},{"cell_type":"markdown","metadata":{"id":"kJx-lSAXWf3-"},"source":["# Animator Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EnNz0LiRkjin"},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","from matplotlib.animation import FuncAnimation\n","from matplotlib.lines import Line2D\n","from IPython.display import display, HTML\n","from datetime import datetime\n","from typing import Dict, List, Optional\n","import gc\n","\n","class IBLAnimationRenderer:\n","    \"\"\"\n","    High-performance animation renderer for IBL behavioral data with video synchronization.\n","    Displays full trial duration with moving time indicators for optimal performance and context.\n","    \"\"\"\n","\n","    def __init__(self, mode='test', baseDir=None, verbose=True, fps=24, one=None, eid=None, dataLoader=None, cameras=['left', 'right']):\n","        \"\"\"Initialize animation renderer with optional DataLoader integration.\"\"\"\n","        assert mode in ['test', 'save'], f\"Mode must be 'test' or 'save', got {mode}\"\n","\n","        # Validate camera selection\n","        valid_cameras = ['left', 'right', 'body']\n","        if not isinstance(cameras, list) or len(cameras) == 0 or len(cameras) > 2:\n","            raise ValueError(\"cameras must be a list of 1-2 camera names\")\n","        for cam in cameras:\n","            if cam not in valid_cameras:\n","                raise ValueError(f\"Invalid camera '{cam}'. Must be one of {valid_cameras}\")\n","\n","        self.mode = mode\n","        self.baseDir = baseDir or '.'\n","        self.verbose = verbose\n","        self.fps = fps\n","        self.one = one\n","        self.eid = eid\n","        self.dataLoader = dataLoader\n","        self.cameras = cameras  # Store selected cameras\n","\n","        # Animation elements\n","        self.lineObjects = {}\n","        self.timeIndicators = []\n","        self.videoImages = {}  # Dynamic camera video images\n","        self.timeTexts = {}    # Dynamic camera time texts\n","        self.axVideos = {}     # Dynamic camera axes\n","\n","        # Video loading (dynamic camera setup)\n","        self.videoUrls = {cam: None for cam in self.cameras}\n","        self.videoFrameCaches = {cam: {} for cam in self.cameras}\n","        self.cacheStartFrames = {cam: None for cam in self.cameras}\n","        self.chunkSize = 100\n","\n","        # Debug logging\n","        self.debugLog = []\n","\n","        # Initialize DataLoader integration if provided\n","        if self.dataLoader is not None:\n","            self._validateDataLoader()\n","            if self.verbose:\n","                subject = self.dataLoader.sessionInfo.get('subject', 'Unknown')\n","                print(f\"IBL Animation Renderer initialized with DataLoader for subject {subject}\")\n","        elif self.verbose:\n","            print(f\"IBL Animation Renderer initialized (Mode: {mode}, FPS: {fps})\")\n","\n","    def render(self, animationData: Dict) -> None:\n","        \"\"\"Create animation from prepared IBL data.\"\"\"\n","        try:\n","            self._extractMetadata(animationData)\n","            self._initializeVideoLoader()\n","            self._createFigure()\n","            self._initializeVideo()\n","            self._initializePlots(animationData)\n","            self._initializeEvents(animationData)\n","            self._initializeTimeIndicators()\n","            self._createAnimation()\n","        except Exception as e:\n","            if self.verbose:\n","                print(f\"Animation error: {str(e)}\")\n","            raise\n","        finally:\n","            self._cleanup()\n","\n","    def _extractMetadata(self, animationData: Dict) -> None:\n","        \"\"\"Extract essential timing and session information.\"\"\"\n","        if 'synchronized_data' not in animationData or 'video_times' not in animationData['synchronized_data']:\n","            raise ValueError(\"Missing video timing data\")\n","\n","        self.trialInfo = animationData['trial_info']\n","        self.sessionInfo = animationData['metadata']\n","        self.videoTimes = animationData['synchronized_data']['video_times']\n","        self.totalFrames = len(self.videoTimes)\n","        self.syncData = animationData['synchronized_data']\n","\n","        # CRITICAL DEBUG: Video/Graph Time Alignment\n","        if self.verbose:\n","            subject = self.sessionInfo.get('subject', 'Unknown')\n","            duration = self.trialInfo['duration']\n","            print(f\"Rendering {duration:.1f}s trial for subject {subject} ({self.totalFrames} frames)\")\n","\n","            print(f\"\\nDEBUG: TIME SYNCHRONIZATION ANALYSIS\")\n","            print(f\"  Trial interval: {self.trialInfo['start_time']:.3f} - {self.trialInfo['end_time']:.3f}s\")\n","            print(f\"  Video times range: {self.videoTimes[0]:.3f} - {self.videoTimes[-1]:.3f}s\")\n","            print(f\"  Video duration: {self.videoTimes[-1] - self.videoTimes[0]:.3f}s\")\n","            print(f\"  Total video frames: {self.totalFrames}\")\n","            print(f\"  Video frame rate: {self.totalFrames / (self.videoTimes[-1] - self.videoTimes[0]):.2f} fps\")\n","\n","            # Check if video times match trial interval\n","            if abs(self.videoTimes[0] - self.trialInfo['start_time']) > 0.1:\n","                print(f\"  WARNING: Video start time mismatch!\")\n","                print(f\"     Video starts at: {self.videoTimes[0]:.3f}s\")\n","                print(f\"     Trial starts at: {self.trialInfo['start_time']:.3f}s\")\n","                print(f\"     Difference: {self.videoTimes[0] - self.trialInfo['start_time']:.3f}s\")\n","\n","            if abs(self.videoTimes[-1] - self.trialInfo['end_time']) > 0.1:\n","                print(f\"  WARNING: Video end time mismatch!\")\n","                print(f\"     Video ends at: {self.videoTimes[-1]:.3f}s\")\n","                print(f\"     Trial ends at: {self.trialInfo['end_time']:.3f}s\")\n","                print(f\"     Difference: {self.videoTimes[-1] - self.trialInfo['end_time']:.3f}s\")\n","\n","            # Check video frame indices if available\n","            if 'video_frame_indices' in self.syncData:\n","                frame_indices = self.syncData['video_frame_indices']\n","                print(f\"  Video frame indices: {frame_indices[0]} to {frame_indices[-1]} (range: {frame_indices[-1] - frame_indices[0] + 1})\")\n","\n","                # Check for frame index gaps\n","                if len(frame_indices) > 1:\n","                    frame_gaps = np.diff(frame_indices)\n","                    unique_gaps = np.unique(frame_gaps)\n","                    if len(unique_gaps) > 1:\n","                        print(f\"  WARNING: Non-uniform frame indices!\")\n","                        print(f\"     Frame index gaps: {unique_gaps}\")\n","                    else:\n","                        print(f\"  Uniform frame indices (step: {unique_gaps[0]})\")\n","\n","            # Check if DataLoader has original timestamps\n","            if hasattr(self.dataLoader, 'standardizedTimestamps'):\n","                print(f\"  DataLoader standardized timestamps available for cameras: {list(self.dataLoader.standardizedTimestamps.keys())}\")\n","                for camera in self.cameras:\n","                    if camera in self.dataLoader.standardizedTimestamps:\n","                        cam_times = self.dataLoader.standardizedTimestamps[camera]\n","                        print(f\"     {camera}: {len(cam_times)} frames, {cam_times[0]:.3f} - {cam_times[-1]:.3f}s\")\n","\n","            print(f\"  Animation will use video times as master clock\")\n","\n","    def _createFigure(self) -> None:\n","        \"\"\"Create figure layout with dynamic camera selection and behavioral plots.\"\"\"\n","        self.fig = plt.figure(figsize=(20, 11), facecolor='white')\n","\n","        num_cameras = len(self.cameras)\n","\n","        if num_cameras == 1:\n","            # Single camera layout: camera takes full left half\n","            gs = GridSpec(4, 2,\n","                         width_ratios=[0.8, 1.2],           # Videos | plots (plots get 20% more space)\n","                         height_ratios=[1.0, 1.0, 1.0, 1.0], # Equal height for plots\n","                         hspace=0.2, wspace=0.12,\n","                         left=0.05, right=0.95, top=0.90, bottom=0.08)\n","\n","            # Single video display (full left half)\n","            self.axVideos = {\n","                self.cameras[0]: self.fig.add_subplot(gs[:, 0])  # Spans all rows\n","            }\n","\n","        elif num_cameras == 2:\n","            # Dual camera layout: equal sized cameras in left half\n","            gs = GridSpec(4, 2,\n","                         width_ratios=[0.8, 1.2],           # Videos | plots (plots get 20% more space)\n","                         height_ratios=[1.0, 1.0, 1.0, 1.0], # Equal height rows\n","                         hspace=0.2, wspace=0.12,\n","                         left=0.05, right=0.95, top=0.90, bottom=0.08)\n","\n","            # Two equal-sized video displays\n","            self.axVideos = {\n","                self.cameras[0]: self.fig.add_subplot(gs[0:2, 0]),  # Top half - spans 2 rows\n","                self.cameras[1]: self.fig.add_subplot(gs[2:4, 0])   # Bottom half - spans 2 rows\n","            }\n","\n","        # Set video titles and properties\n","        for i, camera in enumerate(self.cameras):\n","            ax = self.axVideos[camera]\n","            ax.set_title(f'{camera.title()} Camera', fontsize=11, fontweight='bold', pad=10)\n","            ax.axis('off')\n","\n","        # Behavioral plots (standardized sizes in right half)\n","        self.axPlots = {\n","            'whisker_nose': self.fig.add_subplot(gs[0, 1]),\n","            'wheel': self.fig.add_subplot(gs[1, 1]),\n","            'pupil': self.fig.add_subplot(gs[2, 1]),\n","            'tongue': self.fig.add_subplot(gs[3, 1])\n","        }\n","\n","        # Configure plot appearance with axis labels\n","        for ax in self.axPlots.values():\n","            ax.grid(True, alpha=0.3, linewidth=0.5)\n","            ax.set_xlim(self.trialInfo['start_time'], self.trialInfo['end_time'])\n","\n","        # Enhanced figure title with all required information\n","        trialIdx = self.trialInfo.get('trial_index', 'Unknown')\n","        duration = self.trialInfo['duration']\n","        subject = self.sessionInfo.get('subject', 'Unknown')\n","        lab = self.sessionInfo.get('lab', 'Unknown')\n","        self.fig.suptitle(f'IBL Behavioral Analysis - Trial {trialIdx} - Subject {subject} - Lab {lab} - {duration:.1f}s',\n","                         fontsize=16, fontweight='bold', y=0.95)\n","\n","    def _initializeVideo(self) -> None:\n","        \"\"\"Initialize video displays for selected cameras.\"\"\"\n","        placeholderFrame = np.zeros((480, 640), dtype=np.uint8)\n","\n","        # Initialize video displays dynamically based on selected cameras\n","        self.videoImages = {}\n","        self.timeTexts = {}\n","\n","        for camera in self.cameras:\n","            ax = self.axVideos[camera]\n","\n","            # Create video display\n","            self.videoImages[camera] = ax.imshow(placeholderFrame, cmap='gray', animated=True)\n","\n","            # Create time text overlay\n","            self.timeTexts[camera] = ax.text(0.02, 0.98, '', transform=ax.transAxes,\n","                                           fontsize=11, fontweight='bold', color='white',\n","                                           bbox=dict(boxstyle='round', facecolor='black', alpha=0.7),\n","                                           verticalalignment='top', animated=True)\n","\n","    def _initializePlots(self, animationData: Dict) -> None:\n","        \"\"\"Initialize all behavioral plots with full trial duration data.\"\"\"\n","        self._initializeWhiskerNosePlot()\n","        self._initializeWheelPlot()\n","        self._initializePupilPlot()\n","        self._initializeTonguePlot()\n","\n","    def _initializeWhiskerNosePlot(self) -> None:\n","        \"\"\"Initialize whisker motion and nose tracking plot.\"\"\"\n","        ax = self.axPlots['whisker_nose']\n","        ax.set_title('Whisker Motion Energy & Nose Position', fontsize=11, fontweight='bold')\n","\n","        axTwin = ax.twinx()\n","\n","        # Whisker motion (left axis)\n","        ax.set_ylabel('Motion Energy (a.u.)', color='red', fontweight='bold')\n","        ax.tick_params(axis='y', labelcolor='red')\n","\n","        # Nose position (right axis)\n","        axTwin.set_ylabel('Nose Position (pixels)', color='blue', fontweight='bold')\n","        axTwin.tick_params(axis='y', labelcolor='blue')\n","\n","        lines = {}\n","\n","        # Plot whisker motion energy data\n","        if 'motion_energy' in self.syncData:\n","            motionData = self.syncData['motion_energy']\n","            times = motionData.get('times', [])\n","\n","            if 'whisker_raw' in motionData and len(times) > 0:\n","                rawValues = motionData['whisker_raw']\n","                lines['whisker_raw'] = ax.plot(times, rawValues, color='yellow',\n","                                              linewidth=1.5, label='Whisker Raw', alpha=0.8)[0]\n","\n","            if 'whisker_smoothed' in motionData and len(times) > 0:\n","                smoothedValues = motionData['whisker_smoothed']\n","                lines['whisker_smooth'] = ax.plot(times, smoothedValues, color='red',\n","                                                 linewidth=2.5, label='Whisker Smooth')[0]\n","\n","            # Set y-limits based on data\n","            if lines:\n","                allValues = []\n","                for key in ['whisker_raw', 'whisker_smoothed']:\n","                    if key in motionData:\n","                        allValues.extend(motionData[key])\n","\n","                if allValues:\n","                    vmin, vmax = np.nanmin(allValues), np.nanmax(allValues)\n","                    vrange = vmax - vmin\n","                    ax.set_ylim(vmin - 0.1 * vrange, vmax + 0.1 * vrange)\n","\n","        # Plot nose tracking data\n","        if 'pose' in self.syncData and 'features' in self.syncData['pose']:\n","            poseData = self.syncData['pose']\n","            poseTimes = poseData.get('times', [])\n","            features = poseData.get('features', {})\n","\n","            if 'nose_tip_x' in features and len(poseTimes) > 0:\n","                noseX = features['nose_tip_x']['values']\n","                validMask = features['nose_tip_x']['valid_mask']\n","                validTimes = poseTimes[validMask]\n","                validValues = noseX[validMask]\n","                if len(validValues) > 0:\n","                    lines['nose_x'] = axTwin.plot(validTimes, validValues, color='blue',\n","                                                 linewidth=2, label='Nose X')[0]\n","\n","            if 'nose_tip_y' in features and len(poseTimes) > 0:\n","                noseY = features['nose_tip_y']['values']\n","                validMask = features['nose_tip_y']['valid_mask']\n","                validTimes = poseTimes[validMask]\n","                validValues = noseY[validMask]\n","                if len(validValues) > 0:\n","                    lines['nose_y'] = axTwin.plot(validTimes, validValues, color='cyan',\n","                                                 linewidth=2, label='Nose Y')[0]\n","\n","            # Set y-limits for nose position with min-max standardization\n","            noseValues = []\n","            for feature in ['nose_tip_x', 'nose_tip_y']:\n","                if feature in features:\n","                    values = features[feature]['values']\n","                    validMask = features[feature]['valid_mask']\n","                    noseValues.extend(values[validMask])\n","\n","            if noseValues:\n","                noseMin, noseMax = np.nanmin(noseValues), np.nanmax(noseValues)\n","                noseRange = noseMax - noseMin\n","                if noseRange > 0:\n","                    axTwin.set_ylim(noseMin - 0.1 * noseRange, noseMax + 0.1 * noseRange)\n","                else:\n","                    axTwin.set_ylim(noseMin - 10, noseMax + 10)\n","\n","        # Add legends\n","        if any('whisker' in key for key in lines.keys()):\n","            ax.legend(loc='upper left', fontsize=10)\n","        if any('nose' in key for key in lines.keys()):\n","            axTwin.legend(loc='upper right', fontsize=10)\n","\n","        self.lineObjects['whisker_nose'] = lines\n","\n","    def _initializeWheelPlot(self) -> None:\n","        \"\"\"Initialize wheel position plot.\"\"\"\n","        ax = self.axPlots['wheel']\n","        ax.set_title('Wheel Position', fontsize=11, fontweight='bold')\n","        ax.set_ylabel('Position (radians)', color='darkblue', fontweight='bold')\n","        ax.tick_params(axis='y', labelcolor='darkblue')\n","\n","        lines = {}\n","\n","        if 'wheel' in self.syncData:\n","            wheelData = self.syncData['wheel']\n","            times = wheelData.get('times', [])\n","\n","            # Display wheel position instead of velocity/acceleration\n","            if 'position_raw' in wheelData and len(times) > 0:\n","                position = wheelData['position_raw']\n","\n","                # Handle NaN values in position data\n","                if np.any(~np.isnan(position)):\n","                    lines['position'] = ax.plot(times, position, color='darkblue',\n","                                               linewidth=2.5, label='Wheel Position')[0]\n","\n","                    # Set y-limits based on actual position data\n","                    validPosition = position[~np.isnan(position)]\n","                    if len(validPosition) > 0:\n","                        pmin, pmax = np.min(validPosition), np.max(validPosition)\n","                        prange = pmax - pmin\n","                        if prange > 0:\n","                            ax.set_ylim(pmin - 0.1 * prange, pmax + 0.1 * prange)\n","                        else:\n","                            ax.set_ylim(pmin - 1, pmax + 1)\n","                    else:\n","                        ax.set_ylim(-10, 10)  # Default range\n","                else:\n","                    # No valid position data\n","                    ax.text(0.5, 0.5, 'No valid wheel position data',\n","                           transform=ax.transAxes, ha='center', va='center',\n","                           fontsize=10, color='gray')\n","                    ax.set_ylim(-1, 1)\n","\n","        # Add legend\n","        if lines:\n","            ax.legend(loc='upper right', fontsize=10)\n","\n","        self.lineObjects['wheel'] = lines\n","\n","    def _initializePupilPlot(self) -> None:\n","        \"\"\"Initialize pupil tracking plot.\"\"\"\n","        ax = self.axPlots['pupil']\n","        ax.set_title('Pupil Tracking', fontsize=11, fontweight='bold')\n","\n","        axTwin = ax.twinx()\n","\n","        lines = {}\n","\n","        if 'pose' in self.syncData and 'features' in self.syncData['pose']:\n","            poseData = self.syncData['pose']\n","            poseTimes = poseData.get('times', [])\n","            features = poseData.get('features', {})\n","\n","            # Pupil X (left axis)\n","            if 'pupil_top_r_x' in features and len(poseTimes) > 0:\n","                pupilX = features['pupil_top_r_x']['values']\n","                validMask = features['pupil_top_r_x']['valid_mask']\n","                validTimes = poseTimes[validMask]\n","                validValues = pupilX[validMask]\n","                if len(validValues) > 0:\n","                    lines['pupil_x'] = ax.plot(validTimes, validValues, color='darkblue',\n","                                              linewidth=2, label='Pupil X')[0]\n","\n","                    # Set y-limits for X axis\n","                    pupilXMin, pupilXMax = np.nanmin(validValues), np.nanmax(validValues)\n","                    pupilXRange = pupilXMax - pupilXMin\n","                    if pupilXRange > 0:\n","                        ax.set_ylim(pupilXMin - 0.1 * pupilXRange, pupilXMax + 0.1 * pupilXRange)\n","                    else:\n","                        ax.set_ylim(pupilXMin - 10, pupilXMax + 10)\n","\n","                    ax.set_ylabel('Pupil X Position (pixels)', color='darkblue', fontweight='bold')\n","                    ax.tick_params(axis='y', labelcolor='darkblue')\n","\n","            # Pupil Y (right axis)\n","            if 'pupil_top_r_y' in features and len(poseTimes) > 0:\n","                pupilY = features['pupil_top_r_y']['values']\n","                validMask = features['pupil_top_r_y']['valid_mask']\n","                validTimes = poseTimes[validMask]\n","                validValues = pupilY[validMask]\n","                if len(validValues) > 0:\n","                    lines['pupil_y'] = axTwin.plot(validTimes, validValues, color='violet',\n","                                                  linewidth=2, label='Pupil Y')[0]\n","\n","                    # Set y-limits for Y axis\n","                    pupilYMin, pupilYMax = np.nanmin(validValues), np.nanmax(validValues)\n","                    pupilYRange = pupilYMax - pupilYMin\n","                    if pupilYRange > 0:\n","                        axTwin.set_ylim(pupilYMin - 0.1 * pupilYRange, pupilYMax + 0.1 * pupilYRange)\n","                    else:\n","                        axTwin.set_ylim(pupilYMin - 10, pupilYMax + 10)\n","\n","                    axTwin.set_ylabel('Pupil Y Position (pixels)', color='violet', fontweight='bold')\n","                    axTwin.tick_params(axis='y', labelcolor='violet')\n","\n","        # Add legends\n","        if any('pupil_x' in key for key in lines.keys()):\n","            ax.legend(loc='upper left', fontsize=10)\n","        if any('pupil_y' in key for key in lines.keys()):\n","            axTwin.legend(loc='upper right', fontsize=10)\n","\n","        self.lineObjects['pupil'] = lines\n","\n","    def _initializeTonguePlot(self) -> None:\n","        \"\"\"Initialize tongue tracking plot.\"\"\"\n","        ax = self.axPlots['tongue']\n","        ax.set_title('Tongue Tracking', fontsize=11, fontweight='bold')\n","        ax.set_ylabel('Tongue Position (pixels)', fontweight='bold')\n","\n","        lines = {}\n","\n","        if 'pose' in self.syncData and 'features' in self.syncData['pose']:\n","            poseData = self.syncData['pose']\n","            poseTimes = poseData.get('times', [])\n","            features = poseData.get('features', {})\n","\n","            tongueValues = []\n","\n","            if 'tongue_end_l_x' in features and len(poseTimes) > 0:\n","                tongueX = features['tongue_end_l_x']['values']\n","                validMask = features['tongue_end_l_x']['valid_mask']\n","                validTimes = poseTimes[validMask]\n","                validValues = tongueX[validMask]\n","                if len(validValues) > 0:\n","                    lines['tongue_x'] = ax.plot(validTimes, validValues, color='magenta',\n","                                               linewidth=2, label='Tongue X')[0]\n","                    tongueValues.extend(validValues)\n","\n","            if 'tongue_end_l_y' in features and len(poseTimes) > 0:\n","                tongueY = features['tongue_end_l_y']['values']\n","                validMask = features['tongue_end_l_y']['valid_mask']\n","                validTimes = poseTimes[validMask]\n","                validValues = tongueY[validMask]\n","                if len(validValues) > 0:\n","                    lines['tongue_y'] = ax.plot(validTimes, validValues, color='brown',\n","                                               linewidth=2, label='Tongue Y')[0]\n","                    tongueValues.extend(validValues)\n","\n","            # Set y-limits based on actual data range\n","            if tongueValues:\n","                tongueMin, tongueMax = np.nanmin(tongueValues), np.nanmax(tongueValues)\n","                tongueRange = tongueMax - tongueMin\n","                if tongueRange > 0:\n","                    ax.set_ylim(tongueMin - 0.1 * tongueRange, tongueMax + 0.1 * tongueRange)\n","                else:\n","                    ax.set_ylim(tongueMin - 10, tongueMax + 10)\n","            else:\n","                ax.set_ylim(0, 480)\n","\n","        ax.legend(loc='upper right', fontsize=10)\n","        self.lineObjects['tongue'] = lines\n","\n","    def _initializeEvents(self, animationData: Dict) -> None:\n","        \"\"\"Initialize experimental event markers.\"\"\"\n","        # Only show requested events: stimOn, stimOff, firstMovement, Response, Feedback\n","        eventStyles = {\n","            'stimOn': {'color': 'red', 'linestyle': '-', 'linewidth': 2, 'label': 'stimOn'},\n","            'stimOff': {'color': 'orange', 'linestyle': '-', 'linewidth': 2, 'label': 'stimOff'},\n","            'firstMovement': {'color': 'cyan', 'linestyle': '-.', 'linewidth': 2, 'label': 'firstMovement'},\n","            'response': {'color': 'blue', 'linestyle': '-', 'linewidth': 2, 'label': 'Response'},\n","            'feedback': {'color': 'magenta', 'linestyle': ':', 'linewidth': 2, 'label': 'Feedback'}\n","        }\n","\n","        trialEvents = animationData['trial_info'].get('events', {})\n","        startTime = self.trialInfo['start_time']\n","        endTime = self.trialInfo['end_time']\n","\n","        debugInfo = {\n","            'trial_interval': {'start': startTime, 'end': endTime},\n","            'available_event_keys': list(trialEvents.keys()),\n","            'events_processed': {}\n","        }\n","\n","        self._log_debug(f\"DEBUGGING EVENT EXTRACTION:\")\n","        self._log_debug(f\"Trial interval: {startTime:.3f} - {endTime:.3f}s\")\n","        self._log_debug(f\"Available trial event keys: {list(trialEvents.keys())}\")\n","\n","        legendHandles = []\n","\n","        for eventName, eventStyle in eventStyles.items():\n","            eventTimes = self._extractEventTimes(trialEvents, eventName)\n","\n","            self._log_debug(f\"{eventName}:\")\n","            self._log_debug(f\"  Raw event times: {eventTimes}\")\n","\n","            eventInfo = {\n","                'raw_times': eventTimes,\n","                'valid_times': [],\n","                'markers_added': 0\n","            }\n","\n","            if eventTimes:\n","                validEventTimes = [t for t in eventTimes if startTime <= t <= endTime]\n","                eventInfo['valid_times'] = validEventTimes\n","\n","                self._log_debug(f\"  Valid event times (within interval): {validEventTimes}\")\n","\n","                if validEventTimes:\n","                    for ax in self.axPlots.values():\n","                        for eventTime in validEventTimes:\n","                            ax.axvline(x=eventTime, alpha=0.8,\n","                                     **{k: v for k, v in eventStyle.items() if k != 'label'})\n","\n","                    legendHandles.append(Line2D([0], [0], **eventStyle))\n","                    eventInfo['markers_added'] = len(validEventTimes)\n","\n","                    self._log_debug(f\"  Added {len(validEventTimes)} event markers\")\n","                else:\n","                    self._log_debug(f\"  No events within trial interval\")\n","            else:\n","                self._log_debug(f\"  No event times found\")\n","\n","            debugInfo['events_processed'][eventName] = eventInfo\n","\n","        # Add legend at bottom center to not obstruct views\n","        if legendHandles:\n","            self.fig.legend(handles=legendHandles,\n","                           loc='lower center',\n","                           bbox_to_anchor=(0.5, 0.01),\n","                           ncol=5, fontsize=10,\n","                           frameon=True,\n","                           title='Events',\n","                           title_fontsize=11)\n","\n","            self._log_debug(f\"Added legend for {len(legendHandles)} event types\")\n","        else:\n","            self._log_debug(f\"No valid events found - no legend added\")\n","\n","        # Store debug info for save mode\n","        if self.mode == 'save':\n","            self.eventDebugInfo = debugInfo\n","\n","    def _extractEventTimes(self, trialEvents: Dict, eventName: str) -> List[float]:\n","        \"\"\"Extract timing information for specific event type.\"\"\"\n","        self._log_debug(f\"  Extracting {eventName}:\")\n","\n","        # Check direct event timing storage\n","        directKey = f\"{eventName}_times\"\n","        self._log_debug(f\"    Checking direct key: {directKey}\")\n","        if directKey in trialEvents:\n","            eventData = trialEvents[directKey]\n","            self._log_debug(f\"    Found data: {eventData} (type: {type(eventData)})\")\n","            if isinstance(eventData, (list, np.ndarray)):\n","                validTimes = [float(t) for t in eventData if not np.isnan(float(t))]\n","                self._log_debug(f\"    Valid times from list: {validTimes}\")\n","                return validTimes\n","            elif not np.isnan(float(eventData)):\n","                validTime = float(eventData)\n","                self._log_debug(f\"    Valid time from scalar: {validTime}\")\n","                return [validTime]\n","\n","        # Check alternative naming conventions\n","        altKeys = [eventName, f\"{eventName}Times\", f\"{eventName.lower()}_times\"]\n","        self._log_debug(f\"    Checking alternative keys: {altKeys}\")\n","\n","        for key in altKeys:\n","            if key in trialEvents:\n","                eventData = trialEvents[key]\n","                self._log_debug(f\"    Found {key}: {eventData} (type: {type(eventData)})\")\n","                if isinstance(eventData, (list, np.ndarray)):\n","                    validTimes = [float(t) for t in eventData if not np.isnan(float(t))]\n","                    self._log_debug(f\"    Valid times from {key}: {validTimes}\")\n","                    return validTimes\n","                elif not np.isnan(float(eventData)):\n","                    validTime = float(eventData)\n","                    self._log_debug(f\"    Valid time from {key}: {validTime}\")\n","                    return [validTime]\n","\n","        self._log_debug(f\"    No valid times found for {eventName}\")\n","        return []\n","\n","    def _log_debug(self, message):\n","        \"\"\"Add debug message to log.\"\"\"\n","        if self.mode == 'save':\n","            self.debugLog.append(message)\n","        if self.verbose:\n","            print(message)\n","\n","    def _initializeVideoLoader(self) -> None:\n","        \"\"\"Initialize video loading system for selected cameras.\"\"\"\n","        if self.one is None or self.eid is None:\n","            self._log_debug(\"No ONE instance or EID provided - video will use placeholders\")\n","            return\n","\n","        try:\n","            from ibllib.io import video\n","            videoUrls = video.url_from_eid(self.eid, one=self.one)\n","\n","            # Initialize only selected cameras\n","            for camera in self.cameras:\n","                self.videoUrls[camera] = videoUrls.get(camera)\n","\n","            cameras_found = [cam for cam in self.cameras if self.videoUrls[cam] is not None]\n","            if cameras_found:\n","                self._log_debug(f\"Video URLs loaded for selected cameras: {cameras_found}\")\n","            else:\n","                self._log_debug(\"No video URLs found for selected cameras\")\n","\n","        except Exception as e:\n","            self._log_debug(f\"Video initialization failed: {e}\")\n","            # Reset to None for all selected cameras\n","            for camera in self.cameras:\n","                self.videoUrls[camera] = None\n","\n","    def _getCameraFrameForTime(self, camera: str, target_time: float):\n","        \"\"\"\n","        Get the best camera frame index for a specific time using interpolation.\n","        This respects the camera's original frame rate instead of forcing alignment.\n","        \"\"\"\n","        if not hasattr(self.dataLoader, 'one') or not hasattr(self.dataLoader, 'eid'):\n","            return None, None, float('inf')\n","\n","        try:\n","            # Get original camera timestamps directly from ONE\n","            from one.api import ONE\n","            one = self.dataLoader.one\n","            eid = self.dataLoader.eid\n","\n","            # Load raw camera timestamps\n","            camera_times = one.load_dataset(eid, f'_ibl_{camera}Camera.times.npy')\n","\n","            # Find closest frame to target time\n","            time_diffs = np.abs(camera_times - target_time)\n","            closest_idx = np.argmin(time_diffs)\n","            actual_time = camera_times[closest_idx]\n","            time_error = abs(actual_time - target_time)\n","\n","            if self.verbose and hasattr(self, '_first_frame_debug') and camera not in self._first_frame_debug:\n","                print(f\"Camera {camera} raw timing: {len(camera_times)} frames, {camera_times[0]:.3f}-{camera_times[-1]:.3f}s\")\n","                if not hasattr(self, '_first_frame_debug'):\n","                    self._first_frame_debug = set()\n","                self._first_frame_debug.add(camera)\n","\n","            return closest_idx, actual_time, time_error\n","\n","        except Exception as e:\n","            if self.verbose:\n","                print(f\"Failed to get raw timestamps for {camera}: {e}\")\n","            return None, None, float('inf')\n","\n","    def _loadVideoChunk(self, startFrame: int, camera: str = 'left') -> None:\n","        \"\"\"Load chunk of video frames using correct timing approach.\"\"\"\n","        if camera not in self.videoUrls or self.videoUrls[camera] is None:\n","            return\n","\n","        try:\n","            from ibllib.io import video\n","\n","            # NEW APPROACH: Use animation timeline to get correct camera frames\n","            animation_start_idx = startFrame\n","            animation_end_idx = min(startFrame + self.chunkSize, self.totalFrames)\n","\n","            # Get the times for these animation frames\n","            animation_times = self.videoTimes[animation_start_idx:animation_end_idx]\n","\n","            # Map each animation time to the best camera frame\n","            camera_frame_indices = []\n","            timing_errors = []\n","\n","            for anim_time in animation_times:\n","                cam_frame_idx, actual_time, time_error = self._getCameraFrameForTime(camera, anim_time)\n","                if cam_frame_idx is not None:\n","                    camera_frame_indices.append(cam_frame_idx)\n","                    timing_errors.append(time_error)\n","                else:\n","                    # Fallback: estimate frame index\n","                    if hasattr(self, '_fallback_camera_start'):\n","                        cam_start_time = self._fallback_camera_start.get(camera, anim_time)\n","                    else:\n","                        cam_start_time = anim_time\n","                    estimated_idx = int((anim_time - cam_start_time) * 60)  # Assume ~60fps\n","                    camera_frame_indices.append(max(0, estimated_idx))\n","                    timing_errors.append(0.1)  # Large error for fallback\n","\n","            if self.verbose and startFrame == 0:\n","                print(f\"\\nDEBUG: Corrected video loading for {camera}\")\n","                print(f\"  Animation frames: {animation_start_idx} to {animation_end_idx-1}\")\n","                print(f\"  Animation times: {animation_times[0]:.3f}s to {animation_times[-1]:.3f}s\")\n","                if camera_frame_indices:\n","                    print(f\"  {camera} frame indices: {camera_frame_indices[0]} to {camera_frame_indices[-1]}\")\n","                    avg_error = np.mean(timing_errors) * 1000  # Convert to ms\n","                    max_error = np.max(timing_errors) * 1000\n","                    print(f\"  Timing errors: avg={avg_error:.1f}ms, max={max_error:.1f}ms\")\n","\n","            # Load the mapped camera frames\n","            if camera_frame_indices:\n","                frames = video.get_video_frames_preload(self.videoUrls[camera], camera_frame_indices)\n","\n","                # Store frames in cache with animation frame indices as keys\n","                for i, frame in enumerate(frames):\n","                    self.videoFrameCaches[camera][startFrame + i] = frame\n","\n","                self.cacheStartFrames[camera] = startFrame\n","\n","                if self.verbose and startFrame == 0:\n","                    print(f\"Loaded {camera}: {len(frames)} frames with corrected timing\")\n","            else:\n","                if self.verbose:\n","                    print(f\"No valid frame indices for {camera}\")\n","\n","        except Exception as e:\n","            if self.verbose:\n","                print(f\"Failed to load {camera} camera chunk at frame {startFrame}: {e}\")\n","\n","    def _getVideoFrame(self, frameIdx: int, camera: str = 'left') -> np.ndarray:\n","        \"\"\"Get video frame using corrected timing approach.\"\"\"\n","\n","        # DEBUG: Show corrected timing for first few frames\n","        if self.verbose and frameIdx < 3:\n","            current_time = self.videoTimes[frameIdx]\n","            cam_frame_idx, actual_time, time_error = self._getCameraFrameForTime(camera, current_time)\n","\n","            print(f\"CORRECTED: Camera {camera} frame {frameIdx}: AnimTime={current_time:.3f}s\")\n","            if cam_frame_idx is not None:\n","                print(f\"   Best camera frame: {cam_frame_idx}, actual time: {actual_time:.3f}s\")\n","                print(f\"   Timing error: {time_error*1000:.1f}ms\")\n","                if time_error < 0.05:  # Less than 50ms\n","                    print(f\"   Excellent timing alignment\")\n","                elif time_error < 0.1:  # Less than 100ms\n","                    print(f\"   Good timing alignment\")\n","                else:\n","                    print(f\"   WARNING: Large timing error\")\n","            else:\n","                print(f\"   Failed to find camera frame\")\n","\n","        # Check if we need to load a new chunk for this camera\n","        if (self.cacheStartFrames[camera] is None or\n","            frameIdx < self.cacheStartFrames[camera] or\n","            frameIdx >= self.cacheStartFrames[camera] + self.chunkSize):\n","\n","            # Calculate chunk start (align to chunk boundaries)\n","            chunkStart = (frameIdx // self.chunkSize) * self.chunkSize\n","            self._loadVideoChunk(chunkStart, camera)\n","\n","        # Return frame from camera-specific cache or placeholder\n","        if frameIdx in self.videoFrameCaches[camera]:\n","            frame = self.videoFrameCaches[camera][frameIdx]\n","            # Handle frame format\n","            if frame.ndim == 3 and frame.shape[2] == 1:\n","                frame = frame[..., 0]\n","            return frame\n","        else:\n","            # Fallback placeholder with debug info\n","            if self.verbose and frameIdx < 5:\n","                print(f\"Using placeholder frame for {camera} at animation frame {frameIdx}\")\n","            return np.random.randint(0, 255, (480, 640), dtype=np.uint8)\n","\n","    def _initializeTimeIndicators(self) -> None:\n","        \"\"\"Initialize vertical time indicator lines.\"\"\"\n","        self.timeIndicators = []\n","        startTime = self.trialInfo['start_time']\n","\n","        for ax in self.axPlots.values():\n","            timeLine = ax.axvline(x=startTime, color='red', linewidth=3,\n","                                 alpha=0.8, animated=True, zorder=10)\n","            self.timeIndicators.append(timeLine)\n","\n","    def _createAnimation(self) -> None:\n","        \"\"\"Create and display animation.\"\"\"\n","        animation = FuncAnimation(\n","            self.fig,\n","            self._updateFrame,\n","            frames=self.totalFrames,\n","            interval=1000 / self.fps,\n","            blit=True,\n","            repeat=False\n","        )\n","\n","        self._saveOrDisplay(animation)\n","\n","    def _updateFrame(self, frameIdx: int) -> List:\n","        \"\"\"Update frame - move time indicators and update video displays.\"\"\"\n","        currentTime = self.videoTimes[frameIdx]\n","        updatedElements = []\n","\n","        # DEBUG: Show timing details for first few frames\n","        if self.verbose and frameIdx < 5:\n","            video_frame_idx = self.syncData['video_frame_indices'][frameIdx] if 'video_frame_indices' in self.syncData else frameIdx\n","            print(f\"Frame {frameIdx}: Time={currentTime:.3f}s, VideoFrameIdx={video_frame_idx}\")\n","\n","            # Check if this matches expected trial timing\n","            trial_progress = (currentTime - self.trialInfo['start_time']) / self.trialInfo['duration']\n","            print(f\"   Trial progress: {trial_progress*100:.1f}% ({currentTime:.3f}s / {self.trialInfo['duration']:.3f}s)\")\n","\n","        # Update video frames for all selected cameras\n","        for camera in self.cameras:\n","            frame = self._getVideoFrame(frameIdx, camera)\n","            self.videoImages[camera].set_array(frame)\n","            updatedElements.append(self.videoImages[camera])\n","\n","        # Update time displays on all videos\n","        timeText = f'Time: {currentTime:.2f}s'\n","        for camera in self.cameras:\n","            self.timeTexts[camera].set_text(timeText)\n","            updatedElements.append(self.timeTexts[camera])\n","\n","        # Move time indicators across all plots\n","        for timeLine in self.timeIndicators:\n","            timeLine.set_xdata([currentTime, currentTime])\n","        updatedElements.extend(self.timeIndicators)\n","\n","        return updatedElements\n","\n","    def _saveOrDisplay(self, animation: FuncAnimation) -> None:\n","        \"\"\"Save or display animation.\"\"\"\n","        if self.mode == 'test':\n","            self._log_debug(\"Displaying animation...\")\n","            display(HTML(animation.to_jshtml()))\n","        else:\n","            self._log_debug(\"Saving animation...\")\n","\n","            sessionId = self.sessionInfo.get('experimentId', 'unknown')\n","            outDir = os.path.join(self.baseDir, f\"session_{sessionId}\")\n","\n","            # Create directory if it doesn't exist\n","            os.makedirs(outDir, exist_ok=True)\n","\n","            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","            trialIdx = self.trialInfo.get('trial_index', 'unknown')\n","            filename = f\"trial_{trialIdx}_{timestamp}.mp4\"\n","            filepath = os.path.join(outDir, filename)\n","\n","            animation.save(filepath, writer='ffmpeg', fps=self.fps,\n","                          bitrate=5000, codec='libx264')\n","\n","            self._log_debug(f\"Animation saved to: {filepath}\")\n","\n","            # Save debug log to JSON file\n","            if hasattr(self, 'eventDebugInfo'):\n","                debugFilename = f\"trial_{trialIdx}_{timestamp}_debug.json\"\n","                debugFilepath = os.path.join(outDir, debugFilename)\n","\n","                debugData = {\n","                    'trial_info': self.trialInfo,\n","                    'session_info': self.sessionInfo,\n","                    'event_debug': self.eventDebugInfo,\n","                    'debug_log': self.debugLog,\n","                    'video_file': filename\n","                }\n","\n","                import json\n","                with open(debugFilepath, 'w') as f:\n","                    json.dump(debugData, f, indent=2, default=str)\n","\n","                self._log_debug(f\"Debug info saved to: {debugFilepath}\")\n","\n","    def _cleanup(self) -> None:\n","        \"\"\"Clean up resources.\"\"\"\n","        gc.collect()\n","\n","    # DataLoader Integration Methods\n","    def _validateDataLoader(self) -> None:\n","        \"\"\"Validate DataLoader state.\"\"\"\n","        if isinstance(self.dataLoader, type):\n","            raise TypeError(\"Expected DataLoader instance, received class.\")\n","\n","        if not hasattr(self.dataLoader, 'sessionInfo') or not self.dataLoader.sessionInfo:\n","            raise AttributeError(\"DataLoader has not loaded session data.\")\n","\n","        if not hasattr(self.dataLoader, 'trialData') or self.dataLoader.trialData is None:\n","            raise ValueError(\"DataLoader does not contain trial data.\")\n","\n","    def animateRandomInterval(self) -> bool:\n","        \"\"\"Create animation of random trial interval.\"\"\"\n","        if self.dataLoader is None:\n","            raise ValueError(\"DataLoader required for this method. Initialize with dataLoader parameter.\")\n","\n","        try:\n","            if self.verbose:\n","                print(\"Creating random interval animation...\")\n","\n","            animationData = self._sampleIntervalAdaptive()\n","            if animationData is None:\n","                return False\n","\n","            self.render(animationData)\n","            if self.verbose:\n","                print(\"Animation completed!\")\n","            return True\n","\n","        except Exception as e:\n","            if self.verbose:\n","                print(f\"Animation failed: {str(e)}\")\n","            return False\n","\n","    def animateSpecificTrial(self, trialIndex: int) -> bool:\n","        \"\"\"Create animation of specific trial.\"\"\"\n","        if self.dataLoader is None:\n","            raise ValueError(\"DataLoader required for this method. Initialize with dataLoader parameter.\")\n","\n","        try:\n","            if trialIndex >= len(self.dataLoader.trialData):\n","                if self.verbose:\n","                    print(f\"Trial {trialIndex} not found\")\n","                return False\n","\n","            if self.verbose:\n","                print(f\"Animating trial {trialIndex}...\")\n","\n","            selectedTrial = self.dataLoader.trialData.iloc[trialIndex]\n","            intervalStart = selectedTrial['intervals_0']\n","            intervalEnd = selectedTrial['intervals_1']\n","\n","            intervalData = {\n","                'trialIndex': trialIndex,\n","                'intervalStart': intervalStart,\n","                'intervalEnd': intervalEnd,\n","                'duration': intervalEnd - intervalStart,\n","                'trialEvents': selectedTrial.to_dict(),\n","                'sessionInfo': self.dataLoader.sessionInfo\n","            }\n","\n","            # Extract behavioral data\n","            if hasattr(self.dataLoader, 'extractPoseDataForInterval'):\n","                intervalData['poseData'] = self.dataLoader.extractPoseDataForInterval(intervalStart, intervalEnd)\n","            if hasattr(self.dataLoader, 'extractWheelDataForInterval'):\n","                intervalData['wheelData'] = self.dataLoader.extractWheelDataForInterval(intervalStart, intervalEnd)\n","            if hasattr(self.dataLoader, 'extractMotionEnergyForInterval'):\n","                intervalData['motionEnergyData'] = self.dataLoader.extractMotionEnergyForInterval(intervalStart, intervalEnd)\n","\n","            animationData = self._prepareAnimationDataLocally(intervalData)\n","            if animationData is None:\n","                return False\n","\n","            self.render(animationData)\n","            if self.verbose:\n","                print(f\"Trial {trialIndex} completed!\")\n","            return True\n","\n","        except Exception as e:\n","            if self.verbose:\n","                print(f\"Failed to animate trial {trialIndex}: {str(e)}\")\n","            return False\n","\n","    def _sampleIntervalAdaptive(self):\n","        \"\"\"Sample interval with adaptive DataLoader compatibility.\"\"\"\n","        if hasattr(self.dataLoader, 'prepareDataForAnimation'):\n","            try:\n","                import random\n","                trialIdx = random.randint(0, len(self.dataLoader.trialData) - 1)\n","                rawData = self.dataLoader.sampleInterval(trialIdx)\n","                return self.dataLoader.prepareDataForAnimation(rawData) if rawData else None\n","            except:\n","                pass\n","\n","        import random\n","        trialIdx = random.randint(0, len(self.dataLoader.trialData) - 1)\n","        rawData = self.dataLoader.sampleInterval(trialIdx)\n","        return self._prepareAnimationDataLocally(rawData) if rawData else None\n","\n","    def _prepareAnimationDataLocally(self, rawIntervalData):\n","        \"\"\"Prepare animation data locally.\"\"\"\n","        try:\n","            startTime = rawIntervalData['intervalStart']\n","            endTime = rawIntervalData['intervalEnd']\n","\n","            animationData = {\n","                'metadata': rawIntervalData['sessionInfo'],\n","                'trial_info': {\n","                    'trial_index': rawIntervalData['trialIndex'],\n","                    'start_time': startTime,\n","                    'end_time': endTime,\n","                    'duration': rawIntervalData['duration'],\n","                    'events': rawIntervalData['trialEvents']\n","                },\n","                'synchronized_data': {}\n","            }\n","\n","            # Process video timing with DEBUG\n","            if hasattr(self.dataLoader, 'videoTimes') and self.dataLoader.videoTimes is not None:\n","                videoFilter = (self.dataLoader.videoTimes >= startTime) & (self.dataLoader.videoTimes <= endTime)\n","                videoTimes = self.dataLoader.videoTimes[videoFilter]\n","\n","                if len(videoTimes) > 0:\n","                    animationData['synchronized_data']['video_times'] = videoTimes\n","\n","                    # CRITICAL FIX: Get actual frame indices, not relative indices\n","                    frame_indices = np.where(videoFilter)[0]\n","                    animationData['synchronized_data']['video_frame_indices'] = frame_indices\n","\n","                    if self.verbose:\n","                        print(f\"\\nDEBUG: Video timing processing\")\n","                        print(f\"  DataLoader video times: {len(self.dataLoader.videoTimes)} total frames\")\n","                        print(f\"  DataLoader time range: {self.dataLoader.videoTimes[0]:.3f} - {self.dataLoader.videoTimes[-1]:.3f}s\")\n","                        print(f\"  Trial interval: {startTime:.3f} - {endTime:.3f}s\")\n","                        print(f\"  Filtered video times: {len(videoTimes)} frames\")\n","                        print(f\"  Filtered time range: {videoTimes[0]:.3f} - {videoTimes[-1]:.3f}s\")\n","                        print(f\"  Frame indices: {frame_indices[0]} to {frame_indices[-1]} (step: {frame_indices[1]-frame_indices[0] if len(frame_indices)>1 else 'N/A'})\")\n","\n","                        # Check for different camera timestamps\n","                        if hasattr(self.dataLoader, 'standardizedTimestamps'):\n","                            print(f\"  Camera timestamp analysis:\")\n","                            for cam_name, cam_times in self.dataLoader.standardizedTimestamps.items():\n","                                cam_filter = (cam_times >= startTime) & (cam_times <= endTime)\n","                                cam_in_trial = cam_times[cam_filter]\n","                                cam_indices = np.where(cam_filter)[0]\n","                                print(f\"    {cam_name}: {len(cam_in_trial)} frames, indices {cam_indices[0] if len(cam_indices)>0 else 'N/A'} to {cam_indices[-1] if len(cam_indices)>0 else 'N/A'}\")\n","\n","                                # Check if camera times match our video times\n","                                if cam_name == 'left' and len(cam_in_trial) > 0:\n","                                    time_diff = abs(cam_in_trial[0] - videoTimes[0]) + abs(cam_in_trial[-1] - videoTimes[-1])\n","                                    if time_diff > 0.1:\n","                                        print(f\"    WARNING: {cam_name} camera times don't match video times (diff: {time_diff:.3f}s)\")\n","                                    else:\n","                                        print(f\"    {cam_name} camera times match video times\")\n","\n","            # Add behavioral data\n","            self._addPoseData(animationData, rawIntervalData)\n","            self._addWheelData(animationData, rawIntervalData)\n","            self._addMotionEnergyData(animationData, rawIntervalData)\n","\n","            return animationData\n","\n","        except Exception as e:\n","            if self.verbose:\n","                print(f\"Error preparing animation data: {str(e)}\")\n","            return None\n","\n","    def _addPoseData(self, animationData, rawIntervalData):\n","        \"\"\"Add pose data to animation data structure.\"\"\"\n","        if 'poseData' in rawIntervalData and rawIntervalData['poseData'] is not None:\n","            poseData = rawIntervalData['poseData']\n","            animationData['synchronized_data']['pose'] = {\n","                'times': poseData['times'].values,\n","                'features': {}\n","            }\n","\n","            # Store each pose feature\n","            for feature in poseData.columns:\n","                if feature != 'times':\n","                    featureData = poseData[feature].values\n","                    validMask = ~np.isnan(featureData)\n","\n","                    animationData['synchronized_data']['pose']['features'][feature] = {\n","                        'values': featureData,\n","                        'valid_mask': validMask,\n","                        'valid_times': poseData['times'].values[validMask],\n","                        'valid_values': featureData[validMask]\n","                    }\n","\n","    def _addWheelData(self, animationData, rawIntervalData):\n","        \"\"\"Add wheel data to animation data structure (position-focused).\"\"\"\n","        if 'wheelData' in rawIntervalData and rawIntervalData['wheelData'] is not None:\n","            wheelData = rawIntervalData['wheelData']\n","            animationData['synchronized_data']['wheel'] = {\n","                'times': wheelData['times'],\n","                'position_raw': wheelData.get('position_raw'),\n","            }\n","\n","    def _addMotionEnergyData(self, animationData, rawIntervalData):\n","        \"\"\"Add motion energy data to animation data structure.\"\"\"\n","        if 'motionEnergyData' in rawIntervalData and rawIntervalData['motionEnergyData'] is not None:\n","            motionData = rawIntervalData['motionEnergyData']\n","            animationData['synchronized_data']['motion_energy'] = {\n","                'times': motionData['times'],\n","                'whisker_raw': motionData.get('whiskerMotionEnergy_raw'),\n","                'whisker_smoothed': motionData.get('whiskerMotionEnergy_smoothed')\n","            }\n","\n","\n","# Complete workflow wrapper function\n","def animateIBLSession(eid, mode='test', fps=24, trialIndex=None, verbose=True, baseDir=None):\n","    \"\"\"\n","    Complete workflow to create IBL behavioral animations.\n","\n","    Args:\n","        eid (str): IBL experiment ID\n","        mode (str): 'test' for inline display, 'save' for MP4 output\n","        fps (int): Animation frame rate\n","        trialIndex (int, optional): Specific trial to animate. If None, random trial\n","        verbose (bool): Enable detailed logging\n","        baseDir (str, optional): Base directory for saving (only used in save mode)\n","\n","    Returns:\n","        bool: True if animation succeeded, False otherwise\n","    \"\"\"\n","    try:\n","        if verbose:\n","            print(\"=\"*60)\n","            print(\"IBL ANIMATION WORKFLOW\")\n","            print(\"=\"*60)\n","            print(f\"Experiment ID: {eid}\")\n","            print(f\"Mode: {mode}, FPS: {fps}\")\n","            if trialIndex is not None:\n","                print(f\"Trial: {trialIndex}\")\n","            else:\n","                print(\"Trial: Random\")\n","\n","        # Step 1: Initialize DataLoader\n","        if verbose:\n","            print(\"\\n1. Initializing DataLoader...\")\n","        dataLoader = IBLDataLoader(eid=eid, verbose=verbose)\n","\n","        # Step 2: Load session data\n","        if verbose:\n","            print(\"\\n2. Loading session data...\")\n","        loadSuccess = dataLoader.loadCompleteSession()\n","        if not loadSuccess:\n","            print(\"Session loading failed\")\n","            return False\n","\n","        # Step 3: Create animation renderer\n","        if verbose:\n","            print(\"\\n3. Creating animation renderer...\")\n","        renderer = createAnimationRenderer(dataLoader, mode=mode, fps=fps, verbose=verbose, baseDir=baseDir)\n","\n","        # Step 4: Create animation\n","        if verbose:\n","            print(\"\\n4. Creating animation...\")\n","        if trialIndex is not None:\n","            success = renderer.animateSpecificTrial(trialIndex)\n","        else:\n","            success = renderer.animateRandomInterval()\n","\n","        if verbose:\n","            if success:\n","                print(\"\\nAnimation workflow completed successfully!\")\n","            else:\n","                print(\"\\nAnimation workflow failed\")\n","\n","        return success\n","\n","    except Exception as e:\n","        if verbose:\n","            print(f\"\\nWorkflow error: {str(e)}\")\n","        return False\n","\n","# Convenience functions for easy use\n","def createAnimationRenderer(dataLoader, mode='test', fps=24, verbose=True, baseDir=None, cameras=['left', 'right']):\n","    \"\"\"\n","    Create animation renderer with DataLoader integration.\n","\n","    Args:\n","        dataLoader: IBLDataLoader instance with loaded session data\n","        mode (str): 'test' for inline display, 'save' for MP4 output\n","        fps (int): Animation frame rate\n","        verbose (bool): Enable status messages\n","        baseDir (str, optional): Base directory for saving\n","        cameras (list): List of 1-2 cameras from ['left', 'right', 'body']\n","\n","    Returns:\n","        IBLAnimationRenderer: Ready-to-use animation renderer with DataLoader integration\n","    \"\"\"\n","    return IBLAnimationRenderer(mode=mode, fps=fps, verbose=verbose,\n","                               one=dataLoader.one, eid=dataLoader.eid,\n","                               baseDir=baseDir, dataLoader=dataLoader, cameras=cameras)\n","\n","def quickAnimate(dataLoader, mode='test', fps=24, cameras=['left', 'right']):\n","    \"\"\"\n","    Quick animation of random interval - one-line convenience function.\n","\n","    Args:\n","        dataLoader: IBLDataLoader instance with loaded session data\n","        mode (str): 'test' for inline display, 'save' for MP4 output\n","        fps (int): Animation frame rate\n","        cameras (list): List of 1-2 cameras from ['left', 'right', 'body']\n","\n","    Returns:\n","        bool: True if animation succeeded, False otherwise\n","    \"\"\"\n","    renderer = createAnimationRenderer(dataLoader, mode=mode, fps=fps, cameras=cameras)\n","    return renderer.animateRandomInterval()\n","\n","def animateAllTrials(eid, baseDir, fps=24, maxTrials=None):\n","    \"\"\"\n","    Animate all trials in a session with memory optimization and progress tracking.\n","\n","    Args:\n","        eid (str): Experiment ID\n","        baseDir (str): Base directory for saving\n","        fps (int): Frame rate\n","        maxTrials (int, optional): Limit number of trials (for testing)\n","\n","    Returns:\n","        bool: True if all trials succeeded, False otherwise\n","    \"\"\"\n","    from tqdm import tqdm\n","    import gc\n","    import matplotlib.pyplot as plt\n","\n","    # Load session once with VERBOSE output\n","    print(\"Loading session data...\")\n","\n","    dataLoader = IBLDataLoader(eid=eid, verbose=True)  # Enable verbose for troubleshooting\n","    print(f\"DataLoader initialized for EID: {eid}\")\n","    if not dataLoader.loadCompleteSession():\n","        print(\"Failed to load session\")\n","        return False\n","    print(f\"Session data loaded successfully - {len(dataLoader.trialData)} trials available\")\n","\n","    numTrials = len(dataLoader.trialData)\n","    if maxTrials:\n","        numTrials = min(numTrials, maxTrials)\n","\n","    successCount = 0\n","    failedTrials = []\n","\n","    # Create progress bar\n","    pbar = tqdm(range(numTrials), desc=\"Animating trials\", unit=\"trial\")\n","\n","    for trialIdx in pbar:\n","        pbar.set_description(f\"Processing trial {trialIdx+1}/{numTrials}\")\n","\n","        try:\n","            # Create renderer with reduced chunk size for memory efficiency\n","            renderer = IBLAnimationRenderer(mode='save', baseDir=baseDir, fps=fps,\n","                                           verbose=False, one=dataLoader.one, eid=dataLoader.eid,\n","                                           dataLoader=dataLoader)\n","            renderer.chunkSize = 50  # Reduce memory usage\n","\n","            # Animate trial\n","            success = renderer.animateSpecificTrial(trialIdx)\n","\n","            if success:\n","                successCount += 1\n","                pbar.set_postfix({\"Success\": f\"{successCount}/{trialIdx+1}\", \"Failed\": len(failedTrials)})\n","            else:\n","                failedTrials.append(trialIdx)\n","                pbar.set_postfix({\"Success\": f\"{successCount}/{trialIdx+1}\", \"Failed\": len(failedTrials)})\n","\n","        except Exception as e:\n","            failedTrials.append(trialIdx)\n","            pbar.set_postfix({\"Success\": f\"{successCount}/{trialIdx+1}\", \"Failed\": len(failedTrials)})\n","            # Log error without interrupting progress bar\n","            tqdm.write(f\"Error on trial {trialIdx}: {e}\")\n","\n","        finally:\n","            # Critical cleanup to prevent memory issues\n","            plt.close('all')\n","            if 'renderer' in locals():\n","                # Clear both camera caches\n","                for cache in renderer.videoFrameCaches.values():\n","                    cache.clear()\n","                del renderer\n","            gc.collect()\n","\n","    pbar.close()\n","\n","    # Final summary\n","    print(f\"\\nBatch processing completed:\")\n","    print(f\"  Successful: {successCount}/{numTrials} trials\")\n","    print(f\"  Failed: {len(failedTrials)} trials\")\n","    if failedTrials:\n","        print(f\"  Failed trial indices: {failedTrials}\")\n","\n","    return successCount == numTrials\n","\n","# Quick batch processing functions\n","def quickBatchAnimate(eid, baseDir, maxTrials=None, fps=24):\n","    \"\"\"\n","    Quick batch animation for testing (limits to first few trials).\n","\n","    Args:\n","        eid (str): Experiment ID\n","        baseDir (str): Base directory for saving\n","        maxTrials (int): Maximum number of trials to process\n","        fps (int): Frame rate\n","    \"\"\"\n","    print(f\"Quick batch test: processing first {maxTrials} trials\")\n","    return animateAllTrials(eid, baseDir, fps=fps, maxTrials=maxTrials)"]},{"cell_type":"markdown","metadata":{"id":"ZD7X4-gJraLa"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1D41dkM-_cKr"},"outputs":[],"source":["eid = '88d24c31-52e4-49cc-9f32-6adbeb9eba87'\n","BASE_DIR = \"/content/drive/MyDrive/S25/Langone/Breathing/Videos\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"s-U3dG0nWJXZ","outputId":"40dcd813-2b87-4fcf-fe7d-3bfcd8e9dd0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading session data...\n","Initializing IBL DataLoader for experiment: 88d24c31-52e4-49cc-9f32-6adbeb9eba87\n","DataLoader initialized for EID: 88d24c31-52e4-49cc-9f32-6adbeb9eba87\n","============================================================\n","STARTING COMPLETE SESSION LOADING\n","============================================================\n","\n","Loading session data...\n","Loading session data...\n","\u001b[36m2025-06-26 20:27:09 INFO     one.py:1475 Loading trials data\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["INFO:ibllib:Loading trials data\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m2025-06-26 20:27:12 INFO     one.py:1475 Loading wheel data\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["INFO:ibllib:Loading wheel data\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m2025-06-26 20:27:14 INFO     one.py:1475 Loading pose data\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["INFO:ibllib:Loading pose data\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m2025-06-26 20:27:17 INFO     one.py:1475 Loading motion_energy data\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["INFO:ibllib:Loading motion_energy data\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[36m2025-06-26 20:27:18 INFO     one.py:1475 Loading pupil data\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["INFO:ibllib:Loading pupil data\n"]},{"name":"stdout","output_type":"stream","text":["✓ Session data loaded successfully\n","✓ Completed: Loading session data\n","\n","Extracting session info...\n","Extracting session information...\n","  Raw session path: /root/Downloads/ONE/openalyx.internationalbrainlab.org/hoferlab/Subjects/SWC_043/2020-09-19/001\n","✓ Session info extracted:\n","  experimentId: 88d24c31-52e4-49cc-9f32-6adbeb9eba87\n","  subject: SWC_043\n","  date: 2020-09-19\n","  sessionNumber: 001\n","  lab: hoferlab\n","  taskProtocol: _iblrig_tasks_ephysChoiceWorld6.4.2\n","  numberOfTrials: 547\n","  sessionPath: /root/Downloads/ONE/openalyx.internationalbrainlab.org/hoferlab/Subjects/SWC_043/2020-09-19/001\n","  dataRepository: Unknown\n","✓ Completed: Extracting session info\n","\n","Validating trial data...\n","Validating and extracting trial data...\n","✓ Trial data loaded: 547 trials\n","  Available features: 15\n","  ✓ All required trial features present\n","  Trial duration stats:\n","    Mean: 4.91s\n","    Min: 2.11s\n","    Max: 63.17s\n","✓ Completed: Validating trial data\n","\n","Validating pose data...\n","Validating and extracting pose data...\n","✓ Pose data loaded: 260646 timepoints\n","  Available features: 34\n","  ✓ All required pose features present\n","  Data completeness check:\n","    times: 260646/260646 (100.0%)\n","    nose_tip_x: 258598/260646 (99.2%)\n","    nose_tip_y: 258598/260646 (99.2%)\n","    pupil_top_r_x: 260645/260646 (100.0%)\n","    pupil_top_r_y: 260645/260646 (100.0%)\n","    pupil_right_r_x: 259827/260646 (99.7%)\n","    pupil_right_r_y: 259827/260646 (99.7%)\n","    pupil_bottom_r_x: 38920/260646 (14.9%)\n","    pupil_bottom_r_y: 38920/260646 (14.9%)\n","    pupil_left_r_x: 260515/260646 (99.9%)\n","    pupil_left_r_y: 260515/260646 (99.9%)\n","    tube_top_x: 257723/260646 (98.9%)\n","    tube_top_y: 257723/260646 (98.9%)\n","    tube_bottom_x: 245489/260646 (94.2%)\n","    tube_bottom_y: 245489/260646 (94.2%)\n","    tongue_end_l_x: 22126/260646 (8.5%)\n","    tongue_end_l_y: 22126/260646 (8.5%)\n","    tongue_end_r_x: 24987/260646 (9.6%)\n","    tongue_end_r_y: 24987/260646 (9.6%)\n","  Temporal coverage: 4360.85 seconds\n","✓ Completed: Validating pose data\n","\n","Extracting behavioral data...\n","Extracting additional behavioral data...\n","✓ Wheel data: 4418069 timepoints\n","✓ Pose data loaded for cameras: ['leftCamera', 'rightCamera', 'bodyCamera']\n","  Feature analysis across 3 cameras:\n","    Common features (1): ['times']\n","    leftCamera unique features (33): ['nose_tip_likelihood', 'nose_tip_x', 'nose_tip_y', 'paw_l_likelihood', 'paw_l_x', 'paw_l_y', 'paw_r_likelihood', 'paw_r_x', 'paw_r_y', 'pupil_bottom_r_likelihood', 'pupil_bottom_r_x', 'pupil_bottom_r_y', 'pupil_left_r_likelihood', 'pupil_left_r_x', 'pupil_left_r_y', 'pupil_right_r_likelihood', 'pupil_right_r_x', 'pupil_right_r_y', 'pupil_top_r_likelihood', 'pupil_top_r_x', 'pupil_top_r_y', 'tongue_end_l_likelihood', 'tongue_end_l_x', 'tongue_end_l_y', 'tongue_end_r_likelihood', 'tongue_end_r_x', 'tongue_end_r_y', 'tube_bottom_likelihood', 'tube_bottom_x', 'tube_bottom_y', 'tube_top_likelihood', 'tube_top_x', 'tube_top_y']\n","    rightCamera unique features (33): ['nose_tip_likelihood', 'nose_tip_x', 'nose_tip_y', 'paw_l_likelihood', 'paw_l_x', 'paw_l_y', 'paw_r_likelihood', 'paw_r_x', 'paw_r_y', 'pupil_bottom_r_likelihood', 'pupil_bottom_r_x', 'pupil_bottom_r_y', 'pupil_left_r_likelihood', 'pupil_left_r_x', 'pupil_left_r_y', 'pupil_right_r_likelihood', 'pupil_right_r_x', 'pupil_right_r_y', 'pupil_top_r_likelihood', 'pupil_top_r_x', 'pupil_top_r_y', 'tongue_end_l_likelihood', 'tongue_end_l_x', 'tongue_end_l_y', 'tongue_end_r_likelihood', 'tongue_end_r_x', 'tongue_end_r_y', 'tube_bottom_likelihood', 'tube_bottom_x', 'tube_bottom_y', 'tube_top_likelihood', 'tube_top_x', 'tube_top_y']\n","    bodyCamera unique features (3): ['tail_start_likelihood', 'tail_start_x', 'tail_start_y']\n","    Pupil tracking available in: ['leftCamera', 'rightCamera']\n","      leftCamera: ['pupil_right_r_x', 'pupil_right_r_y', 'pupil_top_r_x', 'pupil_top_r_y', 'pupil_left_r_y', 'pupil_bottom_r_x', 'pupil_left_r_x', 'pupil_bottom_r_likelihood', 'pupil_left_r_likelihood', 'pupil_bottom_r_y', 'pupil_right_r_likelihood', 'pupil_top_r_likelihood']\n","      rightCamera: ['pupil_right_r_x', 'pupil_right_r_y', 'pupil_top_r_x', 'pupil_top_r_y', 'pupil_left_r_y', 'pupil_bottom_r_x', 'pupil_left_r_x', 'pupil_bottom_r_likelihood', 'pupil_left_r_likelihood', 'pupil_bottom_r_y', 'pupil_right_r_likelihood', 'pupil_top_r_likelihood']\n","✓ Motion energy data loaded for cameras: ['leftCamera', 'rightCamera', 'bodyCamera']\n","    leftCamera: ['whiskerMotionEnergy']\n","    rightCamera: ['whiskerMotionEnergy']\n","    bodyCamera: ['whiskerMotionEnergy']\n","✓ Completed: Extracting behavioral data\n","\n","Loading video timestamps...\n","Loading video timestamps...\n","Video frame counts: {'left': 260646, 'right': 654111, 'body': 130553}\n","! Cameras have different numbers of frames (this is normal)\n","  left: 260646 frames / 4344.10s = 60.00 fps → rounded to 60 fps\n","  right: 654111 frames / 4360.74s = 150.00 fps → rounded to 150 fps\n","  body: 130553 frames / 4351.77s = 30.00 fps → rounded to 30 fps\n","\n","Preserving original camera timing (NO frame rate standardization):\n","  This maintains natural timing relationships between cameras\n","  left: 260646 frames at 60.0 fps (PRESERVED)\n","  right: 654111 frames at 150.0 fps (PRESERVED)\n","  body: 130553 frames at 30.0 fps (PRESERVED)\n","\n","✓ Video processing completed:\n","  Original timestamps preserved for all cameras\n","  No frame rate standardization applied\n","  Primary video time base: left camera (260646 frames)\n","✓ URLs loaded for all cameras\n","✓ Completed: Loading video timestamps\n","\n","============================================================\n","SESSION LOADING COMPLETED SUCCESSFULLY\n","============================================================\n","Session data loaded successfully - 547 trials available\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 1/547:   0%|          | 0/547 [00:00<?, ?trial/s]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 157 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 0\n","    Low quality features (<20%): 6\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y', 'tongue_end_l_x']...\n","✓ Wheel data extracted: 7696 timepoints\n","  Time range: 47.934 - 55.629s\n","  Position range: 15.118927 - 20.302208\n","  Valid positions: 7696\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 157 timepoints\n","  Time range: 53.016 - 55.626s\n","  Data range: 0.788 - 122.040\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 2/547:   0%|          | 1/547 [05:25<49:19:46, 325.25s/trial, Success=1/1, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 388 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 0\n","    Low quality features (<20%): 6\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y', 'tongue_end_l_x']...\n","✓ Wheel data extracted: 6501 timepoints\n","  Time range: 56.029 - 62.529s\n","  Position range: 19.753151 - 24.718536\n","  Valid positions: 6501\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 388 timepoints\n","  Time range: 56.044 - 62.519s\n","  Data range: 0.926 - 16.511\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 3/547:   0%|          | 2/547 [18:43<91:23:06, 603.64s/trial, Success=2/2, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 242 timepoints\n","    High quality features (>80%): 14\n","    Medium quality features (20-80%): 4\n","    Low quality features (<20%): 0\n","✓ Wheel data extracted: 4055 timepoints\n","  Time range: 62.958 - 67.012s\n","  Position range: 21.463482 - 23.629414\n","  Valid positions: 4055\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 242 timepoints\n","  Time range: 62.971 - 67.003s\n","  Data range: 2.529 - 16.503\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 4/547:   1%|          | 3/547 [27:26<85:38:19, 566.73s/trial, Success=3/3, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 176 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 4\n","    Low quality features (<20%): 2\n","    Low quality features will rely on interpolation: ['tongue_end_l_x', 'tongue_end_l_y']\n","✓ Wheel data extracted: 2952 timepoints\n","  Time range: 67.411 - 70.362s\n","  Position range: 21.630667 - 22.475857\n","  Valid positions: 2952\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 176 timepoints\n","  Time range: 67.421 - 70.349s\n","  Data range: 1.206 - 14.584\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 5/547:   1%|          | 4/547 [33:51<74:37:36, 494.76s/trial, Success=4/4, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 155 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 6\n","    Low quality features (<20%): 0\n","✓ Wheel data extracted: 2582 timepoints\n","  Time range: 70.797 - 73.378s\n","  Position range: 22.471037 - 23.237619\n","  Valid positions: 2582\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 155 timepoints\n","  Time range: 70.801 - 73.377s\n","  Data range: 4.360 - 14.256\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 6/547:   1%|          | 5/547 [39:22<65:36:58, 435.83s/trial, Success=5/5, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 1000 timepoints\n","    High quality features (>80%): 14\n","    Medium quality features (20-80%): 0\n","    Low quality features (<20%): 4\n","    Low quality features will rely on interpolation: ['tongue_end_l_x', 'tongue_end_l_y', 'tongue_end_r_x']...\n","✓ Wheel data extracted: 16720 timepoints\n","  Time range: 73.760 - 90.479s\n","  Position range: 23.487770 - 39.723949\n","  Valid positions: 16720\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 1000 timepoints\n","  Time range: 73.762 - 90.476s\n","  Data range: 0.757 - 18.343\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 7/547:   1%|          | 6/547 [1:14:54<152:10:55, 1012.67s/trial, Success=6/6, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 247 timepoints\n","    High quality features (>80%): 14\n","    Medium quality features (20-80%): 0\n","    Low quality features (<20%): 4\n","    Low quality features will rely on interpolation: ['tongue_end_l_x', 'tongue_end_l_y', 'tongue_end_r_x']...\n","✓ Wheel data extracted: 4120 timepoints\n","  Time range: 91.059 - 95.178s\n","  Position range: 40.171467 - 42.980591\n","  Valid positions: 4120\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 247 timepoints\n","  Time range: 91.062 - 95.178s\n","  Data range: 0.885 - 14.108\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 8/547:   1%|▏         | 7/547 [1:23:44<128:14:15, 854.92s/trial, Success=7/7, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 685 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 2\n","    Low quality features (<20%): 4\n","    Low quality features will rely on interpolation: ['tongue_end_l_x', 'tongue_end_l_y', 'tongue_end_r_x']...\n","✓ Wheel data extracted: 11463 timepoints\n","  Time range: 95.567 - 107.029s\n","  Position range: 43.095997 - 60.576862\n","  Valid positions: 11463\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 685 timepoints\n","  Time range: 95.579 - 107.023s\n","  Data range: 1.445 - 24.460\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 9/547:   1%|▏         | 8/547 [1:47:49<156:05:52, 1042.58s/trial, Success=8/8, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 259 timepoints\n","    High quality features (>80%): 10\n","    Medium quality features (20-80%): 6\n","    Low quality features (<20%): 2\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y']\n","✓ Wheel data extracted: 4320 timepoints\n","  Time range: 107.559 - 111.878s\n","  Position range: 56.901493 - 59.340073\n","  Valid positions: 4320\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 259 timepoints\n","  Time range: 107.559 - 111.875s\n","  Data range: 3.074 - 15.583\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 10/547:   2%|▏         | 9/547 [1:57:19<133:43:23, 894.80s/trial, Success=9/9, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 146 timepoints\n","    High quality features (>80%): 10\n","    Medium quality features (20-80%): 6\n","    Low quality features (<20%): 2\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y']\n","✓ Wheel data extracted: 2442 timepoints\n","  Time range: 112.271 - 114.712s\n","  Position range: 56.893814 - 58.956993\n","  Valid positions: 2442\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 146 timepoints\n","  Time range: 112.277 - 114.703s\n","  Data range: 1.063 - 13.203\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 11/547:   2%|▏         | 10/547 [2:02:32<106:42:32, 715.37s/trial, Success=10/10, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 269 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 4\n","    Low quality features (<20%): 2\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y']\n","✓ Wheel data extracted: 4491 timepoints\n","  Time range: 115.087 - 119.577s\n","  Position range: 54.460945 - 59.631897\n","  Valid positions: 4491\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 269 timepoints\n","  Time range: 115.088 - 119.571s\n","  Data range: 1.232 - 15.509\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 12/547:   2%|▏         | 11/547 [2:12:23<100:49:17, 677.16s/trial, Success=11/11, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 152 timepoints\n","    High quality features (>80%): 10\n","    Medium quality features (20-80%): 6\n","    Low quality features (<20%): 2\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y']\n","✓ Wheel data extracted: 2544 timepoints\n","  Time range: 119.985 - 122.528s\n","  Position range: 52.744411 - 54.589771\n","  Valid positions: 2544\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 152 timepoints\n","  Time range: 119.990 - 122.516s\n","  Data range: 0.943 - 15.084\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 13/547:   2%|▏         | 12/547 [2:17:47<84:40:50, 569.81s/trial, Success=12/12, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 136 timepoints\n","    High quality features (>80%): 10\n","    Medium quality features (20-80%): 6\n","    Low quality features (<20%): 2\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y']\n","✓ Wheel data extracted: 2275 timepoints\n","  Time range: 122.920 - 125.194s\n","  Position range: 52.225914 - 52.839500\n","  Valid positions: 2275\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 136 timepoints\n","  Time range: 122.934 - 125.193s\n","  Data range: 2.922 - 14.401\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 14/547:   2%|▏         | 13/547 [2:22:36<71:53:37, 484.68s/trial, Success=13/13, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 382 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 2\n","    Low quality features (<20%): 4\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y', 'tongue_end_l_x']...\n","✓ Wheel data extracted: 6395 timepoints\n","  Time range: 125.634 - 132.028s\n","  Position range: 47.074806 - 51.735302\n","  Valid positions: 6395\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 382 timepoints\n","  Time range: 125.645 - 132.019s\n","  Data range: 1.054 - 15.705\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 15/547:   3%|▎         | 14/547 [2:36:13<86:36:42, 584.99s/trial, Success=14/14, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 177 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 0\n","    Low quality features (<20%): 6\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y', 'tongue_end_l_x']...\n","✓ Wheel data extracted: 2963 timepoints\n","  Time range: 132.466 - 135.428s\n","  Position range: 46.277176 - 47.562603\n","  Valid positions: 2963\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 177 timepoints\n","  Time range: 132.471 - 135.416s\n","  Data range: 1.051 - 14.173\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 16/547:   3%|▎         | 15/547 [2:42:26<77:01:09, 521.18s/trial, Success=15/15, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 278 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 0\n","    Low quality features (<20%): 6\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y', 'tongue_end_l_x']...\n","✓ Wheel data extracted: 4644 timepoints\n","  Time range: 135.817 - 140.460s\n","  Position range: 44.867466 - 48.081009\n","  Valid positions: 4644\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 278 timepoints\n","  Time range: 135.817 - 140.452s\n","  Data range: 0.909 - 14.471\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 17/547:   3%|▎         | 16/547 [2:52:13<79:47:29, 540.96s/trial, Success=16/16, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 261 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 2\n","    Low quality features (<20%): 4\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y', 'tongue_end_l_x']...\n","✓ Wheel data extracted: 4365 timepoints\n","  Time range: 140.864 - 145.228s\n","  Position range: 42.669258 - 45.562275\n","  Valid positions: 4365\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 261 timepoints\n","  Time range: 140.870 - 145.220s\n","  Data range: 0.722 - 16.043\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 18/547:   3%|▎         | 17/547 [3:01:29<80:18:31, 545.49s/trial, Success=17/17, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 270 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 4\n","    Low quality features (<20%): 2\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y']\n","✓ Wheel data extracted: 4510 timepoints\n","  Time range: 145.634 - 150.143s\n","  Position range: 40.506313 - 43.209164\n","  Valid positions: 4510\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 270 timepoints\n","  Time range: 145.638 - 150.139s\n","  Data range: 0.855 - 13.935\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 19/547:   3%|▎         | 18/547 [3:10:52<80:55:58, 550.77s/trial, Success=18/18, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 142 timepoints\n","    High quality features (>80%): 10\n","    Medium quality features (20-80%): 6\n","    Low quality features (<20%): 2\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y']\n","✓ Wheel data extracted: 2379 timepoints\n","  Time range: 150.616 - 152.994s\n","  Position range: 39.354294 - 40.742500\n","  Valid positions: 2379\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 142 timepoints\n","  Time range: 150.624 - 152.983s\n","  Data range: 1.062 - 14.707\n"]},{"name":"stderr","output_type":"stream","text":["Processing trial 20/547:   3%|▎         | 19/547 [3:15:54<69:50:34, 476.20s/trial, Success=19/19, Failed=0]"]},{"name":"stdout","output_type":"stream","text":["  Pose data in interval: 465 timepoints\n","    High quality features (>80%): 12\n","    Medium quality features (20-80%): 4\n","    Low quality features (<20%): 2\n","    Low quality features will rely on interpolation: ['pupil_bottom_r_x', 'pupil_bottom_r_y']\n","✓ Wheel data extracted: 7781 timepoints\n","  Time range: 153.365 - 161.145s\n","  Position range: 37.459881 - 40.595261\n","  Valid positions: 7781\n","Motion energy data type: <class 'pandas.core.frame.DataFrame'>\n","Motion energy DataFrame columns: ['times', 'whiskerMotionEnergy']\n","Using column 'whiskerMotionEnergy' for whisker motion energy\n","✓ Motion energy extracted: 465 timepoints\n","  Time range: 153.368 - 161.131s\n","  Data range: 0.990 - 15.668\n"]}],"source":["animateAllTrials(eid, BASE_DIR)"]},{"cell_type":"markdown","metadata":{"id":"6ygtRKCD4BhZ"},"source":["# Good Result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1a6lPQo_B2OIo9ZQQmDqWQNg_R5UDKF9c"},"executionInfo":{"elapsed":2340810,"status":"ok","timestamp":1750825507180,"user":{"displayName":"Andrew Liao","userId":"01433793926959815213"},"user_tz":240},"id":"0Ux_fOrkXbAx","outputId":"1b60c143-cf55-4b6f-fc6f-2532be6c769a"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Then animate inline\n","success = quickAnimate(dataLoader, mode='test', fps=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qpA0Y9AMcb9"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["OIsx7X6C_TfC","6ygtRKCD4BhZ"],"provenance":[],"authorship_tag":"ABX9TyOb96Og26iGBAFBQhqywqON"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}