{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTyUik_6E0C8"
      },
      "source": [
        "# Goal\n",
        "\n",
        "For each pid with at least 1 region containing >30 neurons, train linear decoder, obtain stats, save to drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD02nyfkQsUL"
      },
      "source": [
        "Each pid info is saved as follows after loading:\n",
        "```\n",
        "{\n",
        "    \"pid\": pid,\n",
        "    \"eid\": eid,\n",
        "    \"n_neurons\": int,\n",
        "    \"regions\": dict,\n",
        "    \"spike_matrix\": np.ndarray,     # (neurons × timebins)\n",
        "    \"bin_edges\": np.ndarray,\n",
        "    \"bin_centers\": np.ndarray,\n",
        "    \"whisker_motion\": np.ndarray,   # raw energy trace resampled to bins\n",
        "    \"cluster_regions\": np.ndarray,  # region acronym per neuron\n",
        "    \"cluster_ids\": np.ndarray,      # neuron ids\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0LgZjEIVlYw"
      },
      "source": [
        "| Dataset                                  | Purpose                                                                        |\n",
        "| ---------------------------------------- | ------------------------------------------------------------------------------ |\n",
        "| `whisker_motion_raw`                     | raw, unmodified motion energy for visualization or inverse transform           |\n",
        "| `whisker_motion_clean`                   | log-transformed, z-scored motion target for modeling                           |\n",
        "| `transform_params`                       | to reconstruct predictions to physical scale                                   |\n",
        "| `X_train`, `X_test`, `y_train`, `y_test` | modeling datasets                                                              |\n",
        "| `meta`                                   | dictionary with fields like `pid`, `n_neurons`, `regions`, `lags`, `cut`, etc. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFQ-ti2bE_2H"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMUPEVbKFnxl",
        "outputId": "4f956212-04b9-46a9-e542-0a5e0c763589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ONE-api\n",
            "  Downloading one_api-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: ruff in /usr/local/lib/python3.12/dist-packages (from ONE-api) (0.14.4)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.12/dist-packages (from ONE-api) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from ONE-api) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.32.1 in /usr/local/lib/python3.12/dist-packages (from ONE-api) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from ONE-api) (2.32.4)\n",
            "Collecting iblutil>=1.14.0 (from ONE-api)\n",
            "  Downloading iblutil-1.20.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ONE-api) (25.0)\n",
            "Collecting boto3 (from ONE-api)\n",
            "  Downloading boto3-1.40.73-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from ONE-api) (6.0.3)\n",
            "Collecting colorlog>=6.0.0 (from iblutil>=1.14.0->ONE-api)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numba>0.53.1 in /usr/local/lib/python3.12/dist-packages (from iblutil>=1.14.0->ONE-api) (0.60.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from iblutil>=1.14.0->ONE-api) (18.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from iblutil>=1.14.0->ONE-api) (1.16.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->ONE-api) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->ONE-api) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->ONE-api) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ONE-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ONE-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ONE-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ONE-api) (2025.10.5)\n",
            "Collecting botocore<1.41.0,>=1.40.73 (from boto3->ONE-api)\n",
            "  Downloading botocore-1.40.73-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->ONE-api)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3->ONE-api)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>0.53.1->iblutil>=1.14.0->ONE-api) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->ONE-api) (1.17.0)\n",
            "Downloading one_api-3.4.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iblutil-1.20.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.73-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.73-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, colorlog, botocore, s3transfer, iblutil, boto3, ONE-api\n",
            "Successfully installed ONE-api-3.4.1 boto3-1.40.73 botocore-1.40.73 colorlog-6.10.1 iblutil-1.20.0 jmespath-1.0.1 s3transfer-0.14.0\n",
            "Collecting ibllib\n",
            "  Downloading ibllib-3.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.12/dist-packages (from ibllib) (1.40.73)\n",
            "Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from ibllib) (8.3.0)\n",
            "Requirement already satisfied: colorlog>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from ibllib) (6.10.1)\n",
            "Collecting flake8>=3.7.8 (from ibllib)\n",
            "  Downloading flake8-7.3.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting globus-sdk (from ibllib)\n",
            "  Downloading globus_sdk-4.1.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from ibllib) (0.21)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.12/dist-packages (from ibllib) (3.10.0)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.12/dist-packages (from ibllib) (0.60.0)\n",
            "Requirement already satisfied: numpy<=2.2,>=1.18 in /usr/local/lib/python3.12/dist-packages (from ibllib) (2.0.2)\n",
            "Collecting nptdms (from ibllib)\n",
            "  Downloading nptdms-1.10.0.tar.gz (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from ibllib) (4.12.0.88)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ibllib) (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from ibllib) (18.1.0)\n",
            "Collecting pynrrd>=0.4.0 (from ibllib)\n",
            "  Downloading pynrrd-1.1.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from ibllib) (8.4.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from ibllib) (2.32.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.12/dist-packages (from ibllib) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from ibllib) (1.16.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from ibllib) (0.25.2)\n",
            "Collecting imagecodecs (from ibllib)\n",
            "  Downloading imagecodecs-2025.11.11-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (20 kB)\n",
            "Collecting sparse (from ibllib)\n",
            "  Downloading sparse-0.17.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ibllib) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.32.1 in /usr/local/lib/python3.12/dist-packages (from ibllib) (4.67.1)\n",
            "Collecting iblatlas>=0.5.3 (from ibllib)\n",
            "  Downloading iblatlas-0.10.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting ibl-neuropixel>=1.6.2 (from ibllib)\n",
            "  Downloading ibl_neuropixel-1.9.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: iblutil>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from ibllib) (1.20.0)\n",
            "Collecting iblqt>=0.4.2 (from ibllib)\n",
            "  Downloading iblqt-0.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting mtscomp>=1.0.1 (from ibllib)\n",
            "  Downloading mtscomp-1.0.2-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: ONE-api>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from ibllib) (3.4.1)\n",
            "Collecting phylib>=2.6.0 (from ibllib)\n",
            "  Downloading phylib-2.6.3-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting psychofit (from ibllib)\n",
            "  Downloading Psychofit-1.0.0.post0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting slidingRP>=1.1.1 (from ibllib)\n",
            "  Downloading slidingRP-1.1.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting pyqt5 (from ibllib)\n",
            "  Downloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting ibl-style (from ibllib)\n",
            "  Downloading ibl_style-0.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.7.8->ibllib)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pycodestyle<2.15.0,>=2.14.0 (from flake8>=3.7.8->ibllib)\n",
            "  Downloading pycodestyle-2.14.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pyflakes<3.5.0,>=3.4.0 (from flake8>=3.7.8->ibllib)\n",
            "  Downloading pyflakes-3.4.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from ibl-neuropixel>=1.6.2->ibllib) (1.5.2)\n",
            "Collecting qtpy>=2.4.1 (from iblqt>=0.4.2->ibllib)\n",
            "  Downloading QtPy-2.4.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyqtgraph>=0.13.7 (from iblqt>=0.4.2->ibllib)\n",
            "  Downloading pyqtgraph-0.13.7-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from iblutil>=1.13.0->ibllib) (25.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.3->ibllib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.3->ibllib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.3->ibllib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.3->ibllib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.3->ibllib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.3->ibllib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.3->ibllib) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.56->ibllib) (0.43.0)\n",
            "Requirement already satisfied: ruff in /usr/local/lib/python3.12/dist-packages (from ONE-api>=3.2.0->ibllib) (0.14.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from ONE-api>=3.2.0->ibllib) (6.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ibllib) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ibllib) (2025.2)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.12/dist-packages (from phylib>=2.6.0->ibllib) (2025.9.1)\n",
            "Collecting responses (from phylib>=2.6.0->ibllib)\n",
            "  Downloading responses-0.25.8-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toolz in /usr/local/lib/python3.12/dist-packages (from phylib>=2.6.0->ibllib) (0.12.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pynrrd>=0.4.0->ibllib) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ibllib) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ibllib) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ibllib) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ibllib) (2025.10.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.1->ibllib) (3.6.0)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.12/dist-packages (from slidingRP>=1.1.1->ibllib) (3.1.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (from slidingRP>=1.1.1->ibllib) (0.14.5)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.73 in /usr/local/lib/python3.12/dist-packages (from boto3->ibllib) (1.40.73)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3->ibllib) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from boto3->ibllib) (0.14.0)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]<3.0.0,>=2.0.0->globus-sdk->ibllib) (2.10.1)\n",
            "Requirement already satisfied: cryptography!=3.4.0,>=3.3.1 in /usr/local/lib/python3.12/dist-packages (from globus-sdk->ibllib) (43.0.3)\n",
            "Collecting figrid (from ibl-style->ibllib)\n",
            "  Downloading figrid-0.1.7-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting PyQt5-sip<13,>=12.15 (from pyqt5->ibllib)\n",
            "  Downloading pyqt5_sip-12.17.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (494 bytes)\n",
            "Collecting PyQt5-Qt5<5.16.0,>=5.15.2 (from pyqt5->ibllib)\n",
            "  Downloading pyqt5_qt5-5.15.18-py3-none-manylinux2014_x86_64.whl.metadata (536 bytes)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->ibllib) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->ibllib) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest->ibllib) (2.19.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->ibllib) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->ibllib) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->ibllib) (2025.10.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->ibllib) (0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography!=3.4.0,>=3.3.1->globus-sdk->ibllib) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.3->ibllib) (1.17.0)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dask->phylib>=2.6.0->ibllib) (3.1.2)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.12/dist-packages (from dask->phylib>=2.6.0->ibllib) (2025.3.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask->phylib>=2.6.0->ibllib) (1.4.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels->slidingRP>=1.1.1->ibllib) (1.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography!=3.4.0,>=3.3.1->globus-sdk->ibllib) (2.23)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.12/dist-packages (from partd>=1.4.0->dask->phylib>=2.6.0->ibllib) (1.0.0)\n",
            "Downloading ibllib-3.4.1-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flake8-7.3.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ibl_neuropixel-1.9.1-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iblatlas-0.10.0-py3-none-any.whl (206 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.9/206.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iblqt-0.8.2-py3-none-any.whl (21 kB)\n",
            "Downloading mtscomp-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Downloading phylib-2.6.3-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.5/80.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynrrd-1.1.3-py3-none-any.whl (23 kB)\n",
            "Downloading slidingRP-1.1.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading globus_sdk-4.1.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.6/404.6 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ibl_style-0.1.0-py3-none-any.whl (11 kB)\n",
            "Downloading imagecodecs-2025.11.11-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Psychofit-1.0.0.post0-py3-none-any.whl (5.7 kB)\n",
            "Downloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sparse-0.17.0-py2.py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading pycodestyle-2.14.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading pyflakes-3.4.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyqt5_qt5-5.15.18-py3-none-manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyqt5_sip-12.17.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.whl (282 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.2/282.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyqtgraph-0.13.7-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading QtPy-2.4.3-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.0/95.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading figrid-0.1.7-py3-none-any.whl (7.1 kB)\n",
            "Downloading responses-0.25.8-py3-none-any.whl (34 kB)\n",
            "Building wheels for collected packages: nptdms\n",
            "  Building wheel for nptdms (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nptdms: filename=nptdms-1.10.0-py3-none-any.whl size=108456 sha256=f3457beb2ed05bfc76d2f33d2f9aac891b83806ad17c8bae4f7c878e0791b153\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/06/c1/f72c8e132b2d7b2bae5bff097aa2b80086fa9873c2b1bbc98c\n",
            "Successfully built nptdms\n",
            "Installing collected packages: PyQt5-Qt5, mtscomp, qtpy, pyqtgraph, PyQt5-sip, pynrrd, pyflakes, pycodestyle, nptdms, mccabe, imagecodecs, sparse, responses, pyqt5, psychofit, flake8, phylib, slidingRP, globus-sdk, figrid, ibl-style, iblqt, iblatlas, ibl-neuropixel, ibllib\n",
            "Successfully installed PyQt5-Qt5-5.15.18 PyQt5-sip-12.17.1 figrid-0.1.7 flake8-7.3.0 globus-sdk-4.1.0 ibl-neuropixel-1.9.1 ibl-style-0.1.0 iblatlas-0.10.0 ibllib-3.4.1 iblqt-0.8.2 imagecodecs-2025.11.11 mccabe-0.7.0 mtscomp-1.0.2 nptdms-1.10.0 phylib-2.6.3 psychofit-1.0.0.post0 pycodestyle-2.14.0 pyflakes-3.4.0 pynrrd-1.1.3 pyqt5-5.15.11 pyqtgraph-0.13.7 qtpy-2.4.3 responses-0.25.8 slidingRP-1.1.1 sparse-0.17.0\n",
            "Connected to https://openalyx.internationalbrainlab.org as user \"intbrainlab\"\n"
          ]
        }
      ],
      "source": [
        "! pip install ONE-api\n",
        "! pip install ibllib\n",
        "\n",
        "from one.api import ONE\n",
        "ONE.setup(base_url='https://openalyx.internationalbrainlab.org', silent=True)\n",
        "one = ONE(password='international')\n",
        "\n",
        "from one.api import ONE\n",
        "one = ONE()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyg7BVIyFDil"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from one.api import ONE\n",
        "from brainbox.io.one import SessionLoader, SpikeSortingLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from iblatlas.atlas import AllenAtlas\n",
        "import numpy as np\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjp-hgsfFD-c",
        "outputId": "5dffc863-405a-4737-e8ca-19c45bdade70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfedtxuNFCLX"
      },
      "outputs": [],
      "source": [
        "# ---------- Config ----------\n",
        "ALPHA = 0.5                     # Ridge regularization\n",
        "TEST_FRAC = 0.2                 # 80/20 temporal split\n",
        "LAGS = np.arange(0, 30)\n",
        "MIN_NEURONS_PER_REGION = 30     # at least one region with >= 30 neurons\n",
        "MIN_TOTAL_NEURONS = 30          # sanity floor; effectively redundant given above\n",
        "\n",
        "RANDOM_STATE = 0\n",
        "\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/S25/Langone/Breathing/experiments/decoding/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se31bySuGfuB"
      },
      "outputs": [],
      "source": [
        "# Final summary CSV (single giant table)\n",
        "SUMMARY_CSV = os.path.join(OUTPUT_DIR, \"ibl_neural_decoding_summary.csv\")\n",
        "CANDIDATES_CSV = os.path.join(OUTPUT_DIR, \"ibl_eid_pid_candidates.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHpzFcySG9L8"
      },
      "outputs": [],
      "source": [
        "SUMMARY_COLUMNS = [\n",
        "    \"pid\",\n",
        "    \"n_neurons\",              # total neurons used in this decoding\n",
        "    \"regions\",                # JSON: {region_acronym: n_neurons, ...}\n",
        "\n",
        "    \"train_R2_all\",\n",
        "    \"train_R_all\",\n",
        "    \"test_R2_all\",\n",
        "    \"test_R_all\",\n",
        "\n",
        "    # JSON: {region: {\"R2\": 0.123, \"R\": 0.456}, ...}\n",
        "    \"train_region_metrics\",\n",
        "    \"test_region_metrics\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENdaccgME-K2"
      },
      "source": [
        "# helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzHdCj8jmr7s"
      },
      "source": [
        "## utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw_6N8Chhg61"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# UTILITIES\n",
        "# ============================================================================\n",
        "\n",
        "def print_skip_messages(captured_output):\n",
        "    \"\"\"Extract and print only skip/warning messages.\"\"\"\n",
        "    for line in captured_output.split('\\n'):\n",
        "        if '[SKIP]' in line or '[WARN]' in line:\n",
        "            print(line)\n",
        "\n",
        "def round5(x):\n",
        "    \"\"\"Round to 5 decimal places. Returns None for NaN/None.\"\"\"\n",
        "    if x is None:\n",
        "        return None\n",
        "    try:\n",
        "        if np.isnan(x):\n",
        "            return None\n",
        "    except (TypeError, ValueError):\n",
        "        pass\n",
        "    return float(np.round(x, 5))\n",
        "\n",
        "def to_json(obj):\n",
        "    \"\"\"Convert dict to JSON string for CSV storage.\"\"\"\n",
        "    return json.dumps(obj, separators=(\",\", \":\"), sort_keys=True)\n",
        "\n",
        "def get_all_pids(one, project=None):\n",
        "    \"\"\"\n",
        "    Return a list of pids that have spike sorting (spikes.times present).\n",
        "    Optionally restrict by project if you only care about a specific IBL dataset,\n",
        "    e.g. project='ibl_neuropixel_brainwide_01'.\n",
        "    \"\"\"\n",
        "    search_kwargs = dict(dataset='spikes.times')\n",
        "    if project is not None:\n",
        "        search_kwargs['project'] = project\n",
        "\n",
        "    try:\n",
        "        pids = one.search_insertions(**search_kwargs)\n",
        "    except AttributeError:\n",
        "        # Fallback: query Alyx directly\n",
        "        insertions = one.alyx.rest(\n",
        "            'insertions', 'list',\n",
        "            dataset='spikes.times',\n",
        "            project=project\n",
        "        )\n",
        "        pids = [ins['id'] for ins in insertions]\n",
        "\n",
        "    print(f\"[INFO] Found {len(pids)} pids with spikes.times\")\n",
        "    return list(sorted(set(pids)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMBIJ8tgmtWW"
      },
      "source": [
        "## data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DyhYgaehlR9"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "def load_session_data(pid, one, ba, min_neurons=30):\n",
        "    \"\"\"\n",
        "    Load spike and whisker motion data for one probe insertion.\n",
        "    Returns dict with spike_matrix, whisker_motion (raw & clean), and metadata.\n",
        "    Returns None if session should be skipped.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Processing pid: {pid}\")\n",
        "\n",
        "        # Get experiment ID\n",
        "        eid_info = one.pid2eid(pid)\n",
        "        eid = eid_info[0] if isinstance(eid_info, (tuple, list)) else eid_info\n",
        "\n",
        "        # Load spike sorting\n",
        "        ssl = SpikeSortingLoader(one=one, pid=pid, atlas=ba)\n",
        "        spikes, clusters, channels = ssl.load_spike_sorting()\n",
        "        clusters = ssl.merge_clusters(spikes, clusters, channels)\n",
        "\n",
        "        # Filter good clusters\n",
        "        good_clusters = np.where(clusters['label'] == 1)[0]\n",
        "        if len(good_clusters) == 0:\n",
        "            print(f\"[SKIP] pid={pid}: no good clusters.\", flush=True)\n",
        "            return None\n",
        "\n",
        "        cluster_regions = clusters['acronym'][good_clusters]\n",
        "        region_counts = dict(Counter(cluster_regions))\n",
        "        total_units = len(good_clusters)\n",
        "        top_region, top_count = max(region_counts.items(), key=lambda kv: kv[1])\n",
        "\n",
        "        if top_count < min_neurons:\n",
        "            print(f\"[SKIP] pid={pid} n_neurons={total_units} max_region={top_region} ({top_count})\", flush=True)\n",
        "            return None\n",
        "\n",
        "        # Load whisker motion\n",
        "        try:\n",
        "            sl = SessionLoader(one=one, eid=eid)\n",
        "            sl.load_motion_energy(views=['left'])\n",
        "            whisker = sl.motion_energy['leftCamera']\n",
        "            whisker_times = np.asarray(whisker['times'])\n",
        "            whisker_trace = np.asarray(whisker['whiskerMotionEnergy'])\n",
        "\n",
        "            mask = np.isfinite(whisker_times)\n",
        "            whisker_times, whisker_trace = whisker_times[mask], whisker_trace[mask]\n",
        "            whisker_times, unique_idx = np.unique(whisker_times, return_index=True)\n",
        "            whisker_trace = whisker_trace[unique_idx]\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] pid={pid}: whisker motion load failed - {e}\", flush=True)\n",
        "            return None\n",
        "\n",
        "        # Bin spikes\n",
        "        bin_size = np.median(np.diff(whisker_times))\n",
        "        if not np.isfinite(bin_size) or bin_size <= 0:\n",
        "            print(f\"[WARN] pid={pid}: invalid bin size.\", flush=True)\n",
        "            return None\n",
        "\n",
        "        bin_edges = np.arange(whisker_times[0], whisker_times[-1] + bin_size, bin_size)\n",
        "        bin_centers = bin_edges[:-1] + np.diff(bin_edges) / 2\n",
        "        n_bins = len(bin_edges) - 1\n",
        "\n",
        "        spike_matrix = np.zeros((len(good_clusters), n_bins), dtype=np.uint16)\n",
        "        for i, cid in enumerate(good_clusters):\n",
        "            spike_times = spikes['times'][spikes['clusters'] == cid]\n",
        "            if len(spike_times) > 0:\n",
        "                counts = np.histogram(spike_times, bins=bin_edges)[0]\n",
        "                spike_matrix[i] = np.clip(counts, 0, 65535).astype(np.uint16)\n",
        "\n",
        "        # Resample and align whisker motion\n",
        "        whisker_sync = resample_to_bins(whisker_times, whisker_trace, bin_centers)\n",
        "        n_frames = min(spike_matrix.shape[1], len(whisker_sync))\n",
        "        spike_matrix = spike_matrix[:, :n_frames]\n",
        "        whisker_sync = whisker_sync[:n_frames]\n",
        "        times = bin_centers[:n_frames]\n",
        "\n",
        "        # Clean motion and store transform params\n",
        "        whisker_clean, transform_params = clean_motion_energy(whisker_sync.copy(), log_transform=True)\n",
        "\n",
        "        print(f\"✓ pid={pid} neurons={spike_matrix.shape[0]} frames={n_frames}\", flush=True)\n",
        "\n",
        "        return {\n",
        "            'pid': pid,\n",
        "            'eid': eid,\n",
        "            'n_neurons': total_units,\n",
        "            'regions': region_counts,\n",
        "            'spike_matrix': spike_matrix,\n",
        "            'whisker_motion_raw': whisker_sync,\n",
        "            'whisker_motion_clean': whisker_clean,\n",
        "            'transform_params': transform_params,\n",
        "            'cluster_regions': cluster_regions,\n",
        "            'times': times,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        print(f\"[WARN] pid={pid}: {type(e).__name__} — {e}\", flush=True)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asmOdV-ImnBm"
      },
      "source": [
        "## preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asIFz3a4UXZE"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PREPROCESSING - With lightweight progress statements\n",
        "# ============================================================================\n",
        "\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Helper functions (no output) ---\n",
        "\n",
        "def resample_to_bins(signal_times, signal_values, bin_centers):\n",
        "    \"\"\"Resample continuous signal to bin centers via interpolation.\"\"\"\n",
        "    signal_times = np.asarray(signal_times, dtype=np.float32)\n",
        "    signal_values = np.asarray(signal_values, dtype=np.float32)\n",
        "\n",
        "    mask = np.isfinite(signal_times) & np.isfinite(signal_values)\n",
        "    signal_times, signal_values = signal_times[mask], signal_values[mask]\n",
        "\n",
        "    if len(signal_times) == 0:\n",
        "        return np.full(len(bin_centers), np.nan, dtype=np.float32)\n",
        "\n",
        "    signal_times, unique_idx = np.unique(signal_times, return_index=True)\n",
        "    signal_values = signal_values[unique_idx]\n",
        "\n",
        "    return np.interp(bin_centers, signal_times, signal_values).astype(np.float32)\n",
        "\n",
        "\n",
        "def clean_motion_energy(motion, zmax=3, pct=99.5, log_transform=True):\n",
        "    \"\"\"\n",
        "    Clean and normalize whisker motion: clip outliers, log transform, z-score.\n",
        "    Returns cleaned motion and transform_params for inverse transform.\n",
        "    \"\"\"\n",
        "    motion = np.asarray(motion, dtype=np.float32)\n",
        "\n",
        "    hi = np.percentile(motion, pct)\n",
        "    np.clip(motion, None, hi, out=motion)\n",
        "\n",
        "    if log_transform:\n",
        "        motion += 1.0\n",
        "        np.log(motion, out=motion)\n",
        "\n",
        "    mean, std = np.nanmean(motion), np.nanstd(motion)\n",
        "    motion -= mean\n",
        "    motion /= std\n",
        "    motion[np.abs(motion) > zmax] = np.nan\n",
        "    np.nan_to_num(motion, copy=False, nan=0.0)\n",
        "\n",
        "    return motion, {\n",
        "        'log_transform': log_transform,\n",
        "        'mean': mean,\n",
        "        'std': std,\n",
        "        'log_epsilon': 1.0\n",
        "    }\n",
        "\n",
        "\n",
        "def inverse_transform_motion(y_pred_clean, transform_params):\n",
        "    \"\"\"\n",
        "    Convert predictions from z-scored, logged space back to raw scale.\n",
        "    Reverses: z-score → log transform.\n",
        "    \"\"\"\n",
        "    y_pred_clean = np.asarray(y_pred_clean, dtype=np.float32)\n",
        "    y_pred_log = y_pred_clean * transform_params['std'] + transform_params['mean']\n",
        "\n",
        "    if transform_params['log_transform']:\n",
        "        return np.exp(y_pred_log) - transform_params['log_epsilon']\n",
        "    return y_pred_log\n",
        "\n",
        "\n",
        "def add_lags(X_TxF, lags=None):\n",
        "    \"\"\"Stack past frames as features. Returns (X_lagged, maxlag).\"\"\"\n",
        "    if lags is None or len(lags) == 0:\n",
        "        return X_TxF, 0\n",
        "\n",
        "    lags = [int(L) for L in lags]\n",
        "    maxlag = int(np.max(lags))\n",
        "    T, F = X_TxF.shape\n",
        "    T_out = T - maxlag\n",
        "\n",
        "    X_lagged = np.zeros((T_out, F * len(lags)), dtype=X_TxF.dtype)\n",
        "    for i, L in enumerate(lags):\n",
        "        start_idx = maxlag - L\n",
        "        X_lagged[:, i*F:(i+1)*F] = X_TxF[start_idx:start_idx + T_out]\n",
        "\n",
        "    return X_lagged, maxlag\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Core preprocessing with disk-backed lag builder\n",
        "# ============================================================================\n",
        "\n",
        "def add_lags_memmap(X_TxF, lags, out_dir=Path(\"/content/mmap_cache\")):\n",
        "    \"\"\"\n",
        "    Build lagged features FAST: construct in RAM, then write once to memmap.\n",
        "\n",
        "    For 150 neurons × 30 lags × 240k timesteps:\n",
        "    - ~5-10 seconds (vs 2-3 minutes with loop + repeated flushes)\n",
        "\n",
        "    Returns (np.memmap array, maxlag, out_path)\n",
        "    \"\"\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    lags = np.asarray(lags, dtype=int)\n",
        "    maxlag = int(lags.max())\n",
        "    T, F = X_TxF.shape\n",
        "    T_out = T - maxlag\n",
        "    n_lags = len(lags)\n",
        "\n",
        "    #print(f\"[LAGS] {n_lags} lags: {T}×{F} → {T_out}×{F*n_lags} ({T_out*F*n_lags*4/1e9:.2f}GB)...\")\n",
        "\n",
        "    # --- Step 1: Build lagged matrix IN MEMORY (fast indexing) ---\n",
        "    X_lagged_ram = np.empty((T_out, F * n_lags), dtype=np.float32)\n",
        "\n",
        "    for i, L in enumerate(lags):\n",
        "        start = maxlag - L\n",
        "        end = start + T_out\n",
        "        X_lagged_ram[:, i*F:(i+1)*F] = X_TxF[start:end]\n",
        "        #if (i + 1) % 10 == 0:\n",
        "            #print(f\"  → {i+1}/{n_lags}\")\n",
        "\n",
        "    #print(f\"[LAGS] writing to disk...\")\n",
        "\n",
        "    # --- Step 2: Write once to memmap (sequential, efficient) ---\n",
        "    out_path = out_dir / f\"lags_{np.random.randint(1e9)}.mmap\"\n",
        "    X_lagged_mm = np.memmap(out_path, dtype=np.float32, mode='w+', shape=(T_out, F * n_lags))\n",
        "\n",
        "    # Write in chunks to avoid peak RAM spike\n",
        "    CHUNK = max(1, int(64_000_000 // (F * n_lags)))\n",
        "    for s in range(0, T_out, CHUNK):\n",
        "        e = min(T_out, s + CHUNK)\n",
        "        X_lagged_mm[s:e] = X_lagged_ram[s:e]\n",
        "\n",
        "    X_lagged_mm.flush()\n",
        "    del X_lagged_ram  # Free the RAM copy\n",
        "\n",
        "    # Re-open read-only\n",
        "    X_lagged_mm = np.memmap(out_path, dtype=np.float32, mode='r', shape=(T_out, F * n_lags))\n",
        "    #print(f\"[LAGS] ✓\")\n",
        "\n",
        "    return X_lagged_mm, maxlag, out_path\n",
        "\n",
        "\n",
        "def preprocess_session(sess, lags=np.arange(0, 30).tolist(), smooth_sigma=1.0,\n",
        "                       sqrt_counts=True, test_frac=0.2, normalization='standard',\n",
        "                       verbose=False):\n",
        "    \"\"\"\n",
        "    Preprocess a single session: load, clean, lag, normalize, split.\n",
        "    \"\"\"\n",
        "    pid = sess[\"pid\"]\n",
        "\n",
        "    # Load motion\n",
        "    #print(f\"[PREP] {pid} | loading motion...\")\n",
        "    m_clean = np.asarray(sess[\"whisker_motion_clean\"], dtype=np.float32)\n",
        "    m_raw   = np.asarray(sess[\"whisker_motion_raw\"], dtype=np.float32)\n",
        "    transform_params = sess[\"transform_params\"]\n",
        "\n",
        "    # Spike preprocessing\n",
        "    #print(f\"[PREP] {pid} | loading spikes...\")\n",
        "    S = sess[\"spike_matrix\"].astype(np.float32, copy=False)\n",
        "\n",
        "    if sqrt_counts:\n",
        "        #print(f\"[PREP] {pid} | sqrt counts...\")\n",
        "        np.sqrt(S, out=S)\n",
        "\n",
        "    if smooth_sigma and smooth_sigma > 0:\n",
        "        #print(f\"[PREP] {pid} | smoothing σ={smooth_sigma}...\")\n",
        "        S = gaussian_filter1d(S, sigma=smooth_sigma, axis=1)\n",
        "\n",
        "    # Time-major transpose\n",
        "    #print(f\"[PREP] {pid} | transpose...\")\n",
        "    X_raw = S.T\n",
        "    del S\n",
        "\n",
        "    # Add lags via memmap\n",
        "    #print(f\"[PREP] {pid} | adding {len(lags)} lags...\")\n",
        "    X, cut, lag_path = add_lags_memmap(X_raw, lags)\n",
        "    del X_raw\n",
        "\n",
        "    # Align targets\n",
        "    #print(f\"[PREP] {pid} | align targets...\")\n",
        "    y_clean = m_clean[cut:cut + len(X)]\n",
        "    y_raw   = m_raw[cut:cut + len(X)]\n",
        "\n",
        "    # Drop NaNs\n",
        "    mask = np.isfinite(y_clean)\n",
        "    if (~mask).any():\n",
        "        n_nan = (~mask).sum()\n",
        "        #print(f\"[PREP] {pid} | dropping {n_nan} NaNs...\")\n",
        "        X       = X[mask]\n",
        "        y_clean = y_clean[mask]\n",
        "        y_raw   = y_raw[mask]\n",
        "\n",
        "    # Split train/test\n",
        "    #print(f\"[PREP] {pid} | splitting train/test...\")\n",
        "    split = int((1 - test_frac) * len(y_clean))\n",
        "    X_train_mm, X_test_mm = X[:split], X[split:]\n",
        "    y_train, y_test = y_clean[:split], y_clean[split:]\n",
        "    y_train_raw, y_test_raw = y_raw[:split], y_raw[split:]\n",
        "    del X, y_clean, y_raw\n",
        "\n",
        "    # Copy memmap to RAM for normalization (read-only memmap can't be modified in-place)\n",
        "    #print(f\"[PREP] {pid} | copying to RAM...\")\n",
        "    X_train = np.array(X_train_mm, dtype=np.float32, copy=True)\n",
        "    X_test = np.array(X_test_mm, dtype=np.float32, copy=True)\n",
        "    del X_train_mm, X_test_mm\n",
        "\n",
        "    # Normalization\n",
        "    #print(f\"[PREP] {pid} | normalizing ({normalization})...\")\n",
        "    if normalization == 'standard':\n",
        "        mu  = X_train.mean(axis=0, dtype=np.float32)\n",
        "        var = ((X_train - mu) ** 2).mean(axis=0, dtype=np.float32)\n",
        "        std = np.sqrt(var, dtype=np.float32)\n",
        "        std[std < 1e-6] = 1.0\n",
        "        X_train -= mu;  X_train /= std\n",
        "        X_test  -= mu;  X_test  /= std\n",
        "        scale_params = {\"mean\": mu, \"std\": std}\n",
        "    elif normalization == 'layer':\n",
        "        def norm_layer_inplace(X):\n",
        "            m = X.mean(axis=1, keepdims=True, dtype=np.float32)\n",
        "            s2 = ((X - m) ** 2).mean(axis=1, keepdims=True, dtype=np.float32)\n",
        "            s = np.sqrt(s2, dtype=np.float32)\n",
        "            s[s < 1e-6] = 1.0\n",
        "            X -= m; X /= s\n",
        "        norm_layer_inplace(X_train)\n",
        "        norm_layer_inplace(X_test)\n",
        "        scale_params = None\n",
        "    else:\n",
        "        scale_params = None\n",
        "\n",
        "    meta = {\n",
        "        'pid': pid,\n",
        "        'n_neurons': sess[\"n_neurons\"],\n",
        "        'regions': sess[\"regions\"],\n",
        "        'cluster_regions': sess[\"cluster_regions\"],\n",
        "        'lags': lags,\n",
        "        'cut': cut,\n",
        "        'transform_params': transform_params,\n",
        "        'scale_params': scale_params,\n",
        "        'lag_path': str(lag_path),  # For cleanup after training\n",
        "    }\n",
        "\n",
        "    print(f\"[PREP] {pid} | ✓ train={X_train.shape} test={X_test.shape}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, y_train_raw, y_test_raw, meta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAzRM01Cmo5I"
      },
      "source": [
        "## training and eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb2L0L1YUnD7"
      },
      "outputs": [],
      "source": [
        "def compute_region_metrics(y_true, y_pred, X_features, cluster_regions, lags):\n",
        "    \"\"\"Create feature masks for per-region analysis.\"\"\"\n",
        "    n_lags = len(lags)\n",
        "    region_metrics = {}\n",
        "\n",
        "    for region in np.unique(cluster_regions):\n",
        "        region_mask = (cluster_regions == region)\n",
        "        if region_mask.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        feature_mask = np.zeros(X_features.shape[1], dtype=bool)\n",
        "        for neuron_idx in np.where(region_mask)[0]:\n",
        "            feature_mask[neuron_idx * n_lags:(neuron_idx + 1) * n_lags] = True\n",
        "\n",
        "        region_metrics[region] = {'feature_mask': feature_mask}\n",
        "\n",
        "    return region_metrics\n",
        "\n",
        "\n",
        "def compute_region_predictions(model, X_features, region_metrics):\n",
        "    \"\"\"Compute predictions using only neurons from each region.\"\"\"\n",
        "    region_predictions = {}\n",
        "    w = model[\"coef_\"]\n",
        "\n",
        "    for region, info in region_metrics.items():\n",
        "        X_region = X_features.copy()\n",
        "        X_region[:, ~info['feature_mask']] = 0\n",
        "        region_predictions[region] = X_region @ w\n",
        "\n",
        "    return region_predictions\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def train_and_evaluate_session(\n",
        "    sess, meta, X_train, X_test, y_train, y_test,\n",
        "    y_train_raw, y_test_raw, alpha=0.5,\n",
        "    chunk=20000, verbose=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Ridge regression via chunked normal equations (XᵀX, Xᵀy).\n",
        "    Peak RAM ≈ O(n_features²), not O(n_samples·n_features).\n",
        "\n",
        "    Returns (model_dict, summary_row, predictions)\n",
        "    \"\"\"\n",
        "    pid = sess[\"pid\"]\n",
        "    n_samples, n_features = X_train.shape\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[TRAIN] pid={pid} | {n_samples}×{n_features} | α={alpha}\")\n",
        "\n",
        "    # --- Build Gram matrix & RHS incrementally ---\n",
        "    G = np.zeros((n_features, n_features), dtype=np.float32)\n",
        "    b = np.zeros(n_features, dtype=np.float32)\n",
        "\n",
        "    for s in range(0, n_samples, chunk):\n",
        "        e = min(n_samples, s + chunk)\n",
        "        Xc = X_train[s:e].astype(np.float32, copy=False)\n",
        "        yc = y_train[s:e].astype(np.float32, copy=False)\n",
        "        G += Xc.T @ Xc\n",
        "        b += Xc.T @ yc\n",
        "\n",
        "    G += alpha * np.eye(n_features, dtype=np.float32)\n",
        "\n",
        "    # --- Solve (G + αI)w = b ---\n",
        "    w = la.solve(G.astype(np.float64), b.astype(np.float64), assume_a='pos').astype(np.float32)\n",
        "    del G, b\n",
        "\n",
        "    # --- Predictions (chunked to keep RAM flat) ---\n",
        "    def predict_chunked(X, w, chunk=20000):\n",
        "        y_pred = np.empty(X.shape[0], dtype=np.float32)\n",
        "        for s in range(0, X.shape[0], chunk):\n",
        "            e = min(X.shape[0], s + chunk)\n",
        "            y_pred[s:e] = X[s:e] @ w\n",
        "        return y_pred\n",
        "\n",
        "    y_train_pred = predict_chunked(X_train, w, chunk)\n",
        "    y_test_pred  = predict_chunked(X_test,  w, chunk)\n",
        "\n",
        "    # --- Inverse-transform to raw scale ---\n",
        "    y_train_pred_raw = inverse_transform_motion(y_train_pred, meta['transform_params'])\n",
        "    y_test_pred_raw  = inverse_transform_motion(y_test_pred,  meta['transform_params'])\n",
        "\n",
        "    # --- Overall metrics ---\n",
        "    train_R2 = r2_score(y_train_raw, y_train_pred_raw)\n",
        "    train_R  = pearsonr(y_train_raw, y_train_pred_raw)[0]\n",
        "    test_R2  = r2_score(y_test_raw,  y_test_pred_raw)\n",
        "    test_R   = pearsonr(y_test_raw,  y_test_pred_raw)[0]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[EVAL] pid={pid} | Train R²={train_R2:.3f} | Test R²={test_R2:.3f}\")\n",
        "\n",
        "    # --- Per-region metrics ---\n",
        "    region_info = compute_region_metrics(y_train_raw, y_train_pred_raw, X_train,\n",
        "                                         meta['cluster_regions'], meta['lags'])\n",
        "\n",
        "    model_dict = {\"coef_\": w}\n",
        "\n",
        "    train_region_metrics = {}\n",
        "    for region, y_pred in compute_region_predictions(model_dict, X_train, region_info).items():\n",
        "        y_pred_raw = inverse_transform_motion(y_pred, meta['transform_params'])\n",
        "        train_region_metrics[region] = {\n",
        "            \"R2\": round5(r2_score(y_train_raw, y_pred_raw)),\n",
        "            \"R\":  round5(pearsonr(y_train_raw, y_pred_raw)[0])\n",
        "        }\n",
        "\n",
        "    test_region_metrics = {}\n",
        "    for region, y_pred in compute_region_predictions(model_dict, X_test, region_info).items():\n",
        "        y_pred_raw = inverse_transform_motion(y_pred, meta['transform_params'])\n",
        "        test_region_metrics[region] = {\n",
        "            \"R2\": round5(r2_score(y_test_raw, y_pred_raw)),\n",
        "            \"R\":  round5(pearsonr(y_test_raw, y_pred_raw)[0])\n",
        "        }\n",
        "\n",
        "    # --- Timeline predictions for downstream analysis ---\n",
        "    times_full = sess['times']\n",
        "    cut = meta['cut']\n",
        "    times_aligned = times_full[cut:cut + len(y_train_raw) + len(y_test_raw)]\n",
        "\n",
        "    predictions = [\n",
        "        {\"pid\": str(pid), \"time\": round5(t), \"y_true\": round5(yt),\n",
        "         \"y_pred\": round5(yp), \"split\": split}\n",
        "        for split, (ts, ys, yp) in zip(\n",
        "            [\"train\", \"test\"],\n",
        "            [(times_aligned[:len(y_train_raw)], y_train_raw, y_train_pred_raw),\n",
        "             (times_aligned[len(y_train_raw):], y_test_raw, y_test_pred_raw)]\n",
        "        )\n",
        "        for t, yt, yp in zip(ts, ys, yp)\n",
        "    ]\n",
        "\n",
        "    # --- Summary row (for CSV) ---\n",
        "    summary_row = {\n",
        "        \"pid\": str(pid),\n",
        "        \"n_neurons\": meta['n_neurons'],\n",
        "        \"regions\": to_json(meta['regions']),\n",
        "        \"train_R2_all\": round5(train_R2),\n",
        "        \"train_R_all\": round5(train_R),\n",
        "        \"test_R2_all\":  round5(test_R2),\n",
        "        \"test_R_all\":   round5(test_R),\n",
        "        \"train_region_metrics\": to_json(train_region_metrics),\n",
        "        \"test_region_metrics\":  to_json(test_region_metrics),\n",
        "    }\n",
        "\n",
        "    # Compact model object\n",
        "    model = {\"coef_\": w, \"alpha\": alpha}\n",
        "\n",
        "    return model, summary_row, predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j23dqwd5m23v"
      },
      "source": [
        "## file IO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpdqPUMbV9g3"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FILE I/O\n",
        "# ============================================================================\n",
        "\n",
        "def init_summary_csv(path):\n",
        "    \"\"\"Initialize CSV with headers if it doesn't exist.\"\"\"\n",
        "    if not Path(path).exists():\n",
        "        df = pd.DataFrame(columns=[\n",
        "            \"pid\", \"n_neurons\", \"regions\", \"train_R2_all\", \"train_R_all\",\n",
        "            \"test_R2_all\", \"test_R_all\", \"train_region_metrics\", \"test_region_metrics\"\n",
        "        ])\n",
        "        df.to_csv(path, index=False)\n",
        "\n",
        "\n",
        "def append_to_summary_csv(path, row_dict):\n",
        "    \"\"\"Append a single row to summary CSV.\"\"\"\n",
        "    pd.DataFrame([row_dict]).to_csv(path, mode='a', header=False, index=False)\n",
        "\n",
        "\n",
        "def save_predictions_batch(predictions_list, output_file):\n",
        "    \"\"\"Append predictions to parquet file.\"\"\"\n",
        "    df_new = pd.DataFrame(predictions_list)\n",
        "    if Path(output_file).exists():\n",
        "        df_existing = pd.read_parquet(output_file)\n",
        "        df_new = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "    df_new.to_parquet(output_file, index=False)\n",
        "\n",
        "\n",
        "def load_predictions_for_pid(pid, predictions_file):\n",
        "    \"\"\"Load predictions for a specific pid for plotting.\"\"\"\n",
        "    df = pd.read_parquet(predictions_file)\n",
        "    return df[df['pid'] == str(pid)].drop(columns=['pid'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adVKgILEm65z"
      },
      "source": [
        "## viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pBqXTNhOzUo"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def plot_prediction_window(y_true, y_pred, times, window_start_sec=10,\n",
        "                           window_duration_sec=10, split_name=\"test\", pid=None):\n",
        "    \"\"\"Plot 10-second window of predictions vs ground truth.\"\"\"\n",
        "    start_time = times[0] + window_start_sec\n",
        "    end_time = start_time + window_duration_sec\n",
        "    mask = (times >= start_time) & (times <= end_time)\n",
        "\n",
        "    if mask.sum() == 0:\n",
        "        print(f\"[WARN] No data in window [{start_time:.1f}, {end_time:.1f}]\")\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))\n",
        "    ax.plot(times[mask], y_true[mask], 'k-', linewidth=1.5, label='Ground Truth', alpha=0.7)\n",
        "    ax.plot(times[mask], y_pred[mask], 'r-', linewidth=1.5, label='Predicted', alpha=0.7)\n",
        "    ax.set_xlabel('Time (s)')\n",
        "    ax.set_ylabel('Whisker Motion Energy')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    title = f\"{split_name.capitalize()}: {window_start_sec}s-{window_start_sec+window_duration_sec}s\"\n",
        "    if pid:\n",
        "        title = f\"PID: {pid}\\n{title}\"\n",
        "    ax.set_title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    corr = np.corrcoef(y_true[mask], y_pred[mask])[0, 1]\n",
        "    print(f\"\\nWindow stats: Correlation={corr:.5f}, \"\n",
        "          f\"True range=[{y_true[mask].min():.3f}, {y_true[mask].max():.3f}], \"\n",
        "          f\"Pred range=[{y_pred[mask].min():.3f}, {y_pred[mask].max():.3f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_4889XbMiMC"
      },
      "outputs": [],
      "source": [
        "def reconstruct_timeline(\n",
        "    sess,\n",
        "    y_train,\n",
        "    y_test,\n",
        "    y_train_pred,\n",
        "    y_test_pred,\n",
        "    lags=(0,1,2,3,4,5),\n",
        "    motion_key=\"whisker_motion_clean\"\n",
        "):\n",
        "    \"\"\"Map train/test predictions back to original session timeline.\"\"\"\n",
        "    maxlag = max(lags) if lags else 0\n",
        "\n",
        "    times_full = np.asarray(sess[\"times\"])\n",
        "    y_full = np.asarray(sess[motion_key], dtype=float)\n",
        "\n",
        "    # Apply lag cutoff and NaN filtering (matching prepare_session)\n",
        "    times_aligned = times_full[maxlag:maxlag + len(y_full) - maxlag]\n",
        "    y_aligned = y_full[maxlag:maxlag + len(y_full) - maxlag]\n",
        "\n",
        "    mask = np.isfinite(y_aligned)\n",
        "    times_aligned = times_aligned[mask]\n",
        "\n",
        "    # Split timeline\n",
        "    n_train = len(y_train)\n",
        "    times_train = times_aligned[:n_train]\n",
        "    times_test = times_aligned[n_train:n_train + len(y_test)]\n",
        "    split_time = times_train[-1] if len(times_train) > 0 else times_test[0]\n",
        "\n",
        "    return {\n",
        "        'times_full': times_full,\n",
        "        'times_train': times_train,\n",
        "        'times_test': times_test,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test,\n",
        "        'y_train_pred': y_train_pred,\n",
        "        'y_test_pred': y_test_pred,\n",
        "        'split_time': split_time,\n",
        "        't_end_test': times_test[-1] if len(times_test) > 0 else None\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X4gJ29RE_Ge"
      },
      "source": [
        "# main loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9NAfjZPJB-l",
        "outputId": "92285fc8-3766-486e-cb32-0447e303f8b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Found 1179 pids with spikes.times\n"
          ]
        }
      ],
      "source": [
        "all_pids = get_all_pids(one, project=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMk01LwZfgV0",
        "outputId": "b9d0833e-4e7e-4d22-e2d7-d8d1a58b3d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: /root/Downloads/ONE/openalyx.internationalbrainlab.org/histology/ATLAS/Needles/Allen/average_template_25.nrrd Bytes: 32998960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31.470260620117188/31.470260620117188 [00:01<00:00, 17.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: /root/Downloads/ONE/openalyx.internationalbrainlab.org/histology/ATLAS/Needles/Allen/annotation_25.nrrd Bytes: 4035363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.848422050476074/3.848422050476074 [00:00<00:00,  4.34it/s]\n"
          ]
        }
      ],
      "source": [
        "one = ONE()\n",
        "ba = AllenAtlas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBw85RiMXneY"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MAIN PIPELINE - Cleaned\n",
        "# ============================================================================\n",
        "\n",
        "import warnings\n",
        "import json\n",
        "import gc\n",
        "import io\n",
        "import contextlib\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Configuration ---\n",
        "LOCAL_SUMMARY_CSV = \"decoding_summary1.csv\"\n",
        "LOCAL_SKIPPED_CSV = \"skipped1.csv\"\n",
        "SYNC_INTERVAL = 25\n",
        "\n",
        "DRIVE_DIR = Path(\"/content/drive/MyDrive/fullrun/\")\n",
        "DRIVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DRIVE_SUMMARY_CSV = DRIVE_DIR / \"decoding_summary1.csv\"\n",
        "DRIVE_SKIPPED_CSV = DRIVE_DIR / \"skipped1.csv\"\n",
        "\n",
        "MMAP_DIR = Path(\"/content/mmap_cache\")\n",
        "MMAP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Utilities ---\n",
        "\n",
        "def init_outputs_if_missing():\n",
        "    \"\"\"Initialize output CSV files with headers.\"\"\"\n",
        "    if not DRIVE_SUMMARY_CSV.exists():\n",
        "        pd.DataFrame(columns=[\n",
        "            \"pid\", \"n_neurons\", \"regions\", \"train_R2_all\", \"train_R_all\",\n",
        "            \"test_R2_all\", \"test_R_all\", \"train_region_metrics\", \"test_region_metrics\"\n",
        "        ]).to_csv(DRIVE_SUMMARY_CSV, index=False)\n",
        "    if not DRIVE_SKIPPED_CSV.exists():\n",
        "        pd.DataFrame(columns=[\"pid\", \"reason\"]).to_csv(DRIVE_SKIPPED_CSV, index=False)\n",
        "\n",
        "\n",
        "def append_to_csv(csv_path, row_dict):\n",
        "    \"\"\"Append a row to CSV file (create if missing).\"\"\"\n",
        "    file_exists = Path(csv_path).exists() and Path(csv_path).stat().st_size > 0\n",
        "    pd.DataFrame([row_dict]).to_csv(csv_path, mode=\"a\", header=not file_exists, index=False)\n",
        "\n",
        "\n",
        "def append_to_skipped_log(pid, reason):\n",
        "    \"\"\"Log a skipped PID with reason.\"\"\"\n",
        "    append_to_csv(LOCAL_SKIPPED_CSV, {\"pid\": str(pid), \"reason\": reason})\n",
        "\n",
        "\n",
        "def sync_to_drive_and_cleanup():\n",
        "    \"\"\"Sync local CSVs to Google Drive and clean up.\"\"\"\n",
        "    try:\n",
        "        if Path(LOCAL_SUMMARY_CSV).exists() and Path(LOCAL_SUMMARY_CSV).stat().st_size > 0:\n",
        "            local_df = pd.read_csv(LOCAL_SUMMARY_CSV)\n",
        "            if not local_df.empty:\n",
        "                local_df.to_csv(DRIVE_SUMMARY_CSV, mode=\"a\", header=False, index=False)\n",
        "                Path(LOCAL_SUMMARY_CSV).unlink()\n",
        "\n",
        "        if Path(LOCAL_SKIPPED_CSV).exists() and Path(LOCAL_SKIPPED_CSV).stat().st_size > 0:\n",
        "            local_df = pd.read_csv(LOCAL_SKIPPED_CSV)\n",
        "            if not local_df.empty:\n",
        "                local_df.to_csv(DRIVE_SKIPPED_CSV, mode=\"a\", header=False, index=False)\n",
        "                Path(LOCAL_SKIPPED_CSV).unlink()\n",
        "\n",
        "        gc.collect()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[SYNC ERROR] {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# --- Memmap Helpers ---\n",
        "\n",
        "def _as_float32_array(x):\n",
        "    \"\"\"Convert input to float32 array (minimal copy).\"\"\"\n",
        "    if isinstance(x, pd.DataFrame):\n",
        "        return x.to_numpy(dtype=np.float32, copy=False)\n",
        "    x = np.asarray(x)\n",
        "    if x.dtype != np.float32:\n",
        "        return x.astype(np.float32, copy=False)\n",
        "    return x\n",
        "\n",
        "\n",
        "def _to_memmap(arr: np.ndarray, shape, fname: Path):\n",
        "    \"\"\"Write array to disk-backed memmap (chunked to keep RAM flat).\"\"\"\n",
        "    mm = np.memmap(fname, dtype=np.float32, mode='w+', shape=shape)\n",
        "    # Write in chunks: ~64M elements / cols = ~256MB per chunk @ float32\n",
        "    CHUNK = max(1, int(64_000_000 // shape[1]))\n",
        "    start = 0\n",
        "    while start < shape[0]:\n",
        "        end = min(shape[0], start + CHUNK)\n",
        "        mm[start:end] = arr[start:end]\n",
        "        start = end\n",
        "    mm.flush()\n",
        "    # Re-open read-only to prevent accidental copies\n",
        "    return np.memmap(fname, dtype=np.float32, mode='r', shape=shape)\n",
        "\n",
        "\n",
        "def _safe_unlink(path: Path):\n",
        "    \"\"\"Safely remove file if it exists.\"\"\"\n",
        "    try:\n",
        "        if path.exists():\n",
        "            path.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "# --- Core Processing ---\n",
        "\n",
        "def process_pid(pid, one, ba):\n",
        "    \"\"\"\n",
        "    Process a single PID: load → preprocess → train → evaluate.\n",
        "    Returns (summary_row, skipped_flag)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        fbuf = io.StringIO()\n",
        "        with contextlib.redirect_stdout(fbuf), contextlib.redirect_stderr(fbuf):\n",
        "            sess = load_session_data(pid, one, ba)\n",
        "        captured = fbuf.getvalue()\n",
        "\n",
        "        if sess is None:\n",
        "            lines = [l for l in captured.splitlines() if ('[SKIP]' in l or '[WARN]' in l)]\n",
        "            reason = lines[0] if lines else \"Unknown reason\"\n",
        "            append_to_skipped_log(pid, reason)\n",
        "            return None, True\n",
        "\n",
        "        X_train, X_test, y_train, y_test, ytr_raw, yte_raw, meta = preprocess_session(sess)\n",
        "\n",
        "        # Save lag_path to delete later\n",
        "        lag_path = meta.get('lag_path')\n",
        "\n",
        "        # --- Downcast & memmap to cap RAM ---\n",
        "        X_train = _as_float32_array(X_train)\n",
        "        X_test  = _as_float32_array(X_test)\n",
        "        y_train = _as_float32_array(y_train).reshape(-1, 1)\n",
        "        y_test  = _as_float32_array(y_test).reshape(-1, 1)\n",
        "\n",
        "        # Unique file names per PID\n",
        "        uid = uuid.uuid4().hex[:8]\n",
        "        fXtr = MMAP_DIR / f\"Xtr_{uid}.mmap\"\n",
        "        fXte = MMAP_DIR / f\"Xte_{uid}.mmap\"\n",
        "        fytr = MMAP_DIR / f\"ytr_{uid}.mmap\"\n",
        "        fyte = MMAP_DIR / f\"yte_{uid}.mmap\"\n",
        "\n",
        "        # Persist to disk-backed arrays\n",
        "        X_train_mm = _to_memmap(X_train, X_train.shape, fXtr)\n",
        "        X_test_mm  = _to_memmap(X_test,  X_test.shape,  fXte)\n",
        "        y_train_mm = _to_memmap(y_train, y_train.shape, fytr)\n",
        "        y_test_mm  = _to_memmap(y_test,  y_test.shape,  fyte)\n",
        "\n",
        "        # Free original dense arrays ASAP\n",
        "        del X_train, X_test, y_train, y_test\n",
        "        gc.collect()\n",
        "\n",
        "        # --- Training on memmap arrays (read-only views) ---\n",
        "        model, summary_row, _ = train_and_evaluate_session(\n",
        "            sess, meta,\n",
        "            X_train_mm, X_test_mm,\n",
        "            y_train_mm.ravel(), y_test_mm.ravel(),\n",
        "            ytr_raw, yte_raw,\n",
        "            alpha=5,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        append_to_csv(LOCAL_SUMMARY_CSV, summary_row)\n",
        "\n",
        "        # Cleanup heavy objects immediately\n",
        "        del sess, model, meta, ytr_raw, yte_raw, X_train_mm, X_test_mm, y_train_mm, y_test_mm\n",
        "        gc.collect()\n",
        "\n",
        "        # Remove memmap files\n",
        "        _safe_unlink(fXtr)\n",
        "        _safe_unlink(fXte)\n",
        "        _safe_unlink(fytr)\n",
        "        _safe_unlink(fyte)\n",
        "        if lag_path:\n",
        "            _safe_unlink(Path(lag_path))\n",
        "\n",
        "        return summary_row, False\n",
        "\n",
        "    except Exception as e:\n",
        "        append_to_skipped_log(pid, f\"FAILED: {type(e).__name__} — {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, True\n",
        "\n",
        "\n",
        "# --- Main Pipeline ---\n",
        "\n",
        "def main(all_pids, one, ba):\n",
        "    \"\"\"Main driver for full decoding pipeline.\"\"\"\n",
        "    init_outputs_if_missing()\n",
        "    skipped_count, trained_count = 0, 0\n",
        "\n",
        "    pbar = tqdm(all_pids, desc=\"Processing PIDs\", unit=\"pid\")\n",
        "\n",
        "    for idx, pid in enumerate(pbar, 1):\n",
        "        summary_row, skipped = process_pid(pid, one, ba)\n",
        "        gc.collect()\n",
        "\n",
        "        if skipped or summary_row is None:\n",
        "            skipped_count += 1\n",
        "            pbar.set_postfix_str(f\"SKIPPED | {trained_count}✓ {skipped_count}✗\")\n",
        "        else:\n",
        "            trained_count += 1\n",
        "            r2 = summary_row.get('test_R2_all', np.nan)\n",
        "            pbar.set_postfix_str(f\"R²={r2:.3f} | {trained_count}✓ {skipped_count}✗\")\n",
        "\n",
        "        # Periodic sync to Drive\n",
        "        if idx % SYNC_INTERVAL == 0:\n",
        "            synced = sync_to_drive_and_cleanup()\n",
        "            msg = \"Synced\" if synced else \"Sync failed\"\n",
        "            pbar.write(f\"[SYNC] {msg} after {idx} PIDs\")\n",
        "\n",
        "    # Final sync\n",
        "    sync_ok = sync_to_drive_and_cleanup()\n",
        "    print(\"\\n[SYNC] Final sync \" + (\"completed\" if sync_ok else \"failed\"))\n",
        "\n",
        "    # Summary statistics\n",
        "    total = trained_count + skipped_count\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"=== Pipeline Summary ===\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Total PIDs: {total}\")\n",
        "    print(f\"  ✓ Trained: {trained_count}\")\n",
        "    print(f\"  ✗ Skipped: {skipped_count}\")\n",
        "\n",
        "    # Compute overall metrics from Drive CSV\n",
        "    try:\n",
        "        if DRIVE_SUMMARY_CSV.exists() and DRIVE_SUMMARY_CSV.stat().st_size > 0:\n",
        "            df_stats = pd.read_csv(DRIVE_SUMMARY_CSV)\n",
        "            if not df_stats.empty and \"test_R2_all\" in df_stats.columns:\n",
        "                mu = df_stats[\"test_R2_all\"].mean()\n",
        "                sd = df_stats[\"test_R2_all\"].std()\n",
        "                idxmax = df_stats[\"test_R2_all\"].idxmax()\n",
        "                best_pid = df_stats.loc[idxmax, \"pid\"]\n",
        "                best_r2 = df_stats.loc[idxmax, \"test_R2_all\"]\n",
        "                print(f\"\\nPerformance Summary:\")\n",
        "                print(f\"  Mean R²: {mu:.5f} ± {sd:.5f}\")\n",
        "                print(f\"  Best: {best_pid} (R²={best_r2:.5f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not load summary stats: {e}\")\n",
        "\n",
        "    print(\"\\nOutputs:\")\n",
        "    print(f\"  - Summary CSV: {DRIVE_SUMMARY_CSV}\")\n",
        "    print(f\"  - Skipped CSV: {DRIVE_SKIPPED_CSV}\")\n",
        "    print(f\"  - Local disk: CLEAN (synced to Drive)\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEq51e6xQLHd"
      },
      "source": [
        "# run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEMbIe3ubWnD"
      },
      "outputs": [],
      "source": [
        "import scipy.linalg as la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6nSIg4EYEl2",
        "outputId": "bd81c105-6da7-4d11-fe69-c04fd4e723d5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing PIDs:   6%|▋         | 9/144 [04:16<1:08:05, 30.27s/pid, SKIPPED | 0✓ 9✗]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;33m2025-11-13 20:37:40 WARNING  one.py:360  Histology tracing for probe00 does not exist. No channels for probe00\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:ibllib:Histology tracing for probe00 does not exist. No channels for probe00\n",
            "Processing PIDs:   8%|▊         | 12/144 [05:33<1:00:49, 27.65s/pid, SKIPPED | 0✓ 12✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(384, 2),\tlocalCoordinates\n",
            "(192, 3),\tmlapdv\n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "(384,),\trawInd\n",
            "Processing PIDs:  10%|▉         | 14/144 [06:20<55:12, 25.48s/pid, SKIPPED | 0✓ 14✗]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;33m2025-11-13 20:39:41 WARNING  one.py:360  Histology tracing for probe01 does not exist. No channels for probe01\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:ibllib:Histology tracing for probe01 does not exist. No channels for probe01\n",
            "Processing PIDs:  12%|█▏        | 17/144 [07:44<1:00:30, 28.58s/pid, SKIPPED | 0✓ 17✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(384, 2),\tlocalCoordinates\n",
            "(192, 3),\tmlapdv\n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "(384,),\trawInd\n",
            "Processing PIDs:  17%|█▋        | 25/144 [11:07<50:30, 25.47s/pid, SKIPPED | 0✓ 25✗]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SYNC] Synced after 25 PIDs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing PIDs:  24%|██▍       | 35/144 [15:55<55:03, 30.31s/pid, SKIPPED | 0✓ 35✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(384,),\trawInd\n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "(384, 2),\tlocalCoordinates\n",
            "(192, 3),\tmlapdv\n",
            "Processing PIDs:  27%|██▋       | 39/144 [17:38<46:37, 26.65s/pid, SKIPPED | 0✓ 39✗]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;33m2025-11-13 20:51:02 WARNING  one.py:360  Histology tracing for probe01 does not exist. No channels for probe01\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:ibllib:Histology tracing for probe01 does not exist. No channels for probe01\n",
            "Processing PIDs:  34%|███▍      | 49/144 [22:08<43:05, 27.21s/pid, SKIPPED | 0✓ 49✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(384,),\trawInd\n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "(384, 2),\tlocalCoordinates\n",
            "(192, 3),\tmlapdv\n",
            "Processing PIDs:  35%|███▍      | 50/144 [22:32<40:54, 26.11s/pid, SKIPPED | 0✓ 50✗]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SYNC] Synced after 50 PIDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "(192, 3),\tmlapdv\n",
            "(384,),\trawInd\n",
            "(384, 2),\tlocalCoordinates\n",
            "Processing PIDs:  39%|███▉      | 56/144 [25:12<41:06, 28.02s/pid, SKIPPED | 0✓ 56✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(384, 2),\tlocalCoordinates\n",
            "(192, 3),\tmlapdv\n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "(384,),\trawInd\n",
            "Processing PIDs:  42%|████▏     | 60/144 [26:55<36:19, 25.95s/pid, SKIPPED | 0✓ 60✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(192, 3),\tmlapdv\n",
            "(384, 2),\tlocalCoordinates\n",
            "(384,),\trawInd\n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "Processing PIDs:  42%|████▏     | 61/144 [27:23<36:49, 26.62s/pid, SKIPPED | 0✓ 61✗]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PREP] e940541b-c564-46cf-99c8-f2207cfdb79c | ✓ train=(217292, 3870) test=(54323, 3870)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-167243327.py\", line 162, in process_pid\n",
            "    model, summary_row, _ = train_and_evaluate_session(\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3644626716.py\", line 65, in train_and_evaluate_session\n",
            "    w = la.solve(G.astype(np.float64), b.astype(np.float64), assume_a='pos').astype(np.float32)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py\", line 1233, in wrapper\n",
            "    return f(*arrays, *other_args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/scipy/linalg/_basic.py\", line 341, in solve\n",
            "    _solve_check(n, info)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/scipy/linalg/_basic.py\", line 43, in _solve_check\n",
            "    raise LinAlgError('Matrix is singular.')\n",
            "numpy.linalg.LinAlgError: Matrix is singular.\n",
            "Processing PIDs:  44%|████▍     | 63/144 [29:09<50:42, 37.56s/pid, SKIPPED | 0✓ 63✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "(384, 2),\tlocalCoordinates\n",
            "(384,),\trawInd\n",
            "(192, 3),\tmlapdv\n",
            "Processing PIDs:  45%|████▌     | 65/144 [30:05<43:23, 32.96s/pid, SKIPPED | 0✓ 65✗]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;33m2025-11-13 21:03:28 WARNING  one.py:360  Histology tracing for probe01 does not exist. No channels for probe01\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ibllib:Histology tracing for probe01 does not exist. No channels for probe01\n",
            "Processing PIDs:  52%|█████▏    | 75/144 [34:35<30:38, 26.65s/pid, SKIPPED | 0✓ 75✗]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYNC] Synced after 75 PIDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PIDs:  59%|█████▉    | 85/144 [39:30<29:57, 30.46s/pid, SKIPPED | 0✓ 85✗]WARNING:one.alf.io:Inconsistent dimensions for object: clusters \n",
            "(521,),\tchannels\n",
            "(521,),\tdepths\n",
            "(748, 16),\tmetrics\n",
            "Processing PIDs:  62%|██████▏   | 89/144 [41:57<29:50, 32.55s/pid, SKIPPED | 0✓ 89✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(384,),\trawInd\n",
            "(192, 3),\tmlapdv\n",
            "(384, 2),\tlocalCoordinates\n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "Processing PIDs:  64%|██████▍   | 92/144 [43:22<26:06, 30.12s/pid, SKIPPED | 0✓ 92✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(384, 2),\tlocalCoordinates\n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "(384,),\trawInd\n",
            "(192, 3),\tmlapdv\n",
            "Processing PIDs:  69%|██████▉   | 100/144 [46:59<19:34, 26.69s/pid, SKIPPED | 0✓ 100✗]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYNC] Synced after 100 PIDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PIDs:  74%|███████▎  | 106/144 [49:49<17:49, 28.15s/pid, SKIPPED | 0✓ 106✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "(384,),\trawInd\n",
            "(384, 2),\tlocalCoordinates\n",
            "(192, 3),\tmlapdv\n",
            "Processing PIDs:  78%|███████▊  | 113/144 [52:41<12:30, 24.22s/pid, SKIPPED | 0✓ 113✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "(384,),\trawInd\n",
            "(384, 2),\tlocalCoordinates\n",
            "(192, 3),\tmlapdv\n",
            "Processing PIDs:  87%|████████▋ | 125/144 [58:43<09:06, 28.77s/pid, SKIPPED | 0✓ 125✗]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYNC] Synced after 125 PIDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PIDs:  92%|█████████▏| 132/144 [1:02:03<05:34, 27.84s/pid, SKIPPED | 0✓ 132✗]WARNING:one.alf.io:Inconsistent dimensions for object: channels \n",
            "(192, 3),\tmlapdv\n",
            "(192,),\tbrainLocationIds_ccf_2017\n",
            "(384,),\trawInd\n",
            "(384, 2),\tlocalCoordinates\n",
            "Processing PIDs: 100%|██████████| 144/144 [1:07:41<00:00, 28.20s/pid, SKIPPED | 0✓ 144✗]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[SYNC] Final sync completed\n",
            "\n",
            "================================================================================\n",
            "=== Pipeline Summary ===\n",
            "================================================================================\n",
            "Total PIDs: 144\n",
            "  ✓ Trained: 0\n",
            "  ✗ Skipped: 144\n",
            "\n",
            "Performance Summary:\n",
            "  Mean R²: -10863527243515472199548928.00000 ± 209246312195206743018438656.00000\n",
            "  Best: 784c5282-2749-48ca-b211-100d0b24e29b (R²=0.75861)\n",
            "\n",
            "Outputs:\n",
            "  - Summary CSV: /content/drive/MyDrive/fullrun/decoding_summary1.csv\n",
            "  - Skipped CSV: /content/drive/MyDrive/fullrun/skipped1.csv\n",
            "  - Local disk: CLEAN (synced to Drive)\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "main(all_pids, one, ba)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FBGLGTml6-8i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "UTyUik_6E0C8",
        "xFQ-ti2bE_2H",
        "JzHdCj8jmr7s",
        "nMBIJ8tgmtWW",
        "asmOdV-ImnBm",
        "yAzRM01Cmo5I",
        "j23dqwd5m23v",
        "adVKgILEm65z",
        "2X4gJ29RE_Ge"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}